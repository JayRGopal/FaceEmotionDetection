{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D57PMLHEVlOm"
   },
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "frVOpCRsQFEX"
   },
   "outputs": [],
   "source": [
    "PAT_NOW = \"S23_199\"\n",
    "PAT_SHORT_NAME = \"S_199\"\n",
    "\n",
    "MOOD_TRACKING_SHEET_PATH = f'/home/jgopal/NAS/Analysis/AudioFacialEEG/Behavioral Labeling/Mood_Tracking.xlsx'\n",
    "\n",
    "BEHAVIORAL_LABELS_SHEET_PATH = f'/home/jgopal/NAS/Analysis/AudioFacialEEG/Behavioral Labeling/Behavior_Labeling.xlsx'\n",
    "\n",
    "VIDEO_TIMESTAMPS_SHEET_PATH = f'/home/jgopal/NAS/Analysis/AudioFacialEEG/Behavioral Labeling/videoDateTimes/VideoDatetimes{PAT_SHORT_NAME[1:]}.xlsx'\n",
    "\n",
    "OPENFACE_OUTPUT_DIRECTORY = f'/home/jgopal/NAS/Analysis/outputs_OpenFace/{PAT_NOW}/'\n",
    "COMBINED_OUTPUT_DIRECTORY = f'/home/jgopal/NAS/Analysis/outputs_Combined/{PAT_NOW}/'\n",
    "\n",
    "RUNTIME_VAR_PATH = '/home/jgopal/NAS/Analysis/AudioFacialEEG/Runtime_Vars/'\n",
    "RESULTS_PATH_BASE = f'/home/jgopal/NAS/Analysis/AudioFacialEEG/Results/{PAT_SHORT_NAME}/'\n",
    "FEATURE_VIS_PATH = f'/home/jgopal/NAS/Analysis/AudioFacialEEG/Feature_Visualization/{PAT_SHORT_NAME}/'\n",
    "FEATURE_LABEL_PATH = '/home/jgopal/NAS/Analysis/AudioFacialEEG/Feature_Labels/'\n",
    "QC_PATH = '/home/jgopal/NAS/Analysis/AudioFacialEEG/Quality_Control/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "91MVRRaOj-FH"
   },
   "outputs": [],
   "source": [
    "EMO_FEATURE_SETTING = 2\n",
    "\n",
    "# 0 - Our Custom AU --> Emotions, with all emotions\n",
    "# 1 - Our Custom AU --> Emotions, with just OpenDBM's emotions\n",
    "# 2 - OpenDBM's AU--> Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b_KxZhrQtcmv"
   },
   "outputs": [],
   "source": [
    "STATS_FEATURE_SETTING = 3\n",
    "\n",
    "# 0 - Our new features (including autocorrelation, kurtosis, etc.)\n",
    "# 1 - Our new features, excluding extras like autocorrelation and kurtosis\n",
    "# 2 - Just pres_pct\n",
    "# 3 - Our new features, excluding extras. Do NOT threshold AUs before computing metrics. HSE gets 5 event features. OGAU gets num events and presence percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VIc4rrMwD8N9"
   },
   "outputs": [],
   "source": [
    "NORMALIZE_DATA = 0\n",
    "\n",
    "# 0 - No time series normalization\n",
    "# 1 - Yes time series normalization (for each time window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU72QcAcVyLy"
   },
   "source": [
    "# Installs & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "t7QHCYhjQTQ0"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LRX3feHKIR1z"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "# Ignore all warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kKGUjhTqo-3"
   },
   "source": [
    "# Runtime Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "d6_t0DlpTFmi"
   },
   "outputs": [],
   "source": [
    "# SAVE VARIABLES\n",
    "import pickle\n",
    "\n",
    "\n",
    "def get_var_name(our_variable):\n",
    "    namespace = globals()\n",
    "    for name, obj in namespace.items():\n",
    "        if obj is our_variable:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "# Save the dictionary to a file using pickle\n",
    "def save_var(our_variable, RUNTIME_VAR_PATH=RUNTIME_VAR_PATH, forced_name=None):\n",
    "  if forced_name is None:\n",
    "    name_now = get_var_name(our_variable)\n",
    "  else:\n",
    "    name_now = forced_name\n",
    "\n",
    "  with open(RUNTIME_VAR_PATH + f'{name_now}.pkl', 'wb') as file:\n",
    "      pickle.dump(our_variable, file)\n",
    "\n",
    "def load_var(variable_name, RUNTIME_VAR_PATH=RUNTIME_VAR_PATH):\n",
    "  # Load from the file\n",
    "  with open(RUNTIME_VAR_PATH + f'{variable_name}.pkl', 'rb') as file:\n",
    "      return pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r22R0VLV4Gl"
   },
   "source": [
    "# Mood Tracking Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "W_bCqQTwV0cr"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(MOOD_TRACKING_SHEET_PATH, sheet_name=f'{PAT_SHORT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1IyDMQz7PvrR"
   },
   "outputs": [],
   "source": [
    "## Preprocess the mood tracking sheet\n",
    "\n",
    "# Replace the P_number mood headers with just the mood\n",
    "# df.columns = df.columns.str.replace('P[0-9]+ ', '')\n",
    "\n",
    "# Properly deal with the missing values\n",
    "df = df.replace('', np.nan).replace(' ', np.nan).fillna(value=np.nan)\n",
    "\n",
    "df_moodTracking = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moodTracking = df_moodTracking.drop(columns=['Notes'], errors='ignore')\n",
    "\n",
    "df_moodTracking['Datetime'] = pd.to_datetime(df_moodTracking['Datetime']).dt.strftime('%-m/%d/%Y %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "q7eiVTkZVkYq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Anxiety</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Mood</th>\n",
       "      <th>Pain</th>\n",
       "      <th>Hunger</th>\n",
       "      <th>P1 Interest</th>\n",
       "      <th>P2 Distressed</th>\n",
       "      <th>P3 Excited</th>\n",
       "      <th>P4 Upset</th>\n",
       "      <th>...</th>\n",
       "      <th>P12 Alert</th>\n",
       "      <th>P13 Ashamed</th>\n",
       "      <th>P14 Inspired</th>\n",
       "      <th>P15 Nervous</th>\n",
       "      <th>P16 Determined</th>\n",
       "      <th>P17 Attentive</th>\n",
       "      <th>P18 Jittery</th>\n",
       "      <th>P19 Active</th>\n",
       "      <th>P20 Afraid</th>\n",
       "      <th>PANAS_SUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/01/2023 10:41:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4/01/2023 13:26:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4/01/2023 14:51:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4/02/2023 12:28:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Datetime  Anxiety  Depression  Mood  Pain  Hunger  P1 Interest  \\\n",
       "0  4/01/2023 10:41:00      1.0         1.0   9.0   5.0     6.0          NaN   \n",
       "1  4/01/2023 13:26:00      1.0         0.0   7.0   3.0     4.0          NaN   \n",
       "2  4/01/2023 14:51:00      1.0         0.0   8.0   3.0     2.0          5.0   \n",
       "3  4/02/2023 12:28:00      3.0         1.0   7.0   4.0     4.0          NaN   \n",
       "\n",
       "   P2 Distressed  P3 Excited  P4 Upset  ...  P12 Alert  P13 Ashamed  \\\n",
       "0            NaN         NaN       NaN  ...        NaN          NaN   \n",
       "1            NaN         NaN       NaN  ...        NaN          NaN   \n",
       "2            1.0         4.0       1.0  ...        3.0          1.0   \n",
       "3            NaN         NaN       NaN  ...        NaN          NaN   \n",
       "\n",
       "   P14 Inspired  P15 Nervous  P16 Determined  P17 Attentive  P18 Jittery   \\\n",
       "0           NaN          NaN             NaN            NaN           NaN   \n",
       "1           NaN          NaN             NaN            NaN           NaN   \n",
       "2           4.0          4.0             5.0            4.0           1.0   \n",
       "3           NaN          NaN             NaN            NaN           NaN   \n",
       "\n",
       "   P19 Active  P20 Afraid  PANAS_SUM  \n",
       "0         NaN         NaN        NaN  \n",
       "1         NaN         NaN        NaN  \n",
       "2         2.0         3.0       56.0  \n",
       "3         NaN         NaN        NaN  \n",
       "\n",
       "[4 rows x 27 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_moodTracking[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "iM19VmZb7_2t"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# create lists to hold the positive and negative affect items\n",
    "pos_items = [1, 3, 5, 9, 10, 12, 14, 16, 17, 19]\n",
    "neg_items = [2, 4, 6, 7, 8, 11, 13, 15, 18, 20]\n",
    "\n",
    "# get all columns that start with 'P' and split them into pos and neg groups\n",
    "P_cols = [col for col in df_moodTracking.columns if col.startswith('P') and not(col.startswith('Pain')) and not(col.startswith('PANAS')) and not(col.startswith('Positive'))]\n",
    "pos_cols = [col for col in P_cols if int(col[1:3]) in pos_items]\n",
    "neg_cols = [col for col in P_cols if int(col[1:3]) in neg_items]\n",
    "\n",
    "# create new columns for the summed scores\n",
    "df_moodTracking['Positive Affect Score'] = df_moodTracking[pos_cols].fillna(0).astype(int).sum(axis=1, skipna=True)\n",
    "df_moodTracking['Negative Affect Score'] = df_moodTracking[neg_cols].fillna(0).astype(int).sum(axis=1, skipna=True)\n",
    "df_moodTracking['Overall Affect Score'] = df_moodTracking[['Positive Affect Score', 'Negative Affect Score']].fillna(0).astype(int).sum(axis=1, skipna=True)\n",
    "\n",
    "# replace 0s with NaNs in columns 'Positive Affect Score' and 'Negative Affect Score'\n",
    "df_moodTracking[['Positive Affect Score', 'Negative Affect Score', 'Overall Affect Score']] = \\\n",
    "            df_moodTracking[['Positive Affect Score', 'Negative Affect Score', 'Overall Affect Score']].replace(0, np.nan)\n",
    "\n",
    "# drop the original P columns used to create the scores\n",
    "df_moodTracking.drop(columns=pos_cols + neg_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8WpneoG7z-DO"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def normalize_columns(df, method=1):\n",
    "    # Create a copy of the DataFrame\n",
    "    normalized_df = df.copy()\n",
    "\n",
    "    # Get the column names excluding 'Datetime'\n",
    "    columns_to_normalize = [col for col in normalized_df.columns if col != 'Datetime']\n",
    "\n",
    "    if method == 1:\n",
    "        # No scaling or normalization\n",
    "        pass\n",
    "\n",
    "    elif method == 2:\n",
    "        # MinMax scaling to range 0 to 10\n",
    "        scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "        normalized_df[columns_to_normalize] = scaler.fit_transform(normalized_df[columns_to_normalize])\n",
    "\n",
    "    elif method == 3:\n",
    "        # MinMax scaling to range 0 to 1\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        normalized_df[columns_to_normalize] = scaler.fit_transform(normalized_df[columns_to_normalize])\n",
    "\n",
    "    elif method == 4:\n",
    "        # Log scaling\n",
    "        normalized_df[columns_to_normalize] = normalized_df[columns_to_normalize].astype(float)\n",
    "        normalized_df[columns_to_normalize] = np.log1p(normalized_df[columns_to_normalize])\n",
    "\n",
    "    elif method == 5:\n",
    "        # Standard normalization (Z-score normalization)\n",
    "        scaler = StandardScaler()\n",
    "        normalized_df[columns_to_normalize] = scaler.fit_transform(normalized_df[columns_to_normalize])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose a value between 1 and 5.\")\n",
    "\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UlXbkPF68v8n"
   },
   "outputs": [],
   "source": [
    "df_moodTracking = normalize_columns(df_moodTracking, method=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "i7OKfKaj0A7V"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Anxiety</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Mood</th>\n",
       "      <th>Pain</th>\n",
       "      <th>Hunger</th>\n",
       "      <th>PANAS_SUM</th>\n",
       "      <th>Positive Affect Score</th>\n",
       "      <th>Negative Affect Score</th>\n",
       "      <th>Overall Affect Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/01/2023 10:41:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4/01/2023 13:26:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4/01/2023 14:51:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4/02/2023 12:28:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4/02/2023 13:33:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4/03/2023 12:35:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>7.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4/03/2023 14:23:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4/03/2023 16:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4/04/2023 10:15:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4/04/2023 12:36:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4/04/2023 15:31:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4/04/2023 16:05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>5.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4/05/2023 09:42:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4/05/2023 14:34:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>8.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4/05/2023 22:00:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4/06/2023 11:33:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4/06/2023 14:05:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4/06/2023 18:38:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4/06/2023 20:30:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Datetime  Anxiety  Depression       Mood       Pain  Hunger  \\\n",
       "0   4/01/2023 10:41:00      2.0         5.0   8.333333  10.000000    10.0   \n",
       "1   4/01/2023 13:26:00      2.0         0.0   5.000000   3.333333     5.0   \n",
       "2   4/01/2023 14:51:00      2.0         0.0   6.666667   3.333333     0.0   \n",
       "3   4/02/2023 12:28:00      6.0         5.0   5.000000   6.666667     5.0   \n",
       "4   4/02/2023 13:33:00      2.0         5.0   5.000000   3.333333     5.0   \n",
       "5   4/03/2023 12:35:00      2.0         5.0   6.666667   6.666667     7.5   \n",
       "6   4/03/2023 14:23:00      2.0         5.0   6.666667   0.000000     7.5   \n",
       "7   4/03/2023 16:00:00      2.0         0.0   5.000000   3.333333     7.5   \n",
       "8   4/04/2023 10:15:00      2.0         5.0   3.333333   3.333333    10.0   \n",
       "9   4/04/2023 12:36:00      2.0         5.0   3.333333   6.666667     5.0   \n",
       "10  4/04/2023 15:31:00      0.0         0.0  10.000000   0.000000    10.0   \n",
       "11  4/04/2023 16:05:00      NaN         NaN        NaN        NaN     NaN   \n",
       "12  4/05/2023 09:42:00      2.0         5.0   3.333333   3.333333     5.0   \n",
       "13  4/05/2023 14:34:00      2.0         5.0   3.333333   3.333333     5.0   \n",
       "14  4/05/2023 22:00:00      8.0         5.0   5.000000   3.333333    10.0   \n",
       "15  4/06/2023 11:33:00     10.0        10.0   0.000000   0.000000     2.5   \n",
       "16  4/06/2023 14:05:00      6.0        10.0   0.000000   3.333333    10.0   \n",
       "17  4/06/2023 18:38:00      6.0         5.0   0.000000   6.666667     5.0   \n",
       "18  4/06/2023 20:30:00      4.0        10.0   0.000000   3.333333     5.0   \n",
       "\n",
       "    PANAS_SUM  Positive Affect Score  Negative Affect Score  \\\n",
       "0         NaN                    NaN                    NaN   \n",
       "1         NaN                    NaN                    NaN   \n",
       "2    0.000000               0.000000               4.444444   \n",
       "3         NaN                    NaN                    NaN   \n",
       "4         NaN                    NaN                    NaN   \n",
       "5    7.142857               7.142857               4.444444   \n",
       "6         NaN                    NaN                    NaN   \n",
       "7    1.428571               7.142857               0.000000   \n",
       "8    0.000000               0.000000               4.444444   \n",
       "9         NaN                    NaN                    NaN   \n",
       "10        NaN                    NaN                    NaN   \n",
       "11   5.714286               8.571429               2.222222   \n",
       "12        NaN                    NaN                    NaN   \n",
       "13   8.571429              10.000000               3.333333   \n",
       "14        NaN                    NaN                    NaN   \n",
       "15        NaN                    NaN                    NaN   \n",
       "16  10.000000               2.857143              10.000000   \n",
       "17        NaN                    NaN                    NaN   \n",
       "18        NaN                    NaN                    NaN   \n",
       "\n",
       "    Overall Affect Score  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2               0.000000  \n",
       "3                    NaN  \n",
       "4                    NaN  \n",
       "5               7.142857  \n",
       "6                    NaN  \n",
       "7               1.428571  \n",
       "8               0.000000  \n",
       "9                    NaN  \n",
       "10                   NaN  \n",
       "11              5.714286  \n",
       "12                   NaN  \n",
       "13              8.571429  \n",
       "14                   NaN  \n",
       "15                   NaN  \n",
       "16             10.000000  \n",
       "17                   NaN  \n",
       "18                   NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_moodTracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Kg3RagE2u5Pl"
   },
   "outputs": [],
   "source": [
    "if PAT_SHORT_NAME == 'S_214':\n",
    "    df_moodTracking = df_moodTracking.drop(1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-N5cJiTpuyVr"
   },
   "source": [
    "# Video Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QoONs-gXu0NZ"
   },
   "outputs": [],
   "source": [
    "df_videoTimestamps = pd.read_excel(VIDEO_TIMESTAMPS_SHEET_PATH, sheet_name=f'VideoDatetimes_{PAT_SHORT_NAME.split(\"_\")[-1]}')\n",
    "df_videoTimestamps['Filename'] = df_videoTimestamps['Filename'].str.replace('.m2t', '')\n",
    "\n",
    "if PAT_SHORT_NAME == 'S_199':\n",
    "  # There's no H01 video, so let's drop that filename\n",
    "  df_videoTimestamps = df_videoTimestamps.drop(211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "M7WfI-u-PR2h"
   },
   "outputs": [],
   "source": [
    "# Check for any missing videos!\n",
    "\n",
    "def print_difference(list1, list2):\n",
    "    for item in list1:\n",
    "        if item not in list2:\n",
    "            print(item)\n",
    "\n",
    "filenames_master_list = list(df_videoTimestamps['Filename'].values)\n",
    "filenames_we_have = [i[:-4] for i in os.listdir(COMBINED_OUTPUT_DIRECTORY)]\n",
    "\n",
    "print_difference(filenames_master_list, filenames_we_have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "cqMfPe79vFRv"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>VideoStart</th>\n",
       "      <th>VideoEnd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>3332YX00</td>\n",
       "      <td>2023-04-09 11:04:43</td>\n",
       "      <td>2023-04-09 12:04:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>3332YX01</td>\n",
       "      <td>2023-04-09 12:04:43</td>\n",
       "      <td>2023-04-09 13:04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>3332YY00</td>\n",
       "      <td>2023-04-09 13:04:43</td>\n",
       "      <td>2023-04-09 14:04:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>3332Z100</td>\n",
       "      <td>2023-04-09 14:14:03</td>\n",
       "      <td>2023-04-09 15:12:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>48220H00</td>\n",
       "      <td>2023-03-31 15:35:06</td>\n",
       "      <td>2023-03-31 16:35:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Filename          VideoStart            VideoEnd\n",
       "206  3332YX00 2023-04-09 11:04:43 2023-04-09 12:04:43\n",
       "207  3332YX01 2023-04-09 12:04:43 2023-04-09 13:04:35\n",
       "208  3332YY00 2023-04-09 13:04:43 2023-04-09 14:04:42\n",
       "209  3332Z100 2023-04-09 14:14:03 2023-04-09 15:12:06\n",
       "210  48220H00 2023-03-31 15:35:06 2023-03-31 16:35:06"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videoTimestamps[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOrT1X_9WU5X"
   },
   "source": [
    "# OpenFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YV8_wmALY-hV"
   },
   "outputs": [],
   "source": [
    "# DICTIONARY OF SEPARATE DFS\n",
    "\n",
    "def get_dict_openface(output_dir):\n",
    "  # Create an empty dictionary to hold the DataFrames\n",
    "  dfs_openface = {}\n",
    "\n",
    "  # Get a list of all the CSV files in the directory\n",
    "  csv_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.csv')])\n",
    "\n",
    "  # list of columns to keep\n",
    "  columns_to_keep = ['frame', 'timestamp', 'success',\n",
    "                    'AU01_r',\n",
    "                    'AU02_r',\n",
    "                    'AU04_r',\n",
    "                    'AU05_r',\n",
    "                    'AU06_r',\n",
    "                    'AU07_r',\n",
    "                    'AU09_r',\n",
    "                    'AU10_r',\n",
    "                    'AU12_r',\n",
    "                    'AU14_r',\n",
    "                    'AU15_r',\n",
    "                    'AU17_r',\n",
    "                    'AU20_r',\n",
    "                    'AU23_r',\n",
    "                    'AU25_r',\n",
    "                    'AU26_r',\n",
    "                    'AU45_r',\n",
    "                    'AU01_c',\n",
    "                    'AU02_c',\n",
    "                    'AU04_c',\n",
    "                    'AU05_c',\n",
    "                    'AU06_c',\n",
    "                    'AU07_c',\n",
    "                    'AU09_c',\n",
    "                    'AU10_c',\n",
    "                    'AU12_c',\n",
    "                    'AU14_c',\n",
    "                    'AU15_c',\n",
    "                    'AU17_c',\n",
    "                    'AU20_c',\n",
    "                    'AU23_c',\n",
    "                    'AU25_c',\n",
    "                    'AU26_c',\n",
    "                    'AU45_c']\n",
    "\n",
    "  # Loop through the CSV files\n",
    "  for csv_file in csv_files:\n",
    "      # Load data into a pandas df\n",
    "      csv_file_path = os.path.join(output_dir, csv_file)\n",
    "      df_temp = pd.read_csv(csv_file_path)\n",
    "      df_temp.columns = df_temp.columns.str.strip()\n",
    "\n",
    "      # keep every 6th row such that it's 5 fps!\n",
    "      X = 6\n",
    "      df_temp = df_temp[df_temp.index % X == 0]\n",
    "\n",
    "      # filter DataFrame to keep only columns in list\n",
    "      df_temp = df_temp.loc[:, columns_to_keep]\n",
    "\n",
    "      # fix column names to not have leading or trailing spaces!\n",
    "      df_temp = df_temp.rename(columns=lambda x: x.strip())\n",
    "\n",
    "      # Store the DataFrame in the dictionary with the csv file name as the key\n",
    "      # remove the '.csv' by doing csv_file[:-4]\n",
    "      dfs_openface[csv_file[:-4]] = df_temp\n",
    "      del df_temp\n",
    "\n",
    "  return dfs_openface\n",
    "\n",
    "\n",
    "def get_dict_openface_extras(output_dir):\n",
    "  # Create an empty dictionary to hold the DataFrames\n",
    "  dfs_openface = {}\n",
    "\n",
    "  # Get a list of all the CSV files in the directory\n",
    "  csv_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.csv')])\n",
    "\n",
    "  # list of columns to keep\n",
    "  columns_to_keep = ['frame', ' timestamp', ' success',\n",
    "                     'gaze_0_x',\n",
    "                     'gaze_0_y',\n",
    "                     'gaze_0_z',\n",
    "                     'gaze_1_x',\n",
    "                     'gaze_1_y',\n",
    "                     'gaze_1_z',\n",
    "                     'pose_Tx',\n",
    "                     'pose_Ty',\n",
    "                     'pose_Tz',\n",
    "                     'pose_Rx',\n",
    "                     'pose_Ry',\n",
    "                     'pose_Rz']\n",
    "\n",
    "  columns_to_keep = columns_to_keep + [f\"eye_lmk_X_{i}\" for i in range(56)] + [f\"eye_lmk_Y_{i}\" for i in range(56)] + [f\"eye_lmk_Z_{i}\" for i in range(56)] \n",
    "  columns_to_keep = columns_to_keep + [f\"X_{i}\" for i in range(68)] + [f\"Y_{i}\" for i in range(68)] + [f\"Z_{i}\" for i in range(68)]\n",
    "    \n",
    "  # remove special character \n",
    "  columns_to_keep = [one_str.replace(' ', '') for one_str in columns_to_keep]\n",
    "\n",
    "  # Loop through the CSV files\n",
    "  for csv_file in csv_files:\n",
    "      # Load data into a pandas df\n",
    "      csv_file_path = os.path.join(output_dir, csv_file)\n",
    "      df_temp = pd.read_csv(csv_file_path)\n",
    "      df_temp.columns = df_temp.columns.str.strip()\n",
    "\n",
    "      # keep every 6th row such that it's 5 fps!\n",
    "      X = 6\n",
    "      df_temp = df_temp[df_temp.index % X == 0]\n",
    "\n",
    "      # filter DataFrame to keep only columns in list\n",
    "      # remove special character \n",
    "      df_temp = df_temp.loc[:, columns_to_keep]\n",
    "\n",
    "      # fix column names to not have leading or trailing spaces!\n",
    "      df_temp = df_temp.rename(columns=lambda x: x.strip())\n",
    "\n",
    "      # Store the DataFrame in the dictionary with the csv file name as the key\n",
    "      # remove the '.csv' by doing csv_file[:-4]\n",
    "      dfs_openface[csv_file[:-4]] = df_temp\n",
    "      del df_temp\n",
    "\n",
    "  return dfs_openface\n",
    "\n",
    "\n",
    "\n",
    "def only_successful_frames(df):\n",
    "    # get frames where AU/emotion detection was successful!\n",
    "    return df[df['success'] == 1]\n",
    "\n",
    "def apply_function_to_dict(dictionary, func, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply a function to each DataFrame in a dictionary and return a modified copy of the dictionary.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): The dictionary containing DataFrames.\n",
    "        func (function): The function to apply to each DataFrame.\n",
    "        **kwargs: Additional keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        dict: A modified copy of the dictionary with the function applied to each DataFrame.\n",
    "    \"\"\"\n",
    "    return {key: func(df, **kwargs) for key, df in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and load or generate dfs_openface\n",
    "if not os.path.exists(RUNTIME_VAR_PATH + f'dfs_openface_{PAT_SHORT_NAME}.pkl'):\n",
    "    # Generate dfs_openface if not already saved\n",
    "    dfs_openface = get_dict_openface(OPENFACE_OUTPUT_DIRECTORY)\n",
    "    dfs_openface = apply_function_to_dict(dfs_openface, only_successful_frames)\n",
    "    save_var(dfs_openface, forced_name=f'dfs_openface_{PAT_SHORT_NAME}')\n",
    "else:\n",
    "    # Load dfs_openface if it already exists\n",
    "    dfs_openface = load_var(f'dfs_openface_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check and load or generate dfs_openface_extras\n",
    "if not os.path.exists(RUNTIME_VAR_PATH + f'dfs_openface_extras_{PAT_NOW}.pkl'):\n",
    "    # Generate dfs_openface_extras if not already saved\n",
    "    dfs_openface_extras = get_dict_openface_extras(OPENFACE_OUTPUT_DIRECTORY)\n",
    "    dfs_openface_extras = apply_function_to_dict(dfs_openface_extras, only_successful_frames)\n",
    "    save_var(dfs_openface_extras, forced_name=f'dfs_openface_extras_{PAT_NOW}')\n",
    "else:\n",
    "    # Load dfs_openface_extras if it already exists\n",
    "    dfs_openface_extras = load_var(f'dfs_openface_extras_{PAT_NOW}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t52_30WQzYeh"
   },
   "source": [
    "# HSEmotion & OpenGraphAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "aUg5iqkRtIJX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_dict(output_dir, file_now='outputs_hse.csv', filterOutLR=True):\n",
    "\n",
    "  # Initialize an empty dictionary to store the dataframes\n",
    "  df_dict = {}\n",
    "\n",
    "  # Loop through the subfolders in alphabetical order\n",
    "  for subfolder_name in sorted(os.listdir(output_dir)):\n",
    "\n",
    "    # Check if the subfolder contains CSV files\n",
    "    subfolder_path = os.path.join(output_dir, subfolder_name)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "      continue\n",
    "\n",
    "    # Load the first CSV file in the subfolder into a dataframe\n",
    "    csv_file_path = os.path.join(subfolder_path, file_now)\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "      continue\n",
    "\n",
    "    try:\n",
    "      df_temp = pd.read_csv(csv_file_path)\n",
    "    except:\n",
    "      df_temp = pd.DataFrame(columns=['frame', 'timestamp', 'success', 'AU1', 'AU2', 'AU4', 'AU5', 'AU6', 'AU7', 'AU9',\n",
    "       'AU10', 'AU11', 'AU12', 'AU13', 'AU14', 'AU15', 'AU16', 'AU17', 'AU18',\n",
    "       'AU19', 'AU20', 'AU22', 'AU23', 'AU24', 'AU25', 'AU26', 'AU27', 'AU32',\n",
    "       'AU38', 'AU39'])\n",
    "\n",
    "\n",
    "    # OpenGraphAU - we are filtering out L and R!\n",
    "    if filterOutLR:\n",
    "      df_temp = df_temp.filter(regex='^(?!AUL|AUR)')\n",
    "\n",
    "    # Add the dataframe to the dictionary with the subfolder name as the key\n",
    "    # We do [:-4] to remove '.mp4' from the end of the string\n",
    "    df_dict[subfolder_name[:-4]] = df_temp\n",
    "\n",
    "  return df_dict\n",
    "\n",
    "def create_binary_columns(df, threshold):\n",
    "    df_copy = df.copy()\n",
    "    # adds classification columns to opengraphAU\n",
    "    for col in df_copy.columns:\n",
    "        if col.startswith('AU'):\n",
    "            # Add _c to the column name for the new column\n",
    "            new_col_name = col + '_c'\n",
    "            # Apply the binary classification to the new column\n",
    "            df_copy[new_col_name] = df_copy[col].apply(lambda x: 1 if x >= threshold else 0)\n",
    "            # Add _r to the original column name\n",
    "            df_copy = df_copy.rename(columns={col: col + '_r'}, inplace=False)\n",
    "    return df_copy\n",
    "\n",
    "def remove_columns_ending_with_r(df):\n",
    "    columns_to_drop = [col for col in df.columns if col.endswith('_r')]\n",
    "    df = df.drop(columns=columns_to_drop, inplace=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def only_successful_frames(df):\n",
    "    # get frames where AU/emotion detection was successful!\n",
    "    return df[df['success'] == 1]\n",
    "\n",
    "\n",
    "def apply_function_to_dict(dictionary, func, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply a function to each DataFrame in a dictionary and return a modified copy of the dictionary.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): The dictionary containing DataFrames.\n",
    "        func (function): The function to apply to each DataFrame.\n",
    "        **kwargs: Additional keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        dict: A modified copy of the dictionary with the function applied to each DataFrame.\n",
    "    \"\"\"\n",
    "    return {key: func(df, **kwargs) for key, df in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and load or generate dfs_hsemotion\n",
    "if not os.path.exists(RUNTIME_VAR_PATH + f'dfs_hsemotion_{PAT_SHORT_NAME}.pkl'):\n",
    "    # Generate dfs_hsemotion if not already saved\n",
    "    dfs_hsemotion = get_dict(COMBINED_OUTPUT_DIRECTORY, file_now='outputs_hse.csv')\n",
    "    dfs_hsemotion = apply_function_to_dict(dfs_hsemotion, only_successful_frames)\n",
    "    save_var(dfs_hsemotion, forced_name=f'dfs_hsemotion_{PAT_SHORT_NAME}')\n",
    "else:\n",
    "    # Load dfs_hsemotion if it already exists\n",
    "    dfs_hsemotion = load_var(f'dfs_hsemotion_{PAT_SHORT_NAME}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and load or generate dfs_opengraphau\n",
    "if not os.path.exists(RUNTIME_VAR_PATH + f'dfs_opengraphau_{PAT_SHORT_NAME}.pkl'):\n",
    "    # Generate dfs_opengraphau if not already saved\n",
    "    OPENGRAPHAU_THRESHOLD = 0.5\n",
    "    dfs_opengraphau = get_dict(COMBINED_OUTPUT_DIRECTORY, file_now='outputs_ogau.csv')\n",
    "    dfs_opengraphau = apply_function_to_dict(dfs_opengraphau, create_binary_columns, threshold=OPENGRAPHAU_THRESHOLD)\n",
    "    dfs_opengraphau = apply_function_to_dict(dfs_opengraphau, only_successful_frames)\n",
    "    dfs_opengraphau = apply_function_to_dict(dfs_opengraphau, remove_columns_ending_with_r)\n",
    "    save_var(dfs_opengraphau, forced_name=f'dfs_opengraphau_{PAT_SHORT_NAME}')\n",
    "else:\n",
    "    # Load dfs_opengraphau if it already exists\n",
    "    dfs_opengraphau = load_var(f'dfs_opengraphau_{PAT_SHORT_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjDp31PcyrBD"
   },
   "source": [
    "# Select Specific Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rrCWvN5Kmjr"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rgRoAaIsyuFY"
   },
   "outputs": [],
   "source": [
    "def get_data_within_duration(dfs_dict, df_video_timestamps, datetime, duration):\n",
    "    # Takes in:\n",
    "    # dfs_dict -- a dictionary of dataframes containing csv data from one of the pipelines\n",
    "    # df_video_timestamps -- the VideoDateTimes_199 csv\n",
    "    # datetime -- a pd.datetime value to center our extraction\n",
    "    # duration -- a duration (in minutes) BEFORE the datetime to extract\n",
    "\n",
    "    # Outputs:\n",
    "    # One dataframe with all rows we want, with timestamps converted into correct datetimes\n",
    "    start_datetime = datetime - pd.Timedelta(minutes=duration)\n",
    "    end_datetime = datetime\n",
    "\n",
    "    relevant_keys = df_video_timestamps.loc[(pd.to_datetime(df_video_timestamps['VideoEnd']) >= start_datetime) &\n",
    "                                            (pd.to_datetime(df_video_timestamps['VideoStart']) <= end_datetime), 'Filename'].values\n",
    "\n",
    "    relevant_dfs = []\n",
    "    for key in relevant_keys:\n",
    "        if key in dfs_dict:\n",
    "            video_start = pd.to_datetime(df_video_timestamps.loc[df_video_timestamps['Filename'] == key, 'VideoStart'].values[0])\n",
    "            video_end = pd.to_datetime(df_video_timestamps.loc[df_video_timestamps['Filename'] == key, 'VideoEnd'].values[0])\n",
    "            time_mask = ((dfs_dict[key]['timestamp'] >= (start_datetime - video_start).total_seconds()) &\n",
    "                         (dfs_dict[key]['timestamp'] <= (end_datetime - video_start).total_seconds()))\n",
    "            df = dfs_dict[key].loc[time_mask].copy()\n",
    "            df['timestamp'] = video_start + pd.to_timedelta(df['timestamp'], unit='s')\n",
    "            relevant_dfs.append(df)\n",
    "\n",
    "    if relevant_dfs:\n",
    "        df_combined = pd.concat(relevant_dfs, ignore_index=True, sort=False)\n",
    "        df_combined = df_combined.drop(columns='frame')\n",
    "\n",
    "        return df_combined\n",
    "\n",
    "    print(f\"MAJOR ERROR! ZERO RELEVANT DFS!! DATETIME: {datetime}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def get_radius_dict(TIME_RADIUS_IN_MINUTES, INPUT_DF, df_videoTimestamps, df_moodTracking, takeAll=True):\n",
    "  # takes in the:\n",
    "  # --time radius,\n",
    "  # --input dataframe dict (e.g. is it from OpenFace? HSEmotion?)\n",
    "  # --df with video timestamps\n",
    "  # --df with mood tracking patient reports\n",
    "  # --takeAll - are we taking all reports, or filtering out values w/o mood (e.g. anxiety)? True = no filtering\n",
    "\n",
    "  # returns dictionary of timestamp : df with relevant frames\n",
    "\n",
    "  # We'll make a dictionary, with the relevant df for each datetime we have a report\n",
    "  radius_df_dict = {}\n",
    "  for oneIndex in range(len(df_moodTracking)):\n",
    "    # Let's make sure there's a value collected (or takeAll = True)!\n",
    "    if takeAll:\n",
    "      dt_now = get_moodTracking_datetime(oneIndex, df_moodTracking=df_moodTracking)\n",
    "      filtered_df = get_data_within_duration(INPUT_DF, df_videoTimestamps, dt_now, TIME_RADIUS_IN_MINUTES)\n",
    "      radius_df_dict[dt_now] = filtered_df\n",
    "    else:\n",
    "      val_now = df_moodTracking[oneIndex:oneIndex+1]['Anxiety'][oneIndex]\n",
    "      if isinstance(val_now, str):\n",
    "        # Value was collected\n",
    "        dt_now = get_moodTracking_datetime(oneIndex, df_moodTracking=df_moodTracking)\n",
    "        filtered_df = get_data_within_duration(INPUT_DF, df_videoTimestamps, dt_now, TIME_RADIUS_IN_MINUTES)\n",
    "        radius_df_dict[dt_now] = filtered_df\n",
    "      else:\n",
    "        # No value collected!\n",
    "        print('No value for Anxiety for index ', oneIndex, f'corresponding to {get_moodTracking_datetime(oneIndex, df_moodTracking=df_moodTracking)}')\n",
    "  return radius_df_dict\n",
    "\n",
    "def generate_number_list(start, interval, count):\n",
    "    number_list = [start + i * interval for i in range(count)]\n",
    "    return number_list\n",
    "\n",
    "def get_moodTracking_datetime(index, df_moodTracking):\n",
    "  temp_var = pd.to_datetime(pd.to_datetime(df_moodTracking[index:index+1]['Datetime']).dt.strftime('%d-%b-%Y %H:%M:%S'))\n",
    "  return pd.Timestamp(temp_var[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TfTqzxUavwP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN-kMqgqKobt"
   },
   "source": [
    "## Emotion/Affect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "YFPiVE4vEgv0"
   },
   "outputs": [],
   "source": [
    "# EMOTION DETECTION & AFFECT\n",
    "\n",
    "takeAll = True # we are taking all patient reports\n",
    "\n",
    "# start and interval are in minutes\n",
    "TIME_RADIUS_LIST = generate_number_list(start=15, interval=15, count=16)\n",
    "#TIME_RADIUS_LIST = [60, 120, 180, 240]\n",
    "\n",
    "\n",
    "ENABLE_OPENFACE = True\n",
    "\n",
    "if ENABLE_OPENFACE:\n",
    "  openface_radius_dict = {}\n",
    "  openface_extras_radius_dict = {}\n",
    "\n",
    "hsemotion_radius_dict = {}\n",
    "opengraphau_radius_dict = {}\n",
    "\n",
    "for i in TIME_RADIUS_LIST:\n",
    "  if ENABLE_OPENFACE:\n",
    "    openface_radius_now = get_radius_dict(i, dfs_openface, df_videoTimestamps, df_moodTracking, takeAll=takeAll)\n",
    "    openface_radius_dict[f'{i}'] = openface_radius_now\n",
    "    \n",
    "    openface_extras_radius_now = get_radius_dict(i, dfs_openface_extras, df_videoTimestamps, df_moodTracking, takeAll=takeAll)\n",
    "    openface_extras_radius_dict[f'{i}'] = openface_extras_radius_now\n",
    "\n",
    "  hsemotion_radius_now = get_radius_dict(i, dfs_hsemotion, df_videoTimestamps, df_moodTracking, takeAll=takeAll)\n",
    "  hsemotion_radius_dict[f'{i}'] = hsemotion_radius_now\n",
    "\n",
    "  opengraphau_radius_now = get_radius_dict(i, dfs_opengraphau, df_videoTimestamps, df_moodTracking, takeAll=takeAll)\n",
    "  opengraphau_radius_dict[f'{i}'] = opengraphau_radius_now\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_splitter(input_dict, splitter_times):\n",
    "    # Initialize the output dictionary\n",
    "    output_dict = {}\n",
    "    \n",
    "    # Frame rate: 5 frames per second\n",
    "    frame_rate = 5\n",
    "    \n",
    "    # Iterate over each split time\n",
    "    for split_time in splitter_times:\n",
    "        # Initialize the dictionary for the current split time\n",
    "        output_dict[split_time] = {}\n",
    "        \n",
    "        # Calculate the number of rows per split\n",
    "        rows_per_split = split_time * 60 * frame_rate\n",
    "        \n",
    "        # Iterate over the outer dictionary\n",
    "        for outer_key, inner_dict in input_dict.items():\n",
    "            # Initialize the dictionary for the current outer key\n",
    "            output_dict[split_time][outer_key] = {}\n",
    "            \n",
    "            # Iterate over the inner dictionary\n",
    "            for timestamp, df in inner_dict.items():\n",
    "                # Split the DataFrame into chunks of the specified size\n",
    "                split_dfs = [df.iloc[i:i+rows_per_split] for i in range(0, len(df), rows_per_split)]\n",
    "                \n",
    "                # Assign the list of split DataFrames to the appropriate location in the output dictionary\n",
    "                output_dict[split_time][outer_key][timestamp] = split_dfs\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "# Functions to apply feature processing to the inpatient dictionary structure\n",
    "\n",
    "def apply_function_to_dict_list(dictionary, func, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply a function to each DataFrame in a dictionary where values are LISTS of dfs and return a modified copy of the dictionary.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): The dictionary containing DataFrames.\n",
    "        func (function): The function to apply to each DataFrame.\n",
    "        **kwargs: Additional keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        dict: A modified copy of the dictionary with the function applied to each DataFrame.\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for split_time, outer_dict in dictionary.items():\n",
    "        new_dict[split_time] = {}\n",
    "        for outer_key, inner_dict in outer_dict.items():\n",
    "            new_dict[split_time][outer_key] = {}\n",
    "            for timestamp, df_list in inner_dict.items():\n",
    "                new_dict[split_time][outer_key][timestamp] = [func(df, **kwargs) for df in df_list]\n",
    "    return new_dict\n",
    "\n",
    "def average_inner_dfs(dictionary):\n",
    "    \"\"\"\n",
    "    Replace each list of DataFrames in a nested dictionary with the average of the DataFrames in each list.\n",
    "    For columns with strings, convert to numbers if possible and take the string from the first DataFrame otherwise.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): The dictionary containing lists of DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        dict: A modified copy of the dictionary with the average of each list of DataFrames.\n",
    "    \"\"\"\n",
    "    def process_columns(df_list):\n",
    "        \"\"\"\n",
    "        Process columns to calculate averages for numeric columns and keep strings from the first DataFrame.\n",
    "        \"\"\"\n",
    "        if not df_list:\n",
    "            # If df_list is empty, return an empty DataFrame\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        avg_df = pd.DataFrame(index=combined_df.index)\n",
    "\n",
    "        for column in combined_df.columns:\n",
    "            # Try to convert the column to numeric\n",
    "            numeric_series = pd.to_numeric(combined_df[column], errors='coerce')\n",
    "            \n",
    "            if numeric_series.notna().all():\n",
    "                # If all values can be converted to numeric, calculate the mean\n",
    "                avg_df[column] = numeric_series.groupby(combined_df.index).mean()\n",
    "            else:\n",
    "                # Repeat the first DataFrame's values to match the length of the combined DataFrame\n",
    "                avg_df[column] = np.tile(df_list[0][column].values[0], len(combined_df))\n",
    "        \n",
    "        return avg_df\n",
    "    \n",
    "    def create_empty_df_like(sample_df):\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the same columns as sample_df but filled with zeros (or equivalent) based on datatype.\n",
    "        \"\"\"\n",
    "        return pd.DataFrame({col: np.zeros(sample_df.shape[0], dtype=sample_df[col].dtype) for col in sample_df.columns})\n",
    "\n",
    "    new_dict = {}\n",
    "    for split_time, outer_dict in dictionary.items():\n",
    "        new_dict[split_time] = {}\n",
    "        for outer_key, inner_dict in outer_dict.items():\n",
    "            new_dict[split_time][outer_key] = {}\n",
    "            for timestamp, df_list in inner_dict.items():\n",
    "                if df_list:\n",
    "                    # If the df_list is not empty, process normally\n",
    "                    avg_df = process_columns(df_list)\n",
    "                else:\n",
    "                    # If the df_list is empty, find a non-empty DataFrame structure to create a zero-filled DataFrame\n",
    "                    for outer_split_time in dictionary.values():\n",
    "                        for outer_inner_dict in outer_split_time.values():\n",
    "                            for df in outer_inner_dict.values():\n",
    "                                if df:  # Ensure df_list is not empty\n",
    "                                    avg_df = create_empty_df_like(df[0])\n",
    "                                    break\n",
    "                            if 'avg_df' in locals():\n",
    "                                break\n",
    "                        if 'avg_df' in locals():\n",
    "                            break\n",
    "                new_dict[split_time][outer_key][timestamp] = avg_df\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and load or generate openface_radius_dict\n",
    "if not os.path.exists(RUNTIME_VAR_PATH + f'openface_radius_dict_{PAT_SHORT_NAME}.pkl'):\n",
    "    # Generate openface_radius_dict if not already saved\n",
    "    openface_radius_dict = time_splitter(openface_radius_dict, [5, 10])\n",
    "    save_var(openface_radius_dict, forced_name=f'openface_radius_dict_{PAT_SHORT_NAME}')\n",
    "else:\n",
    "    # Load openface_radius_dict if it already exists\n",
    "    openface_radius_dict = load_var(f'openface_radius_dict_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and load or generate hsemotion_radius_dict\n",
    "if not os.path.exists(RUNTIME_VAR_PATH + f'hsemotion_radius_dict_{PAT_SHORT_NAME}.pkl'):\n",
    "    # Generate hsemotion_radius_dict if not already saved\n",
    "    hsemotion_radius_dict = time_splitter(hsemotion_radius_dict, [5, 10])\n",
    "    save_var(hsemotion_radius_dict, forced_name=f'hsemotion_radius_dict_{PAT_SHORT_NAME}')\n",
    "else:\n",
    "    # Load hsemotion_radius_dict if it already exists\n",
    "    hsemotion_radius_dict = load_var(f'hsemotion_radius_dict_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check and load or generate opengraphau_radius_dict\n",
    "if not os.path.exists(RUNTIME_VAR_PATH + f'opengraphau_radius_dict_{PAT_SHORT_NAME}.pkl'):\n",
    "    # Generate opengraphau_radius_dict if not already saved\n",
    "    opengraphau_radius_dict = time_splitter(opengraphau_radius_dict, [5, 10])\n",
    "    save_var(opengraphau_radius_dict, forced_name=f'opengraphau_radius_dict_{PAT_SHORT_NAME}')\n",
    "else:\n",
    "    # Load opengraphau_radius_dict if it already exists\n",
    "    opengraphau_radius_dict = load_var(f'opengraphau_radius_dict_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and load or generate openface_extras_radius_dict\n",
    "if not os.path.exists(RUNTIME_VAR_PATH + f'openface_extras_radius_dict_{PAT_SHORT_NAME}.pkl'):\n",
    "    # Generate openface_extras_radius_dict if not already saved\n",
    "    openface_extras_radius_dict = time_splitter(openface_extras_radius_dict, [5, 10])\n",
    "    save_var(openface_extras_radius_dict, forced_name=f'openface_extras_radius_dict_{PAT_SHORT_NAME}')\n",
    "else:\n",
    "    # Load openface_extras_radius_dict if it already exists\n",
    "    openface_extras_radius_dict = load_var(f'openface_extras_radius_dict_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openface_radius_dict\n",
    "# Keys: Split times (5 mins and 10 mins)\n",
    "# Keys: Time intervals (15, 30, 45, etc.)\n",
    "# Keys: Timestamps\n",
    "# Value: pandas df with AUs, success, timestamp, etc. as columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction 1.0 (June 2024 Inpatient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy paste this in from outpatient notebook after it's done for both FaceDx and OpenFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AU --> Emotion & Lower/Upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emotion to AU mapping\n",
    "\n",
    "# OpenDBM:\n",
    "emo_AUs = {'Happiness': [6, 12],\n",
    "           'Sadness': [1, 4, 15],\n",
    "           'Surprise': [1, 2, 5, 26],\n",
    "           'Fear': [1, 2, 4, 5, 7, 20, 26],\n",
    "           'Anger': [4, 5, 7, 23],\n",
    "           'Disgust': [9, 15, 16],\n",
    "           'Contempt': [12, 14]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AU to lower/upper\n",
    "\n",
    "# OpenDBM:\n",
    "AU_lower = [12, 15, 26, 20, 23, 14]\n",
    "AU_upper = [6, 1, 4, 2, 5, 7, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Processing - HSEmotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_successful_frames(df):\n",
    "    # get frames where AU/emotion detection was successful!\n",
    "    return df[df['success'] == 1]\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def binarize_cols(df, threshold=0.5):\n",
    "  new_df = df.copy()\n",
    "  emotions = [col for col in new_df.columns if col not in ['frame', 'success', 'timestamp']]\n",
    "\n",
    "  for emotion in emotions:\n",
    "      new_df[f'{emotion}_Raw'] = new_df[emotion]\n",
    "      new_df[f'{emotion}_Binary'] = (new_df[f'{emotion}_Raw'] >= threshold).astype(int)\n",
    "\n",
    "  new_df = new_df.drop(columns=emotions, inplace=False)\n",
    "\n",
    "  return new_df\n",
    "\n",
    "\n",
    "def fill_empty_dfs_lists(dictionary):\n",
    "    \"\"\"\n",
    "    Fill empty DataFrames in a nested dictionary structure with a DataFrame of zeros.\n",
    "    \n",
    "    Args:\n",
    "        dictionary (dict): The dictionary containing nested dictionaries with lists of DataFrames.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A modified copy of the dictionary with empty DataFrames filled with zeros.\n",
    "    \"\"\"\n",
    "    # Find the first non-empty DataFrame to use as a template for filling empty DataFrames\n",
    "    non_empty_df = None\n",
    "    for split_time, outer_dict in dictionary.items():\n",
    "        for outer_key, inner_dict in outer_dict.items():\n",
    "            for timestamp, df_list in inner_dict.items():\n",
    "                for df in df_list:\n",
    "                    if not df.empty:\n",
    "                        non_empty_df = df\n",
    "                        break\n",
    "                if non_empty_df is not None:\n",
    "                    break\n",
    "            if non_empty_df is not None:\n",
    "                break\n",
    "        if non_empty_df is not None:\n",
    "            break\n",
    "    \n",
    "    # If no non-empty DataFrame is found, return the original dictionary\n",
    "    if non_empty_df is None:\n",
    "        return dictionary\n",
    "\n",
    "    # Create the modified dictionary\n",
    "    modified_dictionary = {}\n",
    "    for split_time, outer_dict in dictionary.items():\n",
    "        modified_dictionary[split_time] = {}\n",
    "        for outer_key, inner_dict in outer_dict.items():\n",
    "            modified_dictionary[split_time][outer_key] = {}\n",
    "            for timestamp, df_list in inner_dict.items():\n",
    "                modified_df_list = []\n",
    "                for df in df_list:\n",
    "                    if df.empty:\n",
    "                        modified_df = pd.DataFrame(0, index=non_empty_df.index, columns=non_empty_df.columns)\n",
    "                        # Preserve string columns from the non-empty DataFrame\n",
    "                        for column in non_empty_df.columns:\n",
    "                            if non_empty_df[column].dtype == object:\n",
    "                                modified_df[column] = non_empty_df[column]\n",
    "                    else:\n",
    "                        modified_df = df.copy()\n",
    "                    modified_df_list.append(modified_df)\n",
    "                modified_dictionary[split_time][outer_key][timestamp] = modified_df_list\n",
    "\n",
    "    return modified_dictionary\n",
    "\n",
    "\n",
    "def analyze_emotion_events_v2(df, max_frame_gap=10, event_minimum_num_frames=1, method='HSE'):\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Emotions to analyze\n",
    "    emotions_raw = [col for col in df.columns if col not in ['frame', 'success', 'timestamp']]\n",
    "    # Removing \"_Raw\" or \"_Binary\" from each string\n",
    "    processed_strings = [s.replace(\"_Raw\", \"\").replace(\"_Binary\", \"\") for s in emotions_raw]\n",
    "    # Eliminating duplicates\n",
    "    emotions = list(set(processed_strings))\n",
    "\n",
    "    # Create DataFrame for results\n",
    "    if STATS_FEATURE_SETTING == 0:\n",
    "        results_df = pd.DataFrame(index=['avg_event_length', 'avg_event_duration', 'total_num_events', 'avg_probability', 'std', 'skewness', 'kurtosis', 'autocorrelation', 'pres_pct'])\n",
    "    elif STATS_FEATURE_SETTING == 1 or (STATS_FEATURE_SETTING == 3 and method == 'HSE'):\n",
    "        results_df = pd.DataFrame(index=['avg_event_length', 'total_num_events', 'avg_probability', 'std', 'pres_pct'])\n",
    "    elif STATS_FEATURE_SETTING == 2:\n",
    "        results_df = pd.DataFrame(index=['pres_pct'])\n",
    "    elif STATS_FEATURE_SETTING == 3 and (method == 'OGAU' or method=='OF'):\n",
    "        results_df = pd.DataFrame(index=['pres_pct', 'total_num_events'])\n",
    "\n",
    "\n",
    "    def detect_events(emotion_binary_col):\n",
    "        probThreshold = 0.5 # irrelevant because it's a binary column\n",
    "        minInterval = max_frame_gap\n",
    "        minDuration = event_minimum_num_frames\n",
    "\n",
    "        probBinary = emotion_binary_col > probThreshold\n",
    "\n",
    "        # Using np.diff to find changes in the binary array\n",
    "        changes = np.diff(probBinary.astype(int))\n",
    "\n",
    "        # Identify start (1) and stop (-1) points\n",
    "        starts = np.where(changes == 1)[0] + 1  # +1 to correct the index shift caused by diff\n",
    "        stops = np.where(changes == -1)[0] + 1\n",
    "\n",
    "        # Adjust for edge cases\n",
    "        if probBinary.iloc[0]:\n",
    "            starts = np.insert(starts, 0, 0)\n",
    "        if probBinary.iloc[-1]:\n",
    "            stops = np.append(stops, len(probBinary))\n",
    "\n",
    "        # Merge close events and filter by duration\n",
    "        events = []\n",
    "        for start, stop in zip(starts, stops):\n",
    "\n",
    "            # Construct the event considering only indices where probBinary is 1\n",
    "            event = np.arange(start, stop)[probBinary[start:stop].values]\n",
    "\n",
    "            # Check if there is a previous event to potentially merge with\n",
    "            if events and event.size > 0 and events[-1][-1] >= start - minInterval:\n",
    "                # Merge with the previous event\n",
    "                events[-1] = np.unique(np.concatenate([events[-1], event]))\n",
    "            elif event.size >= event_minimum_num_frames:\n",
    "                events.append(event)\n",
    "\n",
    "        # Filter events by minimum duration\n",
    "        valid_events = [event for event in events if len(event) >= minDuration]\n",
    "\n",
    "        return valid_events\n",
    "\n",
    "    for emotion in emotions:\n",
    "        # Identify events\n",
    "        emotion_binary_col = df[f'{emotion}_Binary']\n",
    "        emotion_presence = df[f'{emotion}_Binary'].sum()\n",
    "        pres_pct = emotion_presence / len(df) * 100  # Percentage of frames where emotion is present\n",
    "        events = detect_events(emotion_binary_col)\n",
    "\n",
    "        if not(STATS_FEATURE_SETTING == 2):\n",
    "            # Calculate features for each event\n",
    "            if events:\n",
    "                event_lengths = [len(event) for event in events]\n",
    "                event_durations = [event[-1] - event[0] + 1 for event in events]\n",
    "                probabilities = [df.loc[event, f'{emotion}_Raw'].values for event in events]\n",
    "                probabilities_flattened = np.concatenate(probabilities)\n",
    "\n",
    "                avg_event_length = np.mean(event_lengths)\n",
    "                avg_event_duration = np.mean(event_durations)\n",
    "\n",
    "                total_num_events = len(events)\n",
    "\n",
    "                # NORMALIZE TOTAL NUM EVENTS BASED ON DF SIZE\n",
    "                # total_num_events = len(events) * 1000 / df.shape[0]\n",
    "\n",
    "                avg_probability = np.mean(probabilities_flattened)\n",
    "                std_dev = np.std(probabilities_flattened)\n",
    "                skewness_val = skew(probabilities_flattened)\n",
    "                kurtosis_val = kurtosis(probabilities_flattened)\n",
    "                autocorr = acf(probabilities_flattened, fft=True, nlags=1)[1] if len(probabilities_flattened) > 1 else 0\n",
    "            else:\n",
    "                avg_event_length = 0\n",
    "                avg_event_duration = 0\n",
    "                total_num_events = 0\n",
    "                avg_probability = 0\n",
    "                std_dev = 0\n",
    "                skewness_val = 0\n",
    "                kurtosis_val = 0\n",
    "                autocorr = 0\n",
    "\n",
    "        # Add results to the DataFrame\n",
    "        if STATS_FEATURE_SETTING == 0:\n",
    "            results_df[emotion] = [avg_event_length, avg_event_duration, total_num_events, avg_probability, std_dev, skewness_val, kurtosis_val, autocorr, pres_pct]\n",
    "        elif STATS_FEATURE_SETTING == 1 or (STATS_FEATURE_SETTING == 3 and method == 'HSE'):\n",
    "            results_df[emotion] = [avg_event_length, total_num_events, avg_probability, std_dev, pres_pct]\n",
    "        elif STATS_FEATURE_SETTING == 2:\n",
    "            results_df[emotion] = [pres_pct]\n",
    "        elif STATS_FEATURE_SETTING == 3 and (method == 'OGAU' or method=='OF'):\n",
    "            results_df[emotion] = [pres_pct, total_num_events]\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    results_df.fillna(0, inplace=True)\n",
    "\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def detect_emotions(df, method, emo_AUs, additional_filter=None):\n",
    "    # INPUT:\n",
    "    # df -- dataframe with AUs for each frame\n",
    "    # method -- must be 'OpenFace'\n",
    "    # emo_AUs -- the hash table\n",
    "    # additional_filter -- are we just doing lower half? upper half? This is None or a list of ints (which AUs to keep)\n",
    "\n",
    "    # OUTPUT:\n",
    "    # 3 datafrmes. Each has emotion values for each frame\n",
    "    # emo_hard, emo_soft, emo_binary (see OpenDBM docs for details)\n",
    "\n",
    "\n",
    "    if df.empty:\n",
    "      return (df, df, df)\n",
    "    # We start by mapping AUs to emotions for each of our two methods\n",
    "    # Using this mapping: https://aicure.github.io/open_dbm/docs/emotional-expressivity\n",
    "    if method == 'OpenFace':\n",
    "        columns = ['AU01_r','AU02_r', 'AU04_r', 'AU05_r', 'AU06_r', 'AU07_r', 'AU09_r', 'AU10_r',\n",
    "                    'AU12_r', 'AU14_r', 'AU15_r', 'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r',\n",
    "                    'AU26_r', 'AU45_r',\n",
    "                    'AU01_c',\n",
    "                    'AU02_c',\n",
    "                    'AU04_c',\n",
    "                    'AU05_c',\n",
    "                    'AU06_c',\n",
    "                    'AU07_c',\n",
    "                    'AU09_c',\n",
    "                    'AU10_c',\n",
    "                    'AU12_c',\n",
    "                    'AU14_c',\n",
    "                    'AU15_c',\n",
    "                    'AU17_c',\n",
    "                    'AU20_c',\n",
    "                    'AU23_c',\n",
    "                    'AU25_c',\n",
    "                    'AU26_c',\n",
    "                    'AU45_c']\n",
    "\n",
    "        # hash tables for presence and intensity\n",
    "        emo_AUs_presence = {}\n",
    "        emo_AUs_intensity = {}\n",
    "        for key in emo_AUs.keys(): # loop through emotion strings\n",
    "            new_values_r = [] # regression\n",
    "            new_values_c = [] # classification\n",
    "\n",
    "            for value in emo_AUs[key]:\n",
    "                if isinstance(value, int):\n",
    "                    AU_key_r = \"AU{:02d}_r\".format(value)\n",
    "                    AU_key_c = \"AU{:02d}_c\".format(value)\n",
    "                    if AU_key_r in columns:\n",
    "                        if additional_filter is not None:\n",
    "                          if value in additional_filter:\n",
    "                            new_values_r.append(AU_key_r)\n",
    "                        else:\n",
    "                          new_values_r.append(AU_key_r)\n",
    "                    if AU_key_c in columns:\n",
    "                        if additional_filter is not None:\n",
    "                          if value in additional_filter:\n",
    "                            new_values_c.append(AU_key_c)\n",
    "                        else:\n",
    "                          new_values_c.append(AU_key_c)\n",
    "            if new_values_r:\n",
    "                emo_AUs_intensity[key] = new_values_r\n",
    "            if new_values_c:\n",
    "                emo_AUs_presence[key] = new_values_c\n",
    "\n",
    "    else:\n",
    "        # if the method specified is not OpenFace or OpenGraphAU, raise an error (pipeline doesn't support others yet)\n",
    "        raise ValueError(\"Invalid method parameter. Method must be 'OpenFace'.\")\n",
    "\n",
    "    # Create an empty dictionary to store the emotion scores\n",
    "    emotion_scores_hard = {} # only non-zero if all AUs present\n",
    "    emotion_scores_soft = {} # average of AU intensities even if all not present\n",
    "    emotion_scores_binary = {} # 1 or 0: are all AUs present?\n",
    "\n",
    "    # Compute emotion scores for each emotion\n",
    "    for emotion in emo_AUs_presence.keys():\n",
    "        # Get the relevant columns for presence and intensity\n",
    "        presence_cols = emo_AUs_presence[emotion]\n",
    "        intensity_cols = emo_AUs_intensity[emotion]\n",
    "\n",
    "        # Compute the emotion score for each row in the dataframe\n",
    "        emotion_scores_hard[emotion] = df[intensity_cols].mean(axis=1) * df[presence_cols].all(axis=1)\n",
    "        emotion_scores_hard[emotion] = emotion_scores_hard[emotion].fillna(0)\n",
    "\n",
    "        emotion_scores_soft[emotion] = df[intensity_cols].mean(axis=1)\n",
    "        emotion_scores_soft[emotion] = emotion_scores_soft[emotion].fillna(0)\n",
    "\n",
    "        emotion_scores_binary[emotion] = df[presence_cols].all(axis=1)\n",
    "        emotion_scores_binary[emotion] = emotion_scores_binary[emotion].fillna(0)\n",
    "\n",
    "    # Create a new dataframe with the emotion scores\n",
    "    emotion_df_hard = pd.DataFrame(emotion_scores_hard)\n",
    "    emotion_df_soft = pd.DataFrame(emotion_scores_soft)\n",
    "    emotion_df_binary = pd.DataFrame(emotion_scores_binary)\n",
    "    emotion_df_binary = emotion_df_binary.replace({False: 0, True: 1})\n",
    "\n",
    "    # Let's add timestamp and success on\n",
    "    columns_of_interest = ['timestamp', 'success']\n",
    "    df_temp = df[columns_of_interest]\n",
    "\n",
    "    # Concatenate the columns from df2 with df1\n",
    "    emotion_df_hard = pd.concat([df_temp, emotion_df_hard], axis=1)\n",
    "    emotion_df_soft = pd.concat([df_temp, emotion_df_soft], axis=1)\n",
    "    emotion_df_binary = pd.concat([df_temp, emotion_df_binary], axis=1)\n",
    "\n",
    "    return emotion_df_hard, emotion_df_soft, emotion_df_binary\n",
    "\n",
    "\n",
    "\n",
    "def detect_emotions_og(df, method, emo_AUs, additional_filter=None):\n",
    "    # INPUT:\n",
    "    # df -- dataframe with AUs for each frame\n",
    "    # method -- must be 'OpenGraphAU'\n",
    "    # emo_AUs -- the hash table\n",
    "    # additional_filter -- are we just doing lower half? upper half? This is None or a list of ints (which AUs to keep)\n",
    "\n",
    "    # OUTPUT:\n",
    "    # 1 datafrme with emotion values for each frame\n",
    "    # emo_binary (see OpenDBM docs for details)\n",
    "\n",
    "\n",
    "    if df.empty:\n",
    "      return df\n",
    "    # We start by mapping AUs to emotions for each of our two methods\n",
    "    # Using this mapping: https://aicure.github.io/open_dbm/docs/emotional-expressivity\n",
    "\n",
    "\n",
    "    if method == 'OpenGraphAU':\n",
    "        columns = ['AU1', 'AU2', 'AU4', 'AU5', 'AU6', 'AU7', 'AU9',\n",
    "                   'AU10', 'AU11', 'AU12', 'AU13', 'AU14', 'AU15', 'AU16', 'AU17',\n",
    "                   'AU18', 'AU19', 'AU20', 'AU22', 'AU23', 'AU24', 'AU25', 'AU26', 'AU27', 'AU32',\n",
    "                   'AU38', 'AU39']\n",
    "\n",
    "        # add the classification columns!\n",
    "        columns = [item for sublist in [[col+'_r', col+'_c'] for col in columns] for item in sublist]\n",
    "\n",
    "        # hash tables for presence and intensity\n",
    "        emo_AUs_presence = {}\n",
    "        for key in emo_AUs.keys():\n",
    "            new_values_c = []\n",
    "            for value in emo_AUs[key]:\n",
    "                if isinstance(value, int):\n",
    "                    AU_key_c = f\"AU{value}_c\"\n",
    "\n",
    "                    if AU_key_c in columns:\n",
    "                        if additional_filter is not None:\n",
    "                          if value in additional_filter:\n",
    "                            new_values_c.append(AU_key_c)\n",
    "                        else:\n",
    "                          new_values_c.append(AU_key_c)\n",
    "            if new_values_c:\n",
    "                emo_AUs_presence[key] = new_values_c\n",
    "\n",
    "    else:\n",
    "        # if the method specified is not OpenFace or OpenGraphAU, raise an error (pipeline doesn't support others yet)\n",
    "        raise ValueError(\"Invalid method parameter. Method must be 'OpenGraphAU'.\")\n",
    "\n",
    "    # Create an empty dictionary to store the emotion scores\n",
    "    emotion_scores_binary = {} # 1 or 0: are all AUs present?\n",
    "\n",
    "    # Compute emotion scores for each emotion\n",
    "    for emotion in emo_AUs_presence.keys():\n",
    "        # Get the relevant columns for presence\n",
    "        presence_cols = emo_AUs_presence[emotion]\n",
    "\n",
    "        # Compute the emotion score for each row in the dataframe\n",
    "        emotion_scores_binary[emotion] = df[presence_cols].all(axis=1)\n",
    "        emotion_scores_binary[emotion] = emotion_scores_binary[emotion].fillna(0)\n",
    "\n",
    "    # Create a new dataframe with the emotion scores\n",
    "    emotion_df_binary = pd.DataFrame(emotion_scores_binary)\n",
    "    emotion_df_binary = emotion_df_binary.replace({False: 0, True: 1})\n",
    "\n",
    "    # Let's add timestamp and success on\n",
    "    columns_of_interest = ['timestamp', 'success']\n",
    "    df_temp = df[columns_of_interest]\n",
    "\n",
    "    # Concatenate the columns from df2 with df1\n",
    "    emotion_df_binary = pd.concat([df_temp, emotion_df_binary], axis=1)\n",
    "\n",
    "    return emotion_df_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Variables for Emotional Expressivity!\n",
    "\n",
    "openface_emoHardSoftPres_dict = apply_function_to_dict_list(openface_radius_dict, detect_emotions, method='OpenFace', emo_AUs=emo_AUs)\n",
    "\n",
    "# key: (df_emohard, df_emosoft, df_emopres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will help us get Raw Variables for Overall Expressivity!\n",
    "\n",
    "# key: (df_emohard, df_emosoft, df_emopres)\n",
    "\n",
    "openface_lowerHardSoftPres_dict = apply_function_to_dict_list(openface_radius_dict, detect_emotions, method='OpenFace', emo_AUs=emo_AUs, additional_filter=AU_lower)\n",
    "openface_upperHardSoftPres_dict = apply_function_to_dict_list(openface_radius_dict, detect_emotions, method='OpenFace', emo_AUs=emo_AUs, additional_filter=AU_upper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.4\n",
    "hsemotion_radius_binarized = apply_function_to_dict_list(hsemotion_radius_dict, binarize_cols, threshold=THRESHOLD)\n",
    "hsemotion_emo_stats = apply_function_to_dict_list(hsemotion_radius_binarized, analyze_emotion_events_v2, max_frame_gap=10, event_minimum_num_frames=12, method='HSE')\n",
    "hsemotion_emo_stats_dict_list = fill_empty_dfs_lists(hsemotion_emo_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsemotion_emo_stats_dict = average_inner_dfs(hsemotion_emo_stats_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "opengraphau_emoPres_dict = apply_function_to_dict_list(opengraphau_radius_dict, detect_emotions_og, method='OpenGraphAU', emo_AUs=emo_AUs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "opengraphau_lowerPres_dict = apply_function_to_dict_list(opengraphau_radius_dict, detect_emotions_og, method='OpenGraphAU', emo_AUs=emo_AUs, additional_filter=AU_lower)\n",
    "opengraphau_upperPres_dict = apply_function_to_dict_list(opengraphau_radius_dict, detect_emotions_og, method='OpenGraphAU', emo_AUs=emo_AUs, additional_filter=AU_upper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Our HSEmotion Analysis to OpenFace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openface_combine_and_binarize(soft_hard):\n",
    "    \"\"\"\n",
    "    Combine the middle and last dataframes from detect_emotions output,\n",
    "    with columns for AU raw and binary values renamed appropriately.\n",
    "\n",
    "    Parameters:\n",
    "    - soft_hard: a list of two DataFrames:\n",
    "      - emo_soft: DataFrame, the second output of detect_emotions, with AU raw values\n",
    "      - emo_binary: DataFrame, the third output of detect_emotions, with AU binary values\n",
    "\n",
    "    Returns:\n",
    "    - combined_df: DataFrame, combined dataframe with emotion raw and binary values.\n",
    "    \"\"\"\n",
    "    emo_soft, emo_binary = soft_hard\n",
    "\n",
    "    # Drop 'timestamp' and 'success' columns from emo_binary to prevent duplication\n",
    "    emo_binary = emo_binary.drop(['timestamp', 'success'], axis=1, errors='ignore')\n",
    "\n",
    "    # Rename columns in emo_soft and emo_binary for clarity\n",
    "    emo_soft_columns = {col: f\"{col}_Raw\" for col in emo_soft.columns if col not in ['success', 'timestamp', 'frame']}\n",
    "    emo_binary_columns = {col: f\"{col}_Binary\" for col in emo_binary.columns if col not in ['success', 'timestamp', 'frame']}\n",
    "\n",
    "    emo_soft_renamed = emo_soft.rename(columns=emo_soft_columns)\n",
    "    emo_binary_renamed = emo_binary.rename(columns=emo_binary_columns)\n",
    "\n",
    "    # Combine the dataframes\n",
    "    combined_df = pd.concat([emo_soft_renamed, emo_binary_renamed], axis=1)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def take_second_from_tuple(input):\n",
    "    return input[1]\n",
    "\n",
    "\n",
    "def take_second_third_from_tuple(input):\n",
    "    return [input[1], input[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of dictionary of just soft values\n",
    "openface_emoSoft_dict = apply_function_to_dict_list(openface_emoHardSoftPres_dict, take_second_from_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of list of two dictionaries: soft, presence (binary)\n",
    "openface_emoSoftPres_dict = apply_function_to_dict_list(openface_emoHardSoftPres_dict, take_second_third_from_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENFACE - affect/emotions (longer term)\n",
    "openface_binarized = apply_function_to_dict_list(openface_emoSoftPres_dict, openface_combine_and_binarize)\n",
    "openface_emo_stats = apply_function_to_dict_list(openface_binarized, analyze_emotion_events_v2, max_frame_gap=10, event_minimum_num_frames=12, method='OF')\n",
    "openface_emo_stats_dict_list = fill_empty_dfs_lists(openface_emo_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENFACE - Averaging across time windows!\n",
    "\n",
    "openface_emo_stats_dict = average_inner_dfs(openface_emo_stats_dict_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    \"\"\"\n",
    "    Renames the columns in a DataFrame according to specified pattern.\n",
    "\n",
    "    Args:\n",
    "        df (pandas DataFrame): The DataFrame to rename columns.\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame: The DataFrame with renamed columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Define the mapping for renaming columns\n",
    "    column_mapping = {\n",
    "        '_r': 'int',\n",
    "        '_c': 'pres'\n",
    "    }\n",
    "\n",
    "    # Function to rename the columns\n",
    "    def rename_column(column_name):\n",
    "        au_number = column_name[2:4]\n",
    "        if au_number.endswith('_'):\n",
    "          au_number = '0' + au_number[0:1]\n",
    "        suffix = column_name[-2:]\n",
    "        if suffix in column_mapping:\n",
    "            return f'fac_au{au_number}{column_mapping[suffix]}'\n",
    "        else:\n",
    "            return column_name\n",
    "\n",
    "    # Rename the columns in the copied DataFrame\n",
    "    df_copy = df_copy.rename(columns=rename_column)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_AU_statistics(df):\n",
    "    # Initialize an empty dictionary to store the computed statistics\n",
    "    stats = {'AU': [], 'pres_pct': [], 'int_mean': [], 'int_std': []}\n",
    "\n",
    "    # Iterate over the AU columns\n",
    "    for col in df.columns:\n",
    "        if col.startswith('fac_au') and ('pres' in col):\n",
    "            # Calculate the percentage of frames where AU is present\n",
    "            pres_pct = df[col].mean() * 100\n",
    "            # Extract the AU number\n",
    "            AU = col.split('au')[1][0:2]\n",
    "            # Calculate the mean and standard deviation of intensity for the AU\n",
    "            int_mean = df[f'fac_au{AU}int'].mean()\n",
    "            int_std = df[f'fac_au{AU}int'].std()\n",
    "\n",
    "            # Add the statistics to the dictionary\n",
    "            stats['AU'].append(AU)\n",
    "            stats['pres_pct'].append(pres_pct)\n",
    "            stats['int_mean'].append(int_mean)\n",
    "            stats['int_std'].append(int_std)\n",
    "\n",
    "    # Create a DataFrame from the dictionary of statistics\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "def calculate_AU_statistics_og(df):\n",
    "    # Stats for ONLY binary columns!\n",
    "    # Initialize an empty dictionary to store the computed statistics\n",
    "    stats = {'AU': [], 'pres_pct': []}\n",
    "\n",
    "    # Iterate over the AU columns\n",
    "    for col in df.columns:\n",
    "        if col.startswith('fac_au') and ('pres' in col):\n",
    "            # Calculate the percentage of frames where AU is present\n",
    "            pres_pct = df[col].mean() * 100\n",
    "            # Extract the AU number\n",
    "            AU = col.split('au')[1][0:2]\n",
    "\n",
    "            # Add the statistics to the dictionary\n",
    "            stats['AU'].append(AU)\n",
    "            stats['pres_pct'].append(pres_pct)\n",
    "\n",
    "    # Create a DataFrame from the dictionary of statistics\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "def force_convert_to_float(dictionary):\n",
    "    \"\"\"\n",
    "    Forcefully convert all DataFrames in a nested dictionary structure to have their columns as floats.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): The dictionary containing nested dictionaries with lists of DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        dict: A modified copy of the dictionary with all DataFrames converted to float.\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for split_time, outer_dict in dictionary.items():\n",
    "        new_dict[split_time] = {}\n",
    "        for outer_key, inner_dict in outer_dict.items():\n",
    "            new_dict[split_time][outer_key] = {}\n",
    "            for timestamp, df_list in inner_dict.items():\n",
    "                new_df_list = [df.astype(float) for df in df_list]\n",
    "                new_dict[split_time][outer_key][timestamp] = new_df_list\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Variables!\n",
    "openface_radius_renamed_dict = apply_function_to_dict_list(openface_radius_dict, rename_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Variables!\n",
    "openface_au_derived_dict_list = apply_function_to_dict_list(openface_radius_renamed_dict, calculate_AU_statistics)\n",
    "openface_au_derived_dict_list = fill_empty_dfs_lists(openface_au_derived_dict_list)\n",
    "openface_au_derived_dict_list = force_convert_to_float(openface_au_derived_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENFACE - Averaging across time windows!\n",
    "\n",
    "openface_au_derived_dict = average_inner_dfs(openface_au_derived_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "opengraphau_radius_renamed_dict = apply_function_to_dict_list(opengraphau_radius_dict, rename_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "opengraphau_au_derived_dict_list = apply_function_to_dict_list(opengraphau_radius_renamed_dict, calculate_AU_statistics_og)\n",
    "opengraphau_au_derived_dict_list = force_convert_to_float(opengraphau_au_derived_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENGRAPHAU - Averaging across time windows!\n",
    "\n",
    "opengraphau_au_derived_dict = average_inner_dfs(opengraphau_au_derived_dict_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotional Expressivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emotion_express_statistics(tuple_to_unpack):\n",
    "    \"\"\"\n",
    "    Calculates statistics for each emotion in the given DataFrames.\n",
    "\n",
    "    Args:\n",
    "        tuple_to_unpack: 3-membered tuple that has:\n",
    "          df_emo_inthard (pandas DataFrame): DataFrame with emotion intensity (hard) values.\n",
    "          df_emo_intsoft (pandas DataFrame): DataFrame with emotion intensity (soft) values.\n",
    "          df_emo_pres (pandas DataFrame): DataFrame with emotion presence values.\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame with statistics for each emotion.\n",
    "    \"\"\"\n",
    "    df_emo_inthard, df_emo_intsoft, df_emo_pres = tuple_to_unpack\n",
    "    stats = {'emotion': [], 'pres_pct': [], 'intsoft_mean': [], 'intsoft_std': [], 'inthard_mean': []}\n",
    "\n",
    "    emotions = [col for col in df_emo_inthard.columns if col not in ['timestamp', 'success']]\n",
    "\n",
    "    for emotion in emotions:\n",
    "        pres_pct = (df_emo_pres[emotion] == 1).mean() * 100\n",
    "        intsoft_mean = df_emo_intsoft[emotion].mean()\n",
    "        intsoft_std = df_emo_intsoft[emotion].std()\n",
    "        inthard_mean = df_emo_inthard[emotion].mean()\n",
    "\n",
    "        stats['emotion'].append(emotion)\n",
    "        stats['pres_pct'].append(pres_pct)\n",
    "        stats['intsoft_mean'].append(intsoft_mean)\n",
    "        stats['intsoft_std'].append(intsoft_std)\n",
    "        stats['inthard_mean'].append(inthard_mean)\n",
    "\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    return stats_df\n",
    "\n",
    "def calculate_ee_stats_og(df_emo_pres):\n",
    "    \"\"\"\n",
    "    Calculates statistics for each emotion in the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df_emo_pres (pandas DataFrame): DataFrame with emotion presence values.\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame with statistics for each emotion.\n",
    "    \"\"\"\n",
    "    stats = {'emotion': [], 'pres_pct': []}\n",
    "\n",
    "    emotions = [col for col in df_emo_pres.columns if col not in ['timestamp', 'success']]\n",
    "\n",
    "    for emotion in emotions:\n",
    "        pres_pct = (df_emo_pres[emotion] == 1).mean() * 100\n",
    "\n",
    "\n",
    "        stats['emotion'].append(emotion)\n",
    "        stats['pres_pct'].append(pres_pct)\n",
    "\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ee_stats_hse(df, threshold):\n",
    "    \"\"\"\n",
    "    Calculates statistics for each emotion in the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df with emotion intensities for every video frame\n",
    "    threshold for presence of emotion (i.e. 0.5)\n",
    "\n",
    "    Returns:\n",
    "        pandas DataFrame: A DataFrame with statistics for each emotion.\n",
    "    \"\"\"\n",
    "    df_emo_intsoft = df\n",
    "    stats = {'emotion': [], 'pres_pct': [], 'intsoft_mean': [], 'intsoft_std': []}\n",
    "\n",
    "    emotions = [col for col in df_emo_intsoft.columns if col not in ['timestamp', 'success']]\n",
    "\n",
    "    for emotion in emotions:\n",
    "        pres_pct = (df_emo_intsoft[emotion] >= threshold).mean() * 100\n",
    "        intsoft_mean = df_emo_intsoft[emotion].mean()\n",
    "        intsoft_std = df_emo_intsoft[emotion].std()\n",
    "\n",
    "        stats['emotion'].append(emotion)\n",
    "        stats['pres_pct'].append(pres_pct)\n",
    "        stats['intsoft_mean'].append(intsoft_mean)\n",
    "        stats['intsoft_std'].append(intsoft_std)\n",
    "\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Variables for Emotional Expressivity were calculated above:\n",
    "# openface_emoHardSoftPres\n",
    "# opengraphau_emoHardSoftPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Variables for Emotional Expressivity\n",
    "openface_ee_derived_dict_list = apply_function_to_dict_list(openface_emoHardSoftPres_dict, calculate_emotion_express_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENFACE - Averaging across time windows!\n",
    "\n",
    "openface_ee_derived_dict = average_inner_dfs(openface_ee_derived_dict_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opengraphau_ee_derived_dict_list = apply_function_to_dict_list(opengraphau_emoPres_dict, calculate_ee_stats_og)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENGRAPHAU - Averaging across time windows!\n",
    "\n",
    "opengraphau_ee_derived_dict = average_inner_dfs(opengraphau_ee_derived_dict_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsemotion_ee_derived_dict_list = apply_function_to_dict_list(hsemotion_radius_dict, calculate_ee_stats_hse, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSEmotion - Averaging across time windows!\n",
    "\n",
    "hsemotion_ee_derived_dict = average_inner_dfs(hsemotion_ee_derived_dict_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Expressivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_oe_raw_vars(regular_tuple, lower_tuple, upper_tuple):\n",
    "    # Takes in 3 3-membered tuples, each of which should be hardSoftPres\n",
    "    # regular, lower, upper\n",
    "\n",
    "    # Outputs one df with the raw variables for overall expressivity\n",
    "\n",
    "    df_emo_inthard, df_emo_intsoft, df_emo_pres = regular_tuple\n",
    "    df_emo_inthard_lower, df_emo_intsoft_lower, df_emo_pres_lower = lower_tuple\n",
    "    df_emo_inthard_upper, df_emo_intsoft_upper, df_emo_pres_upper = upper_tuple\n",
    "\n",
    "    df_emo_inthard = df_emo_inthard.drop(columns=['timestamp'])\n",
    "    df_emo_intsoft = df_emo_intsoft.drop(columns=['timestamp'])\n",
    "    df_emo_pres = df_emo_pres.drop(columns=['timestamp'])\n",
    "\n",
    "    df_emo_inthard_lower = df_emo_inthard_lower.drop(columns=['timestamp'])\n",
    "    df_emo_intsoft_lower = df_emo_intsoft_lower.drop(columns=['timestamp'])\n",
    "    df_emo_pres_lower = df_emo_pres_lower.drop(columns=['timestamp'])\n",
    "\n",
    "    df_emo_inthard_upper = df_emo_inthard_upper.drop(columns=['timestamp'])\n",
    "    df_emo_intsoft_upper = df_emo_intsoft_upper.drop(columns=['timestamp'])\n",
    "    df_emo_pres_upper = df_emo_pres_upper.drop(columns=['timestamp'])\n",
    "\n",
    "    # Calculate the average values for emo_intsoft and emo_inthard across all frames\n",
    "    avg_emo_intsoft = df_emo_intsoft.mean(axis=1)\n",
    "    avg_emo_inthard = df_emo_inthard.mean(axis=1)\n",
    "\n",
    "    # Calculate lower and upper averages across all frames\n",
    "    avg_emo_intsoft_lower = df_emo_intsoft_lower.mean(axis=1)\n",
    "    avg_emo_inthard_lower = df_emo_inthard_lower.mean(axis=1)\n",
    "    avg_emo_intsoft_upper = df_emo_intsoft_upper.mean(axis=1)\n",
    "    avg_emo_inthard_upper = df_emo_inthard_upper.mean(axis=1)\n",
    "\n",
    "    # Create a new dataframe with the computed statistics\n",
    "    stats_df = pd.DataFrame({'comintsoft': avg_emo_intsoft, 'cominthard': avg_emo_inthard,\n",
    "                             'comlowintsoft': avg_emo_intsoft_lower, 'comlowinthard': avg_emo_inthard_lower,\n",
    "                             'comuppintsoft': avg_emo_intsoft_upper, 'comuppinthard': avg_emo_inthard_upper,})\n",
    "\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_function_to_dict_three(d1, d2, d3, func, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply a function that takes in 3 dfs and return a modified dictionary\n",
    "\n",
    "    Args:\n",
    "        d1, d2, d3: The dictionaries containing DataFrames.\n",
    "        func (function): The function to apply to each DataFrame.\n",
    "        **kwargs: Additional keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        dict_final: A modified copy of the dictionary with the function applied to each DataFrame.\n",
    "    \"\"\"\n",
    "    dict_final = {}\n",
    "    for key in d1.keys():\n",
    "      dict_final[key] = func(d1[key], d2[key], d3[key], **kwargs)\n",
    "\n",
    "    return dict_final\n",
    "\n",
    "def apply_function_to_dict_three_list(d1, d2, d3, func, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply a function that takes in 3 dfs and return a modified dictionary\n",
    "\n",
    "    Args:\n",
    "        d1, d2, d3: The dictionaries containing LISTS of DataFrames.\n",
    "        func (function): The function to apply to each DataFrame.\n",
    "        **kwargs: Additional keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        dict_final: A modified copy of the dictionary with the function applied to each DataFrame in each list!\n",
    "    \"\"\"\n",
    "    dict_final = {}\n",
    "    for key in d1.keys():\n",
    "      num_in_list = len(d1[key])\n",
    "      list_building = []\n",
    "      for i in range(num_in_list):\n",
    "        list_building.append(func(d1[key][i], d2[key][i], d3[key][i], **kwargs))\n",
    "      \n",
    "      dict_final[key] = list_building\n",
    "        \n",
    "\n",
    "    return dict_final\n",
    "\n",
    "def calculate_oe_summary_statistics(df):\n",
    "    # Compute comintsoft_pct\n",
    "    comintsoft_pct = (df['comintsoft'] > 0).mean() * 100\n",
    "\n",
    "    # Compute comintsoft_mean and comintsoft_std\n",
    "    comintsoft_mean = df['comintsoft'].mean()\n",
    "    comintsoft_std = df['comintsoft'].std()\n",
    "\n",
    "    # Compute cominthard_mean and cominthard_std\n",
    "    cominthard_mean = df['cominthard'].mean()\n",
    "    cominthard_std = df['cominthard'].std()\n",
    "\n",
    "    # Compute comlowintsoft_pct\n",
    "    comlowintsoft_pct = (df['comlowintsoft'] > 0).mean() * 100\n",
    "\n",
    "    # Compute comlowintsoft_mean and comlowintsoft_std\n",
    "    comlowintsoft_mean = df['comlowintsoft'].mean()\n",
    "    comlowintsoft_std = df['comlowintsoft'].std()\n",
    "\n",
    "    # Compute comuppinthard_mean and comuppinthard_std\n",
    "    comuppinthard_mean = df['comuppinthard'].mean()\n",
    "    comuppinthard_std = df['comuppinthard'].std()\n",
    "\n",
    "    # Create a new DataFrame with the summary statistics\n",
    "    summary_df = pd.DataFrame({\n",
    "        'comintsoft_pct': [comintsoft_pct],\n",
    "        'comintsoft_mean': [comintsoft_mean],\n",
    "        'comintsoft_std': [comintsoft_std],\n",
    "        'cominthard_mean': [cominthard_mean],\n",
    "        'cominthard_std': [cominthard_std],\n",
    "        'comlowintsoft_pct': [comlowintsoft_pct],\n",
    "        'comlowintsoft_mean': [comlowintsoft_mean],\n",
    "        'comlowintsoft_std': [comlowintsoft_std],\n",
    "        'comuppinthard_mean': [comuppinthard_mean],\n",
    "        'comuppinthard_std': [comuppinthard_std]\n",
    "    })\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Variables for Overall Expressivity!\n",
    "\n",
    "#openface_oe_raw = apply_function_to_dict_three(openface_emoHardSoftPres, openface_lowerHardSoftPres, openface_upperHardSoftPres, compute_oe_raw_vars)\n",
    "#opengraphau_oe_raw = apply_function_to_dict_three(opengraphau_emoHardSoftPres, opengraphau_lowerHardSoftPres, opengraphau_upperHardSoftPres, compute_oe_raw_vars)\n",
    "\n",
    "\n",
    "openface_oe_raw_dict_list = {}\n",
    "\n",
    "\n",
    "# Loop through the dictionaries and sample one item from each with the same key\n",
    "for time_now in openface_emoHardSoftPres_dict.keys():\n",
    "    openface_oe_raw_dict_list[time_now] = {}\n",
    "    for key in openface_emoHardSoftPres_dict[time_now].keys():\n",
    "        openface_emo = openface_emoHardSoftPres_dict[time_now][key]\n",
    "        openface_lower = openface_lowerHardSoftPres_dict[time_now][key]\n",
    "        openface_upper = openface_upperHardSoftPres_dict[time_now][key]\n",
    "    \n",
    "        # Call the compute_oe_raw_vars function with the sampled items\n",
    "        openface_oe_raw_dict_list[time_now][key] = apply_function_to_dict_three_list(openface_emo, openface_lower, openface_upper, compute_oe_raw_vars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Variables for Overall Expressivity!\n",
    "\n",
    "openface_oe_derived_dict_list = apply_function_to_dict_list(openface_oe_raw_dict_list, calculate_oe_summary_statistics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENFACE - Averaging across time windows!\n",
    "\n",
    "openface_oe_derived_dict = average_inner_dfs(openface_oe_derived_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_head_movement(df):\n",
    "    # Ensure the pose columns are floats\n",
    "    pose_cols = ['pose_Tx', 'pose_Ty', 'pose_Tz', 'pose_Rx', 'pose_Ry', 'pose_Rz']\n",
    "    df[pose_cols] = df[pose_cols].astype(float)\n",
    "    \n",
    "    # Calculate Euclidean head movement (displacement)\n",
    "    df['mov_headvel'] = np.sqrt(df[['pose_Tx', 'pose_Ty', 'pose_Tz']].diff().fillna(0).pow(2).sum(axis=1))\n",
    "    \n",
    "    # Assign frame-wise pitch, yaw, and roll directly from pose_Rx, pose_Ry, pose_Rz\n",
    "    df['mov_hposepitch'] = df['pose_Rx']\n",
    "    df['mov_hposeyaw'] = df['pose_Ry']\n",
    "    df['mov_hposeroll'] = df['pose_Rz']\n",
    "    \n",
    "    # Calculate angular head movement using diff for pose_Rx, pose_Ry, pose_Rz, then take Euclidean norm\n",
    "    df['mov_hposedist'] = np.sqrt(df[['pose_Rx', 'pose_Ry', 'pose_Rz']].diff().fillna(0).pow(2).sum(axis=1))\n",
    "    \n",
    "    # Calculate mean and std for the new variables\n",
    "    output_dict = {}\n",
    "    variables = ['mov_headvel', 'mov_hposepitch', 'mov_hposeyaw', 'mov_hposeroll', 'mov_hposedist']\n",
    "    for var in variables:\n",
    "        output_dict[f\"{var}_mean\"] = df[var].mean()\n",
    "        output_dict[f\"{var}_std\"] = df[var].std()\n",
    "    \n",
    "    # Create output DataFrame from the output_dict\n",
    "    output_df = pd.DataFrame([output_dict])\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Variables for Head Movement!\n",
    "\n",
    "openface_hm_derived_dict_list = apply_function_to_dict_list(openface_extras_radius_dict, process_head_movement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENFACE - Averaging across time windows!\n",
    "\n",
    "openface_hm_derived_dict = average_inner_dfs(openface_hm_derived_dict_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye Gaze Directionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gaze_data(df):\n",
    "    # Ensure all gaze-related columns are floats\n",
    "    gaze_cols = ['gaze_0_x', 'gaze_0_y', 'gaze_0_z', 'gaze_1_x', 'gaze_1_y', 'gaze_1_z']\n",
    "    df[gaze_cols] = df[gaze_cols].astype(float)\n",
    "    \n",
    "    # Initialize output dictionary\n",
    "    output_dict = {}\n",
    "    \n",
    "    # Mapping for renaming\n",
    "    rename_map = {\n",
    "        'gaze_0_x': 'righteyex', 'gaze_0_y': 'righteyey', 'gaze_0_z': 'righteyez',\n",
    "        'gaze_1_x': 'lefteyex', 'gaze_1_y': 'lefteyey', 'gaze_1_z': 'lefteyez'\n",
    "    }\n",
    "    \n",
    "    # Calculate mean and std for each gaze direction component and rename\n",
    "    for col in gaze_cols:\n",
    "        new_base_name = rename_map[col]\n",
    "        output_dict[f\"mov_{new_base_name}_mean\"] = df[col].mean()\n",
    "        output_dict[f\"mov_{new_base_name}_std\"] = df[col].std()\n",
    "    \n",
    "    # Calculate Euclidean displacement for each eye in each frame\n",
    "    df['mov_leyedisp'] = np.sqrt((df['gaze_1_x'].diff()**2 + df['gaze_1_y'].diff()**2 + df['gaze_1_z'].diff()**2).fillna(0))\n",
    "    df['mov_reyedisp'] = np.sqrt((df['gaze_0_x'].diff()**2 + df['gaze_0_y'].diff()**2 + df['gaze_0_z'].diff()**2).fillna(0))\n",
    "    \n",
    "    # Add mean and std for the Euclidean displacements to output dict\n",
    "    output_dict['mov_leyedisp_mean'] = df['mov_leyedisp'].mean()\n",
    "    output_dict['mov_leyedisp_std'] = df['mov_leyedisp'].std()\n",
    "    output_dict['mov_reyedisp_mean'] = df['mov_reyedisp'].mean()\n",
    "    output_dict['mov_reyedisp_std'] = df['mov_reyedisp'].std()\n",
    "    \n",
    "    # Create output DataFrame from the output_dict\n",
    "    output_df = pd.DataFrame([output_dict])\n",
    "    \n",
    "    return output_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Variables for Head Movement!\n",
    "\n",
    "openface_eg_derived_dict_list = apply_function_to_dict_list(openface_extras_radius_dict, process_gaze_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "openface_eg_derived_dict = average_inner_dfs(openface_eg_derived_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye Blink Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def compute_ear(row):\n",
    "    # Right eye\n",
    "    try:\n",
    "        d1 = euclidean((row['eye_lmk_X_10'], row['eye_lmk_Y_10'], row['eye_lmk_Z_10']), (row['eye_lmk_X_18'], row['eye_lmk_Y_18'], row['eye_lmk_Z_18']))\n",
    "        d2 = euclidean((row['eye_lmk_X_12'], row['eye_lmk_Y_12'], row['eye_lmk_Z_12']), (row['eye_lmk_X_16'], row['eye_lmk_Y_16'], row['eye_lmk_Z_16']))\n",
    "        d3 = euclidean((row['eye_lmk_X_8'], row['eye_lmk_Y_8'], row['eye_lmk_Z_8']), (row['eye_lmk_X_14'], row['eye_lmk_Y_14'], row['eye_lmk_Z_14']))\n",
    "        right_ear = (d1 + d2) / (2.0 * d3)\n",
    "    except:\n",
    "        right_ear = 0 # Need some default value if the lmk values are infinity or 0\n",
    "        \n",
    "    # Left eye\n",
    "    try:\n",
    "        d4 = euclidean((row['eye_lmk_X_38'], row['eye_lmk_Y_38'], row['eye_lmk_Z_38']), (row['eye_lmk_X_46'], row['eye_lmk_Y_46'], row['eye_lmk_Z_46']))\n",
    "        d5 = euclidean((row['eye_lmk_X_40'], row['eye_lmk_Y_40'], row['eye_lmk_Z_40']), (row['eye_lmk_X_44'], row['eye_lmk_Y_44'], row['eye_lmk_Z_44']))\n",
    "        d6 = euclidean((row['eye_lmk_X_36'], row['eye_lmk_Y_36'], row['eye_lmk_Z_36']), (row['eye_lmk_X_42'], row['eye_lmk_Y_42'], row['eye_lmk_Z_42']))\n",
    "        left_ear = (d4 + d5) / (2.0 * d6)\n",
    "    except:\n",
    "        left_ear = 0 # Need some default value if the lmk values are infinity or 0\n",
    "    \n",
    "    # Overall EAR\n",
    "    return (right_ear + left_ear) / 2.0\n",
    "\n",
    "def ebb_process_video_df(df):\n",
    "    \n",
    "    # Calculate EAR for each frame\n",
    "    df['EAR'] = df.apply(compute_ear, axis=1)\n",
    "    \n",
    "    # Identify frames where a blink occurs\n",
    "    df['is_blink'] = (df['EAR'] < 0.2) & (df['EAR'].shift(1) >= 0.2)\n",
    "    \n",
    "    # For each blink, find the timestamp difference to the previous blink\n",
    "    blink_timestamps = df[df['is_blink']]['timestamp']\n",
    "    mov_blinkdur = blink_timestamps.diff().fillna(0)  # This calculates the time between blinks\n",
    "    \n",
    "    # Convert to float\n",
    "    try:\n",
    "        mov_blinkdur = mov_blinkdur.apply(lambda x: x.total_seconds() if isinstance(x, pd.Timedelta) else x).astype(float)\n",
    "\n",
    "        # Filter out blink durations over 10 seconds\n",
    "        mov_blinkdur.loc[mov_blinkdur < 10]\n",
    "    except:\n",
    "        mov_blinkdur = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Initialize all values at zero\n",
    "    features = {\n",
    "        'mov_blink_ear_mean': 0,\n",
    "        'mov_blink_ear_std': 0,\n",
    "        'mov_blink_count': 0,\n",
    "        'mov_blinkdur_mean': 0,\n",
    "        'mov_blinkdur_std': 0\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if df['is_blink'].sum() > 0:\n",
    "        # Calculate requested features\n",
    "        blink_ear_values = df[df['is_blink']]['EAR']\n",
    "        features = {\n",
    "            'mov_blink_ear_mean': blink_ear_values.mean(),\n",
    "            'mov_blink_ear_std': 0 if np.isnan(blink_ear_values.std()) else blink_ear_values.std(),\n",
    "            'mov_blink_count': df['is_blink'].sum(),\n",
    "            'mov_blinkdur_mean': mov_blinkdur.mean(),\n",
    "            'mov_blinkdur_std': 0 if np.isnan(mov_blinkdur.std()) else mov_blinkdur.std(),\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame([features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Derived Variables for Eye Blink Behavior!\n",
    "\n",
    "openface_ebb_derived_dict_list = apply_function_to_dict_list(openface_extras_radius_dict, ebb_process_video_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openface_ebb_derived_dict = average_inner_dfs(openface_ebb_derived_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fl_process_video_df(df):\n",
    "    # Preparing the column names for X, Y, Z coordinates\n",
    "    x_cols = [f'X_{i}' for i in range(68)]\n",
    "    y_cols = [f'Y_{i}' for i in range(68)]\n",
    "    z_cols = [f'Z_{i}' for i in range(68)]\n",
    "    \n",
    "    # Calculating the displacement for each landmark across frames\n",
    "    disp_cols = []\n",
    "    for x_col, y_col, z_col in zip(x_cols, y_cols, z_cols):\n",
    "        disp_col = f'{x_col}_disp'\n",
    "        df[disp_col] = np.sqrt((df[x_col].diff() ** 2) + (df[y_col].diff() ** 2) + (df[z_col].diff() ** 2))\n",
    "        disp_cols.append(disp_col)\n",
    "    \n",
    "    # Calculating the mean and standard deviation of displacements for each landmark\n",
    "    output_df = pd.DataFrame()\n",
    "    for col in disp_cols:\n",
    "        landmark_num = col.split('_')[1]\n",
    "        output_df[f'fac_lmk{landmark_num}disp_mean'] = [df[col].mean()]\n",
    "        output_df[f'fac_lmk{landmark_num}disp_std'] = [df[col].std()]\n",
    "    \n",
    "    # Return a DataFrame with calculated mean and standard deviation for each landmark displacement\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Variables for Facial Landmark!\n",
    "\n",
    "openface_fl_derived_dict_list = apply_function_to_dict_list(openface_extras_radius_dict, fl_process_video_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENFACE - Averaging across time windows!\n",
    "\n",
    "openface_fl_derived_dict = average_inner_dfs(openface_fl_derived_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Tremor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fac_tremor(df, window_size=5):\n",
    "    # Pad the DataFrame at the beginning and end to handle edge cases\n",
    "    df_padded = pd.concat([df.iloc[:window_size-1].copy(), df, df.iloc[-window_size+1:].copy()]).reset_index(drop=True)\n",
    "    \n",
    "    # Initialize a DataFrame to hold the median tremor values for each landmark\n",
    "    tremor_medians = pd.DataFrame()\n",
    "    \n",
    "    for i in range(68):  # For each landmark\n",
    "        # Prepare column names\n",
    "        x_col = f'X_{i}'\n",
    "        y_col = f'Y_{i}'\n",
    "        z_col = f'Z_{i}'\n",
    "        \n",
    "        # Calculate rolling mean positions\n",
    "        rolling_means = df_padded[[x_col, y_col, z_col]].rolling(window=window_size, center=True).mean()\n",
    "        \n",
    "        # Calculate Euclidean distance from each frame's position to the rolling mean position\n",
    "        distances = np.sqrt((df_padded[x_col] - rolling_means[x_col])**2 + \n",
    "                            (df_padded[y_col] - rolling_means[y_col])**2 + \n",
    "                            (df_padded[z_col] - rolling_means[z_col])**2)\n",
    "        \n",
    "        # Calculate median of distances for each window\n",
    "        tremor_median = distances.rolling(window=window_size, center=True).median()\n",
    "        \n",
    "        # Append the median tremor value for the landmark to the tremor_medians DataFrame\n",
    "        tremor_medians[f'fac_tremor_median_{i+1:02d}'] = tremor_median\n",
    "        \n",
    "    # Calculate the mean of median tremors across all frames for each landmark\n",
    "    output_df = tremor_medians.mean().rename(lambda x: f'{x}_mean').to_frame().transpose()\n",
    "    \n",
    "    # Adjust the DataFrame to start from the original index\n",
    "    output_df.index = [0]\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Variables for Facial Tremor!\n",
    "\n",
    "openface_ft_derived_dict_list = apply_function_to_dict_list(openface_extras_radius_dict, calculate_fac_tremor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENFACE - Averaging across time windows!\n",
    "\n",
    "openface_ft_derived_dict = average_inner_dfs(openface_ft_derived_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pain Expressivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pain_expressivity(df):\n",
    "    # Calculate fac_paiintsoft for each frame\n",
    "    soft_columns = [\"AU04_r\", \"AU06_r\", \"AU07_r\", \"AU09_r\", \"AU10_r\", \"AU12_r\", \"AU20_r\", \"AU26_r\"]\n",
    "    df['fac_paiintsoft'] = df[soft_columns].mean(axis=1) / 5\n",
    "    \n",
    "    # Calculate fac_paiinthard for each frame\n",
    "    hard_columns = [\"AU04_c\", \"AU06_c\", \"AU07_c\", \"AU09_c\", \"AU10_c\", \"AU12_c\", \"AU20_c\", \"AU26_c\"]\n",
    "    df['fac_paiinthard'] = np.where(df[hard_columns].min(axis=1) > 0, df['fac_paiintsoft'], 0)\n",
    "    \n",
    "    # Calculate overall features\n",
    "    results = {\n",
    "        'fac_paiintsoft_pct': (df[hard_columns] > 0).any(axis=1).mean(),\n",
    "        'fac_paiintsoft_mean': df['fac_paiintsoft'].mean(),\n",
    "        'fac_paiintsoft_std': df['fac_paiintsoft'].std(),\n",
    "        'fac_paiinthard_mean': df['fac_paiinthard'].mean(),\n",
    "        'fac_paiinthard_std': df['fac_paiinthard'].std()\n",
    "    }\n",
    "\n",
    "    # Ensure no NaNs - replace NaNs with 0 for aggregation metrics\n",
    "    results = {k: 0 if pd.isna(v) else v for k, v in results.items()}\n",
    "\n",
    "    # Return results as a DataFrame\n",
    "    return pd.DataFrame([results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Variables for Pain Expressivity!\n",
    "\n",
    "openface_pe_derived_dict_list = apply_function_to_dict_list(openface_radius_dict, calculate_pain_expressivity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENFACE - Averaging across time windows!\n",
    "\n",
    "openface_pe_derived_dict = average_inner_dfs(openface_pe_derived_dict_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkcRVBK7CZaJ"
   },
   "source": [
    "# Make Vectors for Each Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFkY8uARYsGL"
   },
   "source": [
    "## Vectors for AU and emotion classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1afeC95tCzgK"
   },
   "outputs": [],
   "source": [
    "## Dictionary of list of relevant dictionaries\n",
    "openface_dict_list_dict = {}\n",
    "\n",
    "for time_split in openface_au_derived_dict.keys():\n",
    "  openface_dict_list_dict[time_split] = {}\n",
    "  for time_window in openface_au_derived_dict[time_split].keys():\n",
    "      openface_dict_list_dict[time_split][time_window] = [ openface_au_derived_dict[time_split][time_window], openface_emo_stats_dict[time_split][time_window], \n",
    "                                                          openface_ee_derived_dict[time_split][time_window], openface_oe_derived_dict[time_split][time_window], \n",
    "                                                          openface_hm_derived_dict[time_split][time_window], openface_eg_derived_dict[time_split][time_window],\n",
    "                                                          openface_ebb_derived_dict[time_split][time_window], openface_fl_derived_dict[time_split][time_window], \n",
    "                                                          openface_ft_derived_dict[time_split][time_window], openface_pe_derived_dict[time_split][time_window] ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xa9VoJpoIz1A"
   },
   "outputs": [],
   "source": [
    "## Dictionary of list of relevant dictionaries\n",
    "opengraphau_dict_list_dict = {}\n",
    "\n",
    "for time_split in opengraphau_au_derived_dict.keys():\n",
    "  opengraphau_dict_list_dict[time_split] = {}\n",
    "  for time_window in opengraphau_au_derived_dict[time_split].keys():\n",
    "      opengraphau_dict_list_dict[time_split][time_window] = [ opengraphau_au_derived_dict[time_split][time_window], opengraphau_ee_derived_dict[time_split][time_window] ]\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eG7PC1CcGoRp"
   },
   "outputs": [],
   "source": [
    "## Dictionary of list of relevant dictionaries\n",
    "hsemotion_dict_list_dict = {}\n",
    "\n",
    "for time_split in hsemotion_emo_stats_dict.keys():\n",
    "  hsemotion_dict_list_dict[time_split] = {}\n",
    "  for time_window in hsemotion_emo_stats_dict[time_split].keys():\n",
    "      hsemotion_dict_list_dict[time_split][time_window] = [ hsemotion_emo_stats_dict[time_split][time_window], hsemotion_ee_derived_dict[time_split][time_window] ]\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp: Vectors for AU and emotion classifiers, 5 min averaging only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify code below so that it can loop through multiple averaging time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opengraphau_dict_list_dict = opengraphau_dict_list_dict[5]\n",
    "openface_dict_list_dict = openface_dict_list_dict[5]\n",
    "hsemotion_dict_list_dict = hsemotion_dict_list_dict[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4DmMSlLa_GC"
   },
   "outputs": [],
   "source": [
    "def partial_combine_dictionaries(dict1, dict2):\n",
    "    # Takes element one (i.e. the AU matrix) from dict1, and all of dict2 (i.e. HSEmotion)\n",
    "    combined_dict = {}\n",
    "\n",
    "    for key in dict1:\n",
    "        combined_dict[key] = [dict1[key][0]] + dict2[key]\n",
    "\n",
    "    return combined_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GNy0xOKbEBj"
   },
   "outputs": [],
   "source": [
    "ogauhsemotion_dict_list_dict = partial_combine_dictionaries(opengraphau_dict_list_dict, hsemotion_dict_list_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRuy_1Y9JAqd"
   },
   "outputs": [],
   "source": [
    "# SAVE VARIABLES - EMOTION & AFFECT\n",
    "\n",
    "save_var(openface_dict_list_dict, forced_name=f'openface_dict_list_dict_{PAT_SHORT_NAME}')\n",
    "\n",
    "save_var(opengraphau_dict_list_dict, forced_name=f'opengraphau_dict_list_dict_{PAT_SHORT_NAME}')\n",
    "\n",
    "save_var(hsemotion_dict_list_dict, forced_name=f'hsemotion_dict_list_dict_{PAT_SHORT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gASR49RVbmwj"
   },
   "outputs": [],
   "source": [
    "# SAVE VARIABLES - EMOTION & AFFECT\n",
    "\n",
    "save_var(ogauhsemotion_dict_list_dict, forced_name=f'ogauhsemotion_dict_list_dict_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmHYCC9OBxzq"
   },
   "outputs": [],
   "source": [
    "# LOAD VARIABLES - EMOTION & AFFECT\n",
    "\n",
    "openface_dict_list_dict = load_var(f'openface_dict_list_dict_{PAT_SHORT_NAME}')\n",
    "\n",
    "opengraphau_dict_list_dict = load_var(f'opengraphau_dict_list_dict_{PAT_SHORT_NAME}')\n",
    "\n",
    "hsemotion_dict_list_dict = load_var(f'hsemotion_dict_list_dict_{PAT_SHORT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enGGAIcNbqmo"
   },
   "outputs": [],
   "source": [
    "# LOAD VARIABLES - EMOTION & AFFECT\n",
    "\n",
    "ogauhsemotion_dict_list_dict = load_var(f'ogauhsemotion_dict_list_dict_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcZea69MJ9Lw"
   },
   "outputs": [],
   "source": [
    "def flatten_dataframes_dict(dataframes_list):\n",
    "    # Initialize an empty dictionary to store the flattened data for each key\n",
    "    flattened_data_dict = {}\n",
    "\n",
    "    # Define the columns to ignore\n",
    "    ignore_columns = ['success', 'timestamp', 'AU', 'emotion']\n",
    "\n",
    "    for dataframes_dict in dataframes_list:\n",
    "       for key, df in dataframes_dict.items():\n",
    "          # Filter out the columns to be ignored\n",
    "          filtered_df = df.drop(columns=[col for col in ignore_columns if col in df.columns])\n",
    "\n",
    "          # Flatten the data by converting each DataFrame into a 1D array\n",
    "          flattened_array = filtered_df.select_dtypes(include=[np.number, int, float, complex, \\\n",
    "                                                                pd.Int64Dtype(), pd.Float64Dtype(), pd.Int32Dtype(), \\\n",
    "                                                                pd.Float32Dtype()]).values.flatten()\n",
    "\n",
    "          # Convert the flattened array to NumPy array and store it in the dictionary\n",
    "          if key in flattened_data_dict:\n",
    "              flattened_data_dict[key] = np.concatenate((flattened_data_dict[key], flattened_array))\n",
    "          else:\n",
    "              flattened_data_dict[key] = np.array(flattened_array)\n",
    "\n",
    "    return flattened_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_kZiz0dKDcm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "openface_vectors_dict = {}\n",
    "\n",
    "for key, openface_dict_list_now in openface_dict_list_dict.items():\n",
    "  openface_vectors_dict[key] = flatten_dataframes_dict(openface_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2P1ntu5JF9A"
   },
   "outputs": [],
   "source": [
    "opengraphau_vectors_dict = {}\n",
    "\n",
    "for key, opengraphau_dict_list_now in opengraphau_dict_list_dict.items():\n",
    "  opengraphau_vectors_dict[key] = flatten_dataframes_dict(opengraphau_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEBJtszEJGPD"
   },
   "outputs": [],
   "source": [
    "hsemotion_vectors_dict = {}\n",
    "\n",
    "for key, hsemotion_dict_list_now in hsemotion_dict_list_dict.items():\n",
    "  hsemotion_vectors_dict[key] = flatten_dataframes_dict(hsemotion_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QhobZ1wbTL2"
   },
   "outputs": [],
   "source": [
    "ogauhsemotion_vectors_dict = {}\n",
    "\n",
    "for key, ogauhsemotion_dict_list_now in ogauhsemotion_dict_list_dict.items():\n",
    "  ogauhsemotion_vectors_dict[key] = flatten_dataframes_dict(ogauhsemotion_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeN1ukKDLHVo"
   },
   "outputs": [],
   "source": [
    "openface_vectors_dict['60'][get_moodTracking_datetime(0, df_moodTracking=df_moodTracking)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L60VuGulO29n"
   },
   "outputs": [],
   "source": [
    "opengraphau_vectors_dict['60'][get_moodTracking_datetime(0, df_moodTracking=df_moodTracking)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKBfxsR8O4OI"
   },
   "outputs": [],
   "source": [
    "hsemotion_vectors_dict['60'][get_moodTracking_datetime(0, df_moodTracking=df_moodTracking)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuRt5B5GbfOi"
   },
   "outputs": [],
   "source": [
    "ogauhsemotion_vectors_dict['60'][get_moodTracking_datetime(0, df_moodTracking=df_moodTracking)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9apyQQMfIso"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gx6_bCNQV6uD"
   },
   "source": [
    "## Labels - datetime conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hRicOaOQVtq"
   },
   "outputs": [],
   "source": [
    "def ts_to_str(timestamp):\n",
    "    return timestamp.strftime('%-m/%-d/%Y %H:%M:%S')\n",
    "\n",
    "def str_to_ts(string_now):\n",
    "  temp_var = pd.to_datetime(pd.to_datetime(string_now).strftime('%d-%b-%Y %H:%M:%S'))\n",
    "  return pd.Timestamp(temp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lwF8VpTv-oC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLtMi6TUvm9K"
   },
   "source": [
    "## Save to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD95XHYFxLl9"
   },
   "outputs": [],
   "source": [
    "!pip install xlsxwriter -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Es85PqYzSht"
   },
   "outputs": [],
   "source": [
    "def ts_to_str_save(timestamp):\n",
    "    # shorter version bc xlsxwriter sheet name char limit\n",
    "    return timestamp.strftime('%-m_%-d %H_%M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntLtIcQow-03"
   },
   "outputs": [],
   "source": [
    "## Save our vectors to excel sheets!\n",
    "\n",
    "def get_dict_name(dictionary):\n",
    "    namespace = globals()\n",
    "    for name, obj in namespace.items():\n",
    "        if isinstance(obj, dict) and obj is dictionary:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def save_dicts_to_excel(dict_list, output_path):\n",
    "  # Create an Excel writer object\n",
    "  writer = pd.ExcelWriter(output_path, engine='xlsxwriter')\n",
    "\n",
    "  # Iterate over the keys in the dictionaries\n",
    "  for key in dict_list[0].keys():\n",
    "      # Write each dataframe to a separate sheet with the corresponding key as the sheet name\n",
    "      for enum, dict_now in enumerate(dict_list):\n",
    "        name_var = f'Matrix_{enum}'\n",
    "        sheet_name_starter = f'{ts_to_str_save(key)}_{name_var}'\n",
    "        dict_now[key].to_excel(writer, sheet_name=sheet_name_starter[:31])\n",
    "\n",
    "  # Save the Excel file\n",
    "  writer.close()\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtF5RndQv9qj"
   },
   "source": [
    "### Multi-hour features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-zpJEZSyU2S"
   },
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(FEATURE_VIS_PATH, exist_ok=True)\n",
    "\n",
    "for i in opengraphau_dict_list_dict.keys():\n",
    "  save_dicts_to_excel(openface_dict_list_dict[i], FEATURE_VIS_PATH + f'openface_{PAT_SHORT_NAME}_{int(i)}_minutes.xlsx')\n",
    "  save_dicts_to_excel(opengraphau_dict_list_dict[i], FEATURE_VIS_PATH + f'opengraphau_{PAT_SHORT_NAME}_{int(i)}_minutes.xlsx')\n",
    "  save_dicts_to_excel(hsemotion_dict_list_dict[i], FEATURE_VIS_PATH + f'hsemotion_{PAT_SHORT_NAME}_{int(i)}_minutes.xlsx')\n",
    "  save_dicts_to_excel(ogauhsemotion_dict_list_dict[i], FEATURE_VIS_PATH + f'ogauhse_{PAT_SHORT_NAME}_{int(i)}_minutes.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9v5twoysUh12"
   },
   "source": [
    "# Linear Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t1JM3aZYVaF"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QiQpmItUj02"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(x=5):\n",
    "  np.random.seed(x)\n",
    "  random.seed(x)\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9pC29y3V5vc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "def linRegOneMetric(vectors_dict, y, randShuffle=False, do_lasso=False, do_ridge=False, alpha=1.0):\n",
    "  # runs simple linear regression via one-left-out\n",
    "  # vectors_dict -- dictionary mapping time radius (in minutes) to features\n",
    "  # y -- a numpy array with labels (self-reported metrics)\n",
    "  # randShuffle -- do we shuffle the self-report labels?\n",
    "  # if do_lasso, does lasso regression\n",
    "  # if do_ridge, does ridge regression. Overrides do_lasso\n",
    "  # alpha - this is the weighting of either lasso or ridge\n",
    "\n",
    "  # returns a dictionary with several results:\n",
    "  # scores -- dictionary mapping each time radius to list of MSEs from each one-left-out\n",
    "  # preds -- dictionary mapping each time radius to a list of each one-left-out model's prediction\n",
    "  # y -- returns y again for convenience\n",
    "  # models -- dictionary mapping each time radius to a list of each one-left-out trained model (simple linear regression)\n",
    "\n",
    "  scores = {}\n",
    "  preds = {}\n",
    "  models = {}\n",
    "\n",
    "  if randShuffle:\n",
    "    y_using = np.random.permutation(y)\n",
    "  else:\n",
    "    y_using = y\n",
    "\n",
    "  for i in vectors_dict.keys():\n",
    "    model = LinearRegression()\n",
    "    if do_lasso:\n",
    "      model = Lasso(alpha=alpha)\n",
    "    if do_ridge:\n",
    "      model = Ridge(alpha=alpha)\n",
    "\n",
    "    # Compute MSEs via scikitlearn cross_val_score\n",
    "    scores_temp = cross_val_score(model, vectors_dict[i], y_using, cv=vectors_dict[i].shape[0], scoring='neg_mean_squared_error')\n",
    "    scores[i] = -1 * scores_temp\n",
    "\n",
    "    # Predictions via cross_val_predict\n",
    "    preds[i] = cross_val_predict(model, vectors_dict[i], y_using, cv=vectors_dict[i].shape[0])\n",
    "\n",
    "    # Now we need to iterate through and actually save the models themselves, since cross_val_score doesn't let us do that!\n",
    "    models_i_building = []\n",
    "    for test_index in range(vectors_dict[i].shape[0]):\n",
    "\n",
    "      X_train = np.delete(vectors_dict[i], test_index, axis=0)\n",
    "\n",
    "      y_train = np.delete(y_using, test_index, axis=0)\n",
    "\n",
    "      model = LinearRegression()\n",
    "      if do_lasso:\n",
    "        model = Lasso(alpha=alpha)\n",
    "      if do_ridge:\n",
    "        model = Ridge(alpha=alpha)\n",
    "      model.fit(X_train, y_train)\n",
    "      models_i_building.append(model)\n",
    "\n",
    "    models[i] = models_i_building\n",
    "\n",
    "  return scores, preds, y, models\n",
    "\n",
    "\n",
    "\n",
    "def plot_predictions(y, y_pred, randShuffleR=None, ax=None, time_rad=None, metric=None):\n",
    "    # Makes one scatterplot with Pearson's R and p value on it\n",
    "    # give it the randShuffle Pearson's R\n",
    "    # if you want to display that on the plot\n",
    "\n",
    "    # Compute Pearson's R\n",
    "    pearson_corr, p_val = pearsonr(y, y_pred)\n",
    "\n",
    "    # Create the scatter plot on the specified axes\n",
    "    if ax is None:\n",
    "        ax_original = None\n",
    "        fig, ax = plt.subplots()\n",
    "        # adjust fonts!\n",
    "        text_font = 16\n",
    "    else:\n",
    "        ax_original = ax\n",
    "        text_font = 16\n",
    "\n",
    "\n",
    "    ax.scatter(y, y_pred, label='Predicted vs. True', s=24)\n",
    "\n",
    "\n",
    "\n",
    "    # Add the correlation coefficient and p-value on the plot\n",
    "    ax.text(0.05, 0.90, f'Pearson\\'s R: {pearson_corr:.2f}', transform=ax.transAxes, fontsize=text_font)\n",
    "    ax.text(0.05, 0.80, f'P Value: {p_val:.2f}', transform=ax.transAxes, fontsize=text_font)\n",
    "    if not(randShuffleR is None):\n",
    "      ax.text(0.05, 0.70, f'Random Shuffle R: {randShuffleR:.2f}', transform=ax.transAxes, fontsize=text_font)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Self-Reported Scores', fontsize=17)\n",
    "    ax.set_ylabel('Predicted Scores', fontsize=17)\n",
    "\n",
    "    if metric is None:\n",
    "      title_starter = 'Predicted vs. True'\n",
    "    else:\n",
    "      title_starter = metric\n",
    "\n",
    "    if time_rad is None:\n",
    "      ax.set_title(f'{title_starter} Scores', fontsize=17)\n",
    "    else:\n",
    "      num_hrs = int(time_rad) / 60\n",
    "      if num_hrs > 1:\n",
    "        ax.set_title(f'{title_starter}, Time Window = {num_hrs} Hours', fontsize=15)\n",
    "      else:\n",
    "        ax.set_title(f'{title_starter}, Time Window = {num_hrs} Hour', fontsize=15)\n",
    "\n",
    "\n",
    "    # Add the line of best fit\n",
    "    sns.regplot(x=y, y=y_pred, ax=ax, line_kws={'color': 'red', 'linestyle': '--'}, label='Line of Best Fit')\n",
    "\n",
    "    # Add the shaded region for the 95% confidence interval\n",
    "    #sns.regplot(x=y, y=y_pred, ax=ax, scatter=False, ci=95, color='gray', label='95% Confidence Interval')\n",
    "\n",
    "    # Adjust the font size of the tick labels on the axes\n",
    "    ax.tick_params(axis='both', labelsize=18)\n",
    "\n",
    "    ax.set_adjustable('box')\n",
    "\n",
    "    #set aspect ratio to 1\n",
    "    ratio = 1.0\n",
    "    x_left, x_right = ax.get_xlim()\n",
    "    y_low, y_high = ax.get_ylim()\n",
    "    ax.set_aspect(abs((x_right-x_left)/(y_low-y_high))*ratio)\n",
    "\n",
    "    if ax_original is None:\n",
    "        plt.show()\n",
    "        return pearson_corr, p_val, fig\n",
    "    else:\n",
    "        return pearson_corr, p_val\n",
    "\n",
    "\n",
    "\n",
    "def plot_scatterplots(preds_dict, y, overall_title, savepath, randShuffleR=None):\n",
    "\n",
    "    plt.rcParams['lines.markersize'] = 6\n",
    "    subplot_title_font = 16\n",
    "    full_title_font = 24\n",
    "\n",
    "    num_plots = len(list(preds_dict.keys()))\n",
    "    num_cols = 4\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols\n",
    "\n",
    "    r_list = []\n",
    "    p_list = []\n",
    "\n",
    "    # Calculate the desired figure size for larger plot\n",
    "    figsize = (28, 12)\n",
    "\n",
    "    # Create subplots with auto aspect ratio\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "\n",
    "    #axes.set_adjustable('box')\n",
    "\n",
    "    if num_rows == 1:\n",
    "      axes = axes.reshape((1, num_cols))\n",
    "\n",
    "    # Flatten the axes array if necessary\n",
    "    if num_plots == 1:\n",
    "        fig, axes = plt.subplots(1, 1, figsize=figsize)\n",
    "        axes = np.array([axes]).reshape(1, 1)\n",
    "\n",
    "\n",
    "    # Loop through the dictionaries\n",
    "    for i, (key, y_preds) in enumerate(preds_dict.items()):\n",
    "        y_list = np.array(y).astype(float)\n",
    "        y_pred = np.array(y_preds).astype(float)\n",
    "        #y_pred = np.array([i[0] for i in y_pred])\n",
    "\n",
    "        # Get the subplot coordinates\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "\n",
    "        # Plot predictions on the subplot\n",
    "        if randShuffleR is None:\n",
    "          pearson_corr, p_val = plot_predictions(y_list, y_pred, randShuffleR=randShuffleR, ax=axes[row, col])\n",
    "        else:\n",
    "          pearson_corr, p_val = plot_predictions(y_list, y_pred, randShuffleR=randShuffleR[i], ax=axes[row, col])\n",
    "        r_list.append(pearson_corr)\n",
    "        p_list.append(p_val)\n",
    "\n",
    "        num_hrs = int(key) / 60\n",
    "        if num_hrs > 1:\n",
    "          axes[row, col].set_title(f'Time Window = {num_hrs} Hours', fontsize=subplot_title_font)\n",
    "        else:\n",
    "          axes[row, col].set_title(f'Time Window = {num_hrs} Hour', fontsize=subplot_title_font)\n",
    "        #axes[row, col].set_aspect('equal')\n",
    "\n",
    "        # Remove x-axis and y-axis labels from subplots\n",
    "        axes[row, col].set_xlabel('')\n",
    "        axes[row, col].set_ylabel('')\n",
    "\n",
    "        #axes[row, col].set_adjustable('box')\n",
    "\n",
    "    # Add overall title\n",
    "    fig.suptitle(overall_title, fontsize=30, y=1)\n",
    "\n",
    "    # Set shared x-axis and y-axis labels\n",
    "    fig.text(0.5, 0.00, 'Self-Reported Scores', ha='center', fontsize=full_title_font)\n",
    "    fig.text(-0.01, 0.5, 'Predicted Scores', va='center', rotation='vertical', fontsize=full_title_font)\n",
    "\n",
    "    # Adjust spacing and layout\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(savepath, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return r_list, p_list, fig\n",
    "\n",
    "\n",
    "def make_mse_boxplot(scores, metric, savepath, ax=None, method_now='OpenFace'):\n",
    "    # scores -- dictionary that maps time radius (mins) to list of MSEs from one-left-out\n",
    "    # metric - e.g. Mood or Anxiety\n",
    "\n",
    "    # Combine the data into a single array\n",
    "    data = [MSE_list for MSE_list in list(scores.values())]\n",
    "\n",
    "    # Set the font sizes\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        fig = None\n",
    "\n",
    "    # Create a box plot of the data\n",
    "    labels_now = [f'{int(key) / 60}' for key in scores.keys()]\n",
    "\n",
    "    ax.boxplot(data, labels=labels_now, showmeans=True, meanprops={'marker': 'o', 'markerfacecolor': 'red', 'markersize': 10})\n",
    "\n",
    "    # Determine the highest 75th percentile value among the four entries\n",
    "    max_value = np.max([np.percentile(entry, 75) for entry in data])\n",
    "\n",
    "    # Set the y-axis range conditionally\n",
    "    if max_value > 100:\n",
    "        ax.set_ylim(0, 100)\n",
    "    else:\n",
    "        ax.set_ylim(0, max_value)\n",
    "\n",
    "    # Set the labels and title\n",
    "    ax.set_xlabel('Time Window (Hours)')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_title(f'{metric} Prediction via {method_now}', y=1.1)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.savefig(savepath, bbox_inches='tight')\n",
    "\n",
    "    # Show the plot if fig is None\n",
    "    if fig is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig\n",
    "\n",
    "def make_r_barplot(r_list, time_radius_list, metric, savepath, ax=None, method_now='OpenFace'):\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "    x_labels = [f'{int(i) / 60}' for i in time_radius_list]\n",
    "\n",
    "    if ax is None:\n",
    "        original_ax = None\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        original_ax = ax\n",
    "\n",
    "    ax.bar(x_labels, r_list)\n",
    "\n",
    "    # Set the y-axis range\n",
    "    ax.set_ylim(-0.5, 1)\n",
    "\n",
    "    # Set the labels and title\n",
    "    ax.set_xlabel('Time Window (Hours)')\n",
    "    ax.set_ylabel(\"Pearson's R\")\n",
    "    ax.set_title(f'{metric} Prediction via {method_now}', y=1.1)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.savefig(savepath, bbox_inches='tight')\n",
    "\n",
    "    # Show the plot if ax is None\n",
    "    if original_ax is None:\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "\n",
    "def get_label_from_index(index, spreadsheet_path=FEATURE_LABEL_PATH+'openface_0.5_hours.xlsx'):\n",
    "    if 'experimental' in spreadsheet_path:\n",
    "      matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\", \"Matrix_3\", \"Matrix_4\", \"Matrix_5\", \"Matrix_6\", \"Matrix_7\", \"Matrix_8\", \"Matrix_9\"]\n",
    "      row_label_cols = [\"AU\", \"emotion\", \"emotion\", None, None, None, None, None, None, None]\n",
    "    elif 'hsemotion' in spreadsheet_path:\n",
    "      matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
    "      row_label_cols = [\"emotion\", \"emotion\"]\n",
    "    elif 'opengraphau' in spreadsheet_path:\n",
    "      matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
    "      row_label_cols = [\"AU\", \"emotion\"]\n",
    "    elif 'openface' in spreadsheet_path:\n",
    "      matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\", \"Matrix_3\"]\n",
    "      row_label_cols = [\"AU\", \"emotion\", \"emotion\", None]\n",
    "    elif 'ofauhse' in spreadsheet_path:\n",
    "      matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
    "      row_label_cols = [\"AU\", \"emotion\"]\n",
    "    elif 'ogauhse' in spreadsheet_path:\n",
    "      matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\"]\n",
    "      row_label_cols = [\"AU\", \"emotion\", \"emotion\"]\n",
    "    elif 'all' in spreadsheet_path:\n",
    "      matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\", \"Matrix_3\", \"Matrix_4\", \"Matrix_5\"]\n",
    "      row_label_cols = [\"AU\", \"emotion\", \"emotion\", None, \"AU\", \"emotion\"]\n",
    "    else:\n",
    "      print('BUG IN THE CODE! CHECK get_label_from_index')\n",
    "      print('spreadsheet path is ', spreadsheet_path)\n",
    "\n",
    "\n",
    "    xls = pd.ExcelFile(spreadsheet_path)\n",
    "\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        # Find the sheet ending with the current matrix name\n",
    "        sheet_name = next((s for s in xls.sheet_names if s.endswith(matrix)), None)\n",
    "        if sheet_name is not None:\n",
    "            # Load the sheet into a DataFrame, with the first row as column names\n",
    "            df = pd.read_excel(spreadsheet_path, sheet_name=sheet_name, header=0)\n",
    "\n",
    "            # Get the column labels from the DataFrame\n",
    "            col_labels = [col_now for col_now in df.columns.tolist() if not(col_now in [\"AU\", \"emotion\", \"Unnamed: 0\"])]\n",
    "\n",
    "            if not row_label_cols[i] == 'AU':\n",
    "                if 'emotion' in df.columns:\n",
    "                    row_labels = df['emotion'].tolist()\n",
    "                else:\n",
    "                    row_labels = df['Unnamed: 0'].tolist()\n",
    "            else:\n",
    "                row_labels = df['AU'].tolist()\n",
    "\n",
    "            # Get the numerical entries in the sheet excluding columns \"AU\" and \"emotion\" and \"Unnamed: 0\"\n",
    "            numerical_entries = df.loc[:, ~df.columns.isin([\"AU\", \"emotion\", \"Unnamed: 0\"])].values.flatten()\n",
    "            numerical_entries = numerical_entries[~pd.isnull(numerical_entries)]\n",
    "\n",
    "            # Check if the index is within the range of numerical entries\n",
    "            if index < len(numerical_entries):\n",
    "                # Find the label corresponding to the index\n",
    "                row_index, col_index = divmod(index, len(col_labels))\n",
    "                if row_label_cols[i] == 'AU':\n",
    "                    return f\"AU{row_labels[row_index]} {col_labels[col_index]}\"\n",
    "                else:\n",
    "                    if f'{row_labels[row_index]}' == '0':\n",
    "                        return f\"{col_labels[col_index]}\"\n",
    "                    else:\n",
    "                        return f\"{col_labels[col_index]} {row_labels[row_index]}\"\n",
    "\n",
    "            else:\n",
    "                index -= len(numerical_entries)\n",
    "\n",
    "    # Return None if the index is out of range or no suitable sheets found\n",
    "    print('BUG IN THE CODE! INDEX TOO LARGE! CHECK get_label_from_index')\n",
    "    print('spreadsheet path is ', spreadsheet_path)\n",
    "    return None\n",
    "\n",
    "\n",
    "def getTopFeaturesfromWeights(model_list, spreadsheet_path=FEATURE_LABEL_PATH+'openface_2.0_hours.xlsx'):\n",
    "  # given a list of linear regression models,\n",
    "  # returns their top 5 features (on average) from just weights!\n",
    "\n",
    "  coef_array = [model_now.coef_ for model_now in model_list]\n",
    "  coef_avg = np.mean(coef_array, axis=0)\n",
    "\n",
    "  top_5_features = np.argsort(np.abs(coef_avg))[::-1][:5]\n",
    "\n",
    "  top_5_english = [get_label_from_index(feat_ind, spreadsheet_path=spreadsheet_path) for feat_ind in top_5_features]\n",
    "\n",
    "  return top_5_english\n",
    "\n",
    "\n",
    "def featureAblate(vectors_array, y, do_lasso=False, do_ridge=False):\n",
    "  # runs one-left-out linear regression,\n",
    "  # deleting one feature at a time to determine most important features\n",
    "\n",
    "  # vectors_array -- numpy array of feature vectors\n",
    "  # y -- self-reported labels (e.g. for Mood, Anxiety, or something else)\n",
    "  # if do_lasso, does lasso regression\n",
    "  # if do_ridge, does ridge regression. Overrides do_lasso\n",
    "\n",
    "  # returns scores, prs\n",
    "  # scores -- (n_features, n_timestamps) numpy array of MSEs\n",
    "  # prs -- (n_features,) numpy vector of pearson's R\n",
    "\n",
    "  num_features = vectors_array.shape[1]\n",
    "  num_timestamps = vectors_array.shape[0]\n",
    "\n",
    "  scores = np.zeros((num_features, num_timestamps))\n",
    "  prs = np.zeros((num_features,))\n",
    "\n",
    "  # loop through each feature (for openface, 0 through 144) and delete just that\n",
    "  for deleteNow in range(num_features):\n",
    "    data = np.delete(vectors_array, deleteNow, axis=1)\n",
    "\n",
    "    # make into dictionary to feed into our lin reg function\n",
    "    data = {'placeholder': data}\n",
    "\n",
    "    scores_temp, preds, y, _ = linRegOneMetric(data, y, do_lasso=do_lasso, do_ridge=do_ridge)\n",
    "    scores_temp = scores_temp['placeholder']\n",
    "    preds = preds['placeholder']\n",
    "\n",
    "    # save MSEs\n",
    "    scores[deleteNow, :] =  scores_temp\n",
    "\n",
    "    # compute and save Pearson's R\n",
    "    pearson_corr, _ = pearsonr(y, preds)\n",
    "    prs[deleteNow] = pearson_corr\n",
    "\n",
    "  return scores, prs\n",
    "\n",
    "def featureAblate2D(vectors_array, y):\n",
    "  # runs one-left-out linear regression,\n",
    "  # deleting TWO features at a time to determine most important features\n",
    "\n",
    "  # vectors_array -- numpy array of feature vectors\n",
    "  # y -- self-reported labels (e.g. for Mood, Anxiety, or something else)\n",
    "\n",
    "  # returns prs\n",
    "  # prs -- (n_features, n_features) numpy vector of pearson's R\n",
    "  # Note: ALWAYS index into prs with first index LOWER than second!\n",
    "\n",
    "  num_features = vectors_array.shape[1]\n",
    "\n",
    "  prs = np.zeros((num_features,num_features))\n",
    "\n",
    "  # loop through each feature (for openface, 0 through 144) and delete just that\n",
    "  for deleteNow in range(num_features):\n",
    "    # delete a second one!\n",
    "    for secondDelete in range(deleteNow+1, num_features):\n",
    "      data = np.delete(vectors_array, [deleteNow, secondDelete], axis=1)\n",
    "\n",
    "      # make into dictionary to feed into our lin reg function\n",
    "      data = {'placeholder': data}\n",
    "\n",
    "      _, preds, _, _ = linRegOneMetric(data, y)\n",
    "      preds = preds['placeholder']\n",
    "\n",
    "      # compute and save Pearson's R\n",
    "      pearson_corr, _ = pearsonr(y, preds)\n",
    "      prs[deleteNow, secondDelete] = pearson_corr\n",
    "\n",
    "  return prs\n",
    "\n",
    "def featureAblate3D(vectors_array, y):\n",
    "  # runs one-left-out linear regression,\n",
    "  # deleting THREE features at a time to determine most important features\n",
    "\n",
    "  # vectors_array -- numpy array of feature vectors\n",
    "  # y -- self-reported labels (e.g. for Mood, Anxiety, or something else)\n",
    "\n",
    "  # returns prs\n",
    "  # prs -- (n_features, n_features, n_features) numpy vector of pearson's R\n",
    "  # Note: ALWAYS index into prs with earlier indices LOWER than subsequent ones.\n",
    "\n",
    "  num_features = vectors_array.shape[1]\n",
    "\n",
    "  prs = np.zeros((num_features, num_features, num_features))\n",
    "\n",
    "  # loop through each feature (for openface, 0 through 144) and delete just that\n",
    "  for deleteNow in range(num_features):\n",
    "    # delete a second one!\n",
    "    for secondDelete in range(deleteNow+1, num_features):\n",
    "      # delete a third one!\n",
    "      for thirdDelete in range(secondDelete+1, num_features):\n",
    "        data = np.delete(vectors_array, [deleteNow, secondDelete, thirdDelete], axis=1)\n",
    "\n",
    "        # make into dictionary to feed into our lin reg function\n",
    "        data = {'placeholder': data}\n",
    "\n",
    "        _, preds, _, _ = linRegOneMetric(data, y)\n",
    "        preds = preds['placeholder']\n",
    "\n",
    "        # compute and save Pearson's R\n",
    "        pearson_corr, _ = pearsonr(y, preds)\n",
    "        prs[deleteNow, secondDelete, thirdDelete] = pearson_corr\n",
    "\n",
    "  return prs\n",
    "\n",
    "def plotFeatAbMSEs(feat_ab_scores, original_mse_list, metric, time_radius, savepath, top_n=5, ax=None, spreadsheet_path=FEATURE_LABEL_PATH+'openface_2.0_hours.xlsx'):\n",
    "  # takes feat_ab_scores, a numpy array (n_features, n_timestamps) of MSEs\n",
    "  # outputs box and whisker plot of top_n features for the model\n",
    "\n",
    "  # procedure: get the top_n features with lowest mse averaged across timestamps\n",
    "  # make a box and whisker plot with each feature on x axis and MSEs on y axis\n",
    "  # for x axis labels, convert the index of each feature to english label\n",
    "  # by calling get_label_from_index(feat_ind)\n",
    "\n",
    "\n",
    "  # Get the average MSE across timestamps for each feature\n",
    "  avg_mses = np.mean(feat_ab_scores, axis=1)\n",
    "\n",
    "  # avg MSEs minus original_avg_MSE (make it difference!)\n",
    "  avg_mses = avg_mses - np.mean(original_mse_list)\n",
    "\n",
    "  # Get the indices of the top_n features with the highest difference in MSEs from original\n",
    "  top_indices = np.argsort(avg_mses)[-top_n:]\n",
    "  top_indices = top_indices[::-1]\n",
    "\n",
    "  # Get the English labels for the top_n features\n",
    "  top_labels = [get_label_from_index(ind, spreadsheet_path=spreadsheet_path) for ind in top_indices]\n",
    "\n",
    "  # Get the MSE values for the top_n features\n",
    "  top_mses = feat_ab_scores[top_indices]\n",
    "\n",
    "  # Adjust so it's top mses minus original\n",
    "  original_list_repeated = np.repeat(np.array(original_mse_list).reshape(1, -1), top_n, axis=0)\n",
    "  top_mses = top_mses - original_list_repeated\n",
    "\n",
    "  # Create a box and whisker plot\n",
    "  if ax is None:\n",
    "      original_ax = None\n",
    "      fig, ax = plt.subplots()\n",
    "  else:\n",
    "      original_ax = ax\n",
    "  ax.boxplot(top_mses.T, labels=top_labels, showmeans=True, meanprops={'marker': 'o', 'markerfacecolor': 'red', 'markersize': 10})\n",
    "\n",
    "  # Rotate x-axis labels by 45 degrees\n",
    "  ax.set_xticklabels(top_labels, rotation=45)\n",
    "\n",
    "  # Set the axis labels\n",
    "  ax.set_xlabel('Features')\n",
    "  ax.set_ylabel('Ablated - Original MSEs')\n",
    "\n",
    "  # Set the title\n",
    "  num_hrs = int(time_radius) / 60\n",
    "  if num_hrs > 1:\n",
    "      ax.set_title(f'Top {top_n} Features: {metric}, Time Window = {num_hrs} Hours')\n",
    "  else:\n",
    "      ax.set_title(f'Top {top_n} Features: {metric}, Time Window = {num_hrs} Hour')\n",
    "\n",
    "  plt.savefig(savepath, bbox_inches='tight')\n",
    "\n",
    "  # Show the plot if ax is None\n",
    "  if original_ax is None:\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "  return top_indices, fig\n",
    "\n",
    "def plotFeatAbPRs(feat_ab_prs, original_r_val, metric, time_radius, savepath, top_n=5, ax=None, spreadsheet_path=FEATURE_LABEL_PATH+'openface_2.0_hours.xlsx'):\n",
    "  # takes feat_ab_prs, a numpy array (n_features, ) of Pearson's R vals post-ablation\n",
    "  # outputs bar plot of top_n features TO REMOVE for the model\n",
    "\n",
    "  # procedure: get the top_n features with highest pearson's R\n",
    "  # make a bar plot with each feature on x axis and pearson's R from feat_ab_prs on y axis\n",
    "  # for x axis labels, convert the index of each feature to english label\n",
    "  # by calling get_label_from_index(feat_ind)\n",
    "\n",
    "  # if ax is given, plot on ax. If ax=None, make new fig, ax\n",
    "\n",
    "\n",
    "  # Get the top_n features with highest Pearson's R values\n",
    "  top_features_indices = np.argsort(feat_ab_prs)[-top_n:]\n",
    "  top_features_indices = top_features_indices[::-1]\n",
    "\n",
    "  # Get the labels for the top_n features\n",
    "  top_features_labels = [get_label_from_index(index, spreadsheet_path=spreadsheet_path) for index in top_features_indices]\n",
    "\n",
    "  # Get the corresponding Pearson's R values for the top_n features\n",
    "  top_features_prs = feat_ab_prs[top_features_indices]\n",
    "\n",
    "  # Plot the bar plot\n",
    "  if ax is None:\n",
    "      fig, ax = plt.subplots()\n",
    "  ax.bar(top_features_labels, top_features_prs)\n",
    "\n",
    "  # Rotate x-axis labels by 45 degrees\n",
    "  ax.set_xticklabels(top_features_labels, rotation=45)\n",
    "\n",
    "  # Set plot title and axis labels\n",
    "  # Set the title\n",
    "  num_hrs = int(time_radius) / 60\n",
    "  if num_hrs > 1:\n",
    "      ax.set_title(f'Top {top_n} Features to Remove: {metric}, Time Window = {num_hrs} Hours')\n",
    "  else:\n",
    "      ax.set_title(f'Top {top_n} Features to Remove: {metric}, Time Window = {num_hrs} Hour')\n",
    "\n",
    "  ax.set_xlabel(\"Features\")\n",
    "  ax.set_ylabel(f\"Pearson's R (Original={round(original_r_val, 2)})\")\n",
    "\n",
    "  # Save the plot\n",
    "  plt.savefig(savepath, bbox_inches='tight')\n",
    "\n",
    "  # Show the plot if ax=None\n",
    "  if ax is None:\n",
    "      plt.show()\n",
    "\n",
    "def find_max_indices(array, top_n):\n",
    "    # Flatten the 2D array into a 1D array\n",
    "    flattened_array = array.flatten()\n",
    "\n",
    "    # Find the indices of the top n maximum values in the flattened array\n",
    "    max_indices = np.argsort(flattened_array)[-top_n:][::-1]\n",
    "\n",
    "    # Convert the flattened indices to the corresponding row and column indices in the original array\n",
    "    row_indices, col_indices = np.unravel_index(max_indices, array.shape)\n",
    "\n",
    "    # Combine the row and column indices into pairs\n",
    "    index_combinations = list(zip(row_indices, col_indices))\n",
    "\n",
    "    return index_combinations\n",
    "\n",
    "def plot_feat_scatterplots(vectors_array, y, feat_ind_list, metric, savepath, spreadsheet_path=FEATURE_LABEL_PATH+'openface_2.0_hours.xlsx'):\n",
    "    # for each feature, plot feature on x axis and self-report score on y axis\n",
    "    # vectors_array is the array of feature vectors for ONE time radius\n",
    "    # y - self-reports\n",
    "    # feat_ind_list - list of the indices of the top features\n",
    "    # metric -- e.g. Mood or Anxiety\n",
    "    # savepath - where to save the figure\n",
    "\n",
    "    plt.rcParams['lines.markersize'] = 15\n",
    "\n",
    "    num_plots = len(feat_ind_list)\n",
    "    num_cols = min([len(feat_ind_list), 4])\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols\n",
    "\n",
    "    r_list = []\n",
    "    p_list = []\n",
    "\n",
    "    # Calculate the desired figure size for larger plot\n",
    "    figsize = (28, 12)\n",
    "\n",
    "    # Create subplots with auto aspect ratio\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "\n",
    "    #axes.set_adjustable('box')\n",
    "\n",
    "    if num_rows == 1:\n",
    "      axes = axes.reshape((1, num_cols))\n",
    "\n",
    "    # Flatten the axes array if necessary\n",
    "    if num_plots == 1:\n",
    "        fig, axes = plt.subplots(1, 1, figsize=figsize)\n",
    "        axes = np.array([axes]).reshape(1, 1)\n",
    "\n",
    "\n",
    "    # Loop through the dictionaries\n",
    "    for enum, i in enumerate(feat_ind_list):\n",
    "        x_list = vectors_array[:, i].astype(float)\n",
    "        y_list = np.array(y).astype(float)\n",
    "\n",
    "        #y_pred = np.array([i[0] for i in y_pred])\n",
    "\n",
    "        # Get the subplot coordinates\n",
    "        row = enum // num_cols\n",
    "        col = enum % num_cols\n",
    "\n",
    "        # Plot predictions on the subplot\n",
    "        pearson_corr, p_val = plot_predictions(x_list, y_list, randShuffleR=None, ax=axes[row, col])\n",
    "\n",
    "\n",
    "        axes[row, col].set_title(f'{metric} vs. {get_label_from_index(i, spreadsheet_path=spreadsheet_path)}', fontsize=24)\n",
    "\n",
    "\n",
    "        # Redo x-axis and y-axis labels for subplot\n",
    "        axes[row, col].set_xlabel(get_label_from_index(i, spreadsheet_path=spreadsheet_path), fontsize=24)\n",
    "        axes[row, col].set_ylabel('')\n",
    "\n",
    "        #axes[row, col].set_adjustable('box')\n",
    "\n",
    "    # Add overall title\n",
    "    fig.suptitle(f'Top {num_plots} Features for {metric}', fontsize=30, y=1.05)\n",
    "\n",
    "    # Set shared y-axis label\n",
    "    #fig.text(0.5, 0, f'Self-Reported {metric} Scores', ha='center', fontsize=24)\n",
    "    fig.text(-0.01, 0.5, f'Self-Reported {metric} Scores', va='center', rotation='vertical', fontsize=24)\n",
    "\n",
    "    # Adjust spacing and layout\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(savepath, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return r_list, p_list, fig\n",
    "\n",
    "\n",
    "# def extractOneMetric(metric, vectors_now, df_moodTracking=df_moodTracking, remove_outliers=False):\n",
    "#   # extracts the vectors needed for linear regression\n",
    "#   # e.g. Mood only, for all time windows\n",
    "#   # metric -- a string that is a self-report metric (ex. 'Mood' or 'Pain')\n",
    "#   # vectors_now -- our feature vectors (all)\n",
    "#   # df_moodTracking -- load in and pre-process self-report google sheet\n",
    "\n",
    "#   # returns vectors_return and y\n",
    "#   # vectors_return -- a dictionary mapping time radius (in minutes) to features\n",
    "#   # y -- a numpy array with labels (self-reported metrics)\n",
    "\n",
    "\n",
    "#   y = df_moodTracking[metric].values.astype(float)\n",
    "\n",
    "#   # # just valid indices (remove nan self-reports!)\n",
    "#   # valid_indices = ~pd.isna(y)\n",
    "#   # y = y[valid_indices]\n",
    "\n",
    "#   # Initially, set valid_indices to include all indices\n",
    "#   valid_indices = np.arange(len(y))\n",
    "\n",
    "#   # Step 1: Remove NaN values\n",
    "#   nan_mask = ~pd.isna(y)\n",
    "#   y = y[nan_mask]\n",
    "#   valid_indices = valid_indices[nan_mask]\n",
    "\n",
    "\n",
    "#   if remove_outliers:\n",
    "#     # Step 2: Remove outliers\n",
    "#     mean_y = np.mean(y)\n",
    "#     std_y = np.std(y)\n",
    "#     outlier_mask = (y >= mean_y - 2 * std_y) & (y <= mean_y + 2 * std_y)\n",
    "#     y = y[outlier_mask]\n",
    "#     valid_indices = valid_indices[outlier_mask]\n",
    "\n",
    "\n",
    "\n",
    "#   vectors_return = {}\n",
    "\n",
    "#   # We will delete indices of self-reports where at least one timestamp doesn't have ANY data at all!\n",
    "#   indices_to_delete = []\n",
    "\n",
    "#   # loop through the timestamps\n",
    "#   # Determine which timestamps to delete (indices_to_delete)\n",
    "#   for i in vectors_now.keys():\n",
    "\n",
    "#     vectors_one_timestamp = np.array([vectors_now[i][str_to_ts(dt)] for dt in df_moodTracking['Datetime']])\n",
    "\n",
    "#     # we want just the valid features (where self-report is not nan)\n",
    "#     vectors_one_timestamp = vectors_one_timestamp[valid_indices]\n",
    "\n",
    "#     # Figure out the correct size of the vector and delete all others\n",
    "#     correct_vector_dim = 0\n",
    "#     for enum_num, vector in enumerate(vectors_one_timestamp):\n",
    "#         if vector.size == 0:\n",
    "#             indices_to_delete.append(enum_num)\n",
    "#         elif vector.shape[0] > correct_vector_dim:\n",
    "#           correct_vector_dim = vector.shape[0]\n",
    "#     for enum_num, vector in enumerate(vectors_one_timestamp):\n",
    "#         if vector.size > 0 and vector.shape[0] < correct_vector_dim:\n",
    "#             indices_to_delete.append(enum_num)\n",
    "\n",
    "\n",
    "#   # Delete those indices from all timestamps\n",
    "#   for i in vectors_now.keys():\n",
    "\n",
    "#     vectors_one_timestamp = np.array([vectors_now[i][str_to_ts(dt)] for dt in df_moodTracking['Datetime']])\n",
    "\n",
    "#     # we want just the valid features (where self-report is not nan)\n",
    "#     vectors_one_timestamp = vectors_one_timestamp[valid_indices]\n",
    "\n",
    "#     # Delete indices from the full previous loop\n",
    "#     vectors_one_timestamp = np.delete(vectors_one_timestamp, indices_to_delete, axis=0)\n",
    "\n",
    "#     if vectors_one_timestamp.ndim == 1:\n",
    "#       print(f'WARNING: NEEDED TO RESHAPE FOR TIME WINDOW {i}')\n",
    "#       # Stack the arrays along a new axis to get a 2D array\n",
    "#       vectors_one_timestamp = np.stack(vectors_one_timestamp, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#     vectors_return[i] = vectors_one_timestamp\n",
    "\n",
    "#   y = np.delete(y, indices_to_delete, axis=0)\n",
    "\n",
    "#   # Make sure we get the right 2D shape for each time window\n",
    "#   for i in vectors_return.keys():\n",
    "\n",
    "#     vectors_one_timestamp = vectors_return[i]\n",
    "\n",
    "#     # If it's still 1d, then let's force the right 2D shape\n",
    "#     if vectors_one_timestamp.ndim == 1:\n",
    "#       print(f'WARNING: NEEDED TO DOUBLE RESHAPE FOR TIME WINDOW {i}')\n",
    "#       vectors_return[i] = vectors_one_timestamp.reshape(y.shape[0], -1)\n",
    "\n",
    "\n",
    "#   return vectors_return, y\n",
    "\n",
    "\n",
    "def extractOneMetric(metric, vectors_now, df_moodTracking=df_moodTracking, remove_outliers=False):\n",
    "  # extracts the vectors needed for linear regression\n",
    "  # e.g. Mood only, for all time windows\n",
    "  # metric -- a string that is a self-report metric (ex. 'Mood')\n",
    "  # vectors_now -- our feature vectors (all)\n",
    "  # df_moodTracking -- load in and pre-process self-report google sheet\n",
    "\n",
    "  # returns vectors_return and y\n",
    "  # vectors_return -- a dictionary mapping time radius (in minutes) to features\n",
    "  # y -- a numpy array with labels (self-reported metrics)\n",
    "\n",
    "\n",
    "  y = df_moodTracking[metric].values.astype(float)\n",
    "  y = np.array([float(y_now) for y_now in y])\n",
    "\n",
    "  # # just valid indices (remove nan self-reports!)\n",
    "  # valid_indices = ~pd.isna(y)\n",
    "  # y = y[valid_indices]\n",
    "\n",
    "  # Initially, set valid_indices to include all indices\n",
    "  valid_indices = np.arange(len(y))\n",
    "\n",
    "  # Step 1: Remove NaN values\n",
    "  nan_mask = ~pd.isna(y)\n",
    "  y = y[nan_mask]\n",
    "  valid_indices = valid_indices[nan_mask]\n",
    "\n",
    "\n",
    "  if remove_outliers:\n",
    "    # Step 2: Remove outliers\n",
    "    mean_y = np.mean(y)\n",
    "    std_y = np.std(y)\n",
    "    outlier_mask = (y >= mean_y - 2 * std_y) & (y <= mean_y + 2 * std_y)\n",
    "    y = y[outlier_mask]\n",
    "    valid_indices = valid_indices[outlier_mask]\n",
    "\n",
    "\n",
    "\n",
    "  vectors_return = {}\n",
    "\n",
    "  def modify_keys(dictionary):\n",
    "    # Using dictionary comprehension to create a new dictionary\n",
    "    # with keys that have spaces removed\n",
    "    return {ts_to_str(key): value for key, value in dictionary.items()}\n",
    "\n",
    "  # loop through the inpatient videos at each time window before each timestamp we're considering (e.g. 10 mins)\n",
    "  for i in vectors_now.keys():\n",
    "    vectors_now_dict = vectors_now[i]\n",
    "    \n",
    "    vectors_now_dict_fixed = modify_keys(vectors_now_dict)\n",
    "    \n",
    "    # TODO: Fix bug where some vectors at the end have more values than others!\n",
    "    # vectors_one_timestamp = np.array([vectors_now_dict_fixed[fn] for fn in df_moodTracking['Datetime']])\n",
    "\n",
    "    shapes = []\n",
    "    for j in range(len(df_moodTracking['Datetime'])):\n",
    "        shapes.append(vectors_now_dict_fixed[df_moodTracking['Datetime'][j]].shape[0])\n",
    "    vector_proper_shape = np.min(shapes)\n",
    "    \n",
    "    vectors_one_timestamp = np.array([vectors_now_dict_fixed[fn][:vector_proper_shape] for fn in df_moodTracking['Datetime']])\n",
    "    \n",
    "    # we want just the valid features (where self-report is not nan)\n",
    "    vectors_one_timestamp = vectors_one_timestamp[valid_indices]\n",
    "    \n",
    "    vectors_return[i] = vectors_one_timestamp\n",
    "    \n",
    "  return vectors_return, y\n",
    "\n",
    "\n",
    "\n",
    "def plot_pearsons_r_vs_alpha(pearson_r_list, ALPHAS_FOR_SEARCH, method, save_path):\n",
    "    \"\"\"\n",
    "    Plots Pearson's R values against Alphas and saves the plot to the specified path.\n",
    "\n",
    "    :param pearson_r_list: List of Pearson's R values.\n",
    "    :param ALPHAS_FOR_SEARCH: List of Alpha values.\n",
    "    :param method: The method used (string).\n",
    "    :param save_path: File path to save the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(ALPHAS_FOR_SEARCH, pearson_r_list, marker='o')\n",
    "    plt.title(f'LASSO: Alpha Search {method}')\n",
    "    plt.xlabel('Alpha')\n",
    "    plt.ylabel(\"Pearson's R\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECLHmN7qYYiy"
   },
   "source": [
    "## Alpha Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfmB3Np39WyJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ALPHA PARAMETER SEARCH FOR LASSO - RUN THIS FIRST!\n",
    "\n",
    "all_metrics = [col for col in df_moodTracking.columns if col != 'Datetime']\n",
    "\n",
    "FILE_ENDING = '.png'\n",
    "\n",
    "# We are just searching using lasso regression\n",
    "#RESULTS_PREFIX_LIST = ['OF_L_', 'OGAU_L_', 'OFAUHSE_L_', 'OGAUHSE_L_', 'HSE_L_', 'ALL_L_']\n",
    "RESULTS_PREFIX_LIST = ['OF_L_', 'OGAU_L_', 'OGAUHSE_L_', 'HSE_L_']\n",
    "#RESULTS_PREFIX_LIST = ['OGAUHSE_L_']\n",
    "\n",
    "\n",
    "EMOTIONS_FOR_SEARCH = ['Mood', 'Depression', 'Anxiety', 'Hunger', 'Pain'] # We are just searching on Mood\n",
    "TIME_WINDOW_FOR_SEARCH = '180' # We are just searching 3 hours\n",
    "\n",
    "# List of alpha values to search through\n",
    "#ALPHAS_FOR_SEARCH = np.arange(0, 1.6, 0.1)\n",
    "ALPHAS_FOR_SEARCH = np.arange(0, 5, 0.2)\n",
    "#ALPHAS_FOR_SEARCH = np.arange(0, 10, 0.2)\n",
    "\n",
    "# This will populate with the best alphas for each prefix in RESULTS_PREFIX_LIST\n",
    "best_alphas_lasso = {}\n",
    "\n",
    "for RESULTS_PREFIX in RESULTS_PREFIX_LIST:\n",
    "    do_lasso = False\n",
    "    do_ridge = False\n",
    "\n",
    "    if '_L_' in RESULTS_PREFIX:\n",
    "        do_lasso = True\n",
    "\n",
    "    if '_R_' in RESULTS_PREFIX:\n",
    "        do_ridge = True\n",
    "\n",
    "    if 'OF_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + f'experimental3_openface_0.5_hours.xlsx'\n",
    "        vectors_now = openface_vectors_dict\n",
    "        method_now = 'OpenFace'\n",
    "\n",
    "    elif 'OGAU_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + 'opengraphau_0.5_hours.xlsx'\n",
    "        vectors_now = opengraphau_vectors_dict\n",
    "        method_now = 'OpenGraphAU'\n",
    "\n",
    "    elif 'OFAUHSE_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + 'ofauhse_0.5_hours.xlsx'\n",
    "        vectors_now = ofauhsemotion_vectors_dict\n",
    "        method_now = 'OFAU+HSE'\n",
    "\n",
    "    elif 'OGAUHSE_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + 'ogauhse_0.5_hours.xlsx'\n",
    "        vectors_now = ogauhsemotion_vectors_dict\n",
    "        method_now = 'OGAU+HSE'\n",
    "\n",
    "    elif 'HSE_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + 'hsemotion_0.5_hours.xlsx'\n",
    "        vectors_now = hsemotion_vectors_dict\n",
    "        method_now = 'HSEmotion'\n",
    "\n",
    "    elif 'ALL_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + 'all_0.5_hours.xlsx'\n",
    "        vectors_now = all_vectors_dict\n",
    "        method_now = 'ALL(OF+OG+HSE)'\n",
    "\n",
    "    # Let's put each setting in its own folder!\n",
    "    os.makedirs(RESULTS_PATH_BASE + 'SEARCH_Alpha_Lasso/' + RESULTS_PREFIX, exist_ok=True)\n",
    "    results_prefix_unmodified = RESULTS_PREFIX\n",
    "    RESULTS_PREFIX = 'SEARCH_Alpha_Lasso/' + RESULTS_PREFIX + '/' + RESULTS_PREFIX\n",
    "\n",
    "    # Initialize a dictionary to store the best alpha values for each emotion\n",
    "    best_alphas_lasso[results_prefix_unmodified] = {}\n",
    "\n",
    "    for metric in EMOTIONS_FOR_SEARCH:\n",
    "        print('METRIC NOW: ', metric)\n",
    "        pearson_r_list = []  # Reset the R list for each metric\n",
    "\n",
    "        for alpha_now in ALPHAS_FOR_SEARCH:\n",
    "\n",
    "            avg_best_R = 0\n",
    "\n",
    "            vectors_return, y = extractOneMetric(metric, vectors_now=vectors_now)\n",
    "\n",
    "            # Limit to just one time window for alpha search\n",
    "            tmp_vectors = vectors_return\n",
    "            vectors_return = {}\n",
    "            vectors_return[TIME_WINDOW_FOR_SEARCH] = tmp_vectors[TIME_WINDOW_FOR_SEARCH]\n",
    "            del tmp_vectors\n",
    "\n",
    "            scores, preds, y, models = linRegOneMetric(vectors_return, y, do_lasso=do_lasso, do_ridge=do_ridge, alpha=alpha_now)\n",
    "            scores_r, preds_r, _, models_r = linRegOneMetric(vectors_return, y, randShuffle=True, alpha=alpha_now)\n",
    "\n",
    "            # make scatterplots\n",
    "            randShuffleR, _, _ = plot_scatterplots(preds_r, y, f'{metric} Random Shuffle', RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_scatterRand_{alpha_now}{FILE_ENDING}')\n",
    "            r_list, p_list, scatterFig = plot_scatterplots(preds, y, metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_scatterplots_{alpha_now}{FILE_ENDING}', randShuffleR=randShuffleR)\n",
    "\n",
    "            # Determine our best time radius for this metric based on Pearson's R\n",
    "            best_time_radius = list(scores.keys())[np.argmax(r_list)]\n",
    "            best_mse_list = scores[best_time_radius]\n",
    "            best_avg_mse = np.mean(scores[best_time_radius])\n",
    "            best_pearson_r = r_list[np.argmax(r_list)]\n",
    "\n",
    "            # Add to our avg best R\n",
    "            avg_best_R = avg_best_R + best_pearson_r\n",
    "\n",
    "            # bar plot for pearson r\n",
    "            rPlotFig = make_r_barplot(r_list, list(scores.keys()), metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_R_{alpha_now}{FILE_ENDING}', method_now=method_now)\n",
    "\n",
    "            # make MSE plot\n",
    "            MSEPlotFig = make_mse_boxplot(scores, metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_MSE_{alpha_now}{FILE_ENDING}', method_now=method_now)\n",
    "\n",
    "        # Add one R value for this alpha value to pearson_r_list\n",
    "        avg_best_R = avg_best_R / len(EMOTIONS_FOR_SEARCH)\n",
    "        pearson_r_list.append(avg_best_R)\n",
    "\n",
    "        # Plot R vs. alpha for this setting\n",
    "        plot_pearsons_r_vs_alpha(pearson_r_list=pearson_r_list, ALPHAS_FOR_SEARCH=ALPHAS_FOR_SEARCH, method=method_now, save_path=RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_Alpha_Search{FILE_ENDING}')\n",
    "\n",
    "        # Find best alpha for this metric and store it\n",
    "        best_index_of_alpha = np.argmax(pearson_r_list)\n",
    "        best_alpha_value = ALPHAS_FOR_SEARCH[best_index_of_alpha]\n",
    "        best_alphas_lasso[results_prefix_unmodified][metric] = best_alpha_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IR_M44vjPnVf"
   },
   "outputs": [],
   "source": [
    "# SAVE VARIABLES\n",
    "\n",
    "save_var(best_alphas_lasso, forced_name=f'best_alphas_lasso_{PAT_SHORT_NAME}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVIn5cW7P1in"
   },
   "outputs": [],
   "source": [
    "# LOAD VARIABLES\n",
    "\n",
    "best_alphas_lasso_mood = load_var(f'best_alphas_lasso_{PAT_SHORT_NAME}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbrmWuiEYghJ"
   },
   "source": [
    "## Core Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXn-zIhdEGk0"
   },
   "outputs": [],
   "source": [
    "# GENERATE ALL PLOTS! ONE CODE BLOCK\n",
    "\n",
    "if 'best_alphas_lasso' not in globals():\n",
    "    raise NameError(\"GO RUN THE LASSO ALPHA PARAMETER SEARCH BLOCK FIRST!\")\n",
    "\n",
    "# if 'best_alphas_ridge' not in globals():\n",
    "#     raise NameError(\"GO RUN THE RIDGE ALPHA PARAMETER SEARCH BLOCK FIRST!\")\n",
    "\n",
    "\n",
    "#all_metrics = [col for col in df_moodTracking.columns if col != 'Datetime']\n",
    "#all_metrics = ['Mood', 'Anxiety', 'Hunger']\n",
    "all_metrics = ['Mood', 'Depression', 'Anxiety', 'Hunger', 'Pain']\n",
    "\n",
    "\n",
    "FILE_ENDING = '.png'\n",
    "# RESULTS_PREFIX_LIST = ['OF_', 'OGAU_', 'OFAUHSE_', 'OGAUHSE_', 'HSE_', 'ALL_',\n",
    "#                        'OF_L_', 'OGAU_L_', 'OFAUHSE_L_', 'OGAUHSE_L_', 'HSE_L_', 'ALL_L_',\n",
    "#                        'OF_R_', 'OGAU_R_', 'OFAUHSE_R_', 'OGAUHSE_R_', 'HSE_R_', 'ALL_R_']\n",
    "\n",
    "# RESULTS_PREFIX_LIST = ['OF_L_', 'OGAUHSE_L_', 'OGAU_L_', 'OFAUHSE_L_', 'HSE_L_', 'ALL_L_']\n",
    "\n",
    "RESULTS_PREFIX_LIST = ['OF_L_', 'OGAU_L_', 'OGAUHSE_L_', 'HSE_L_']\n",
    "\n",
    "\n",
    "# Do we remove ground truth labels that are over 2 standard deviations from the mean?\n",
    "REMOVE_OUTLIERS = False\n",
    "\n",
    "\n",
    "for RESULTS_PREFIX in RESULTS_PREFIX_LIST:\n",
    "    do_lasso = False\n",
    "    do_ridge = False\n",
    "\n",
    "    if '_L_' in RESULTS_PREFIX:\n",
    "        do_lasso = True\n",
    "\n",
    "    if '_R_' in RESULTS_PREFIX:\n",
    "        do_ridge = True\n",
    "\n",
    "    if 'OF_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + f'experimental3_openface_0.5_hours.xlsx'\n",
    "        vectors_now = openface_vectors_dict\n",
    "        method_now = 'OpenFace'\n",
    "\n",
    "    elif 'OGAU_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + 'opengraphau_0.5_hours.xlsx'\n",
    "        vectors_now = opengraphau_vectors_dict\n",
    "        method_now = 'OpenGraphAU'\n",
    "\n",
    "    elif 'OFAUHSE_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + 'ofauhse_0.5_hours.xlsx'\n",
    "        vectors_now = ofauhsemotion_vectors_dict\n",
    "        method_now = 'OFAU+HSE'\n",
    "\n",
    "    elif 'OGAUHSE_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + 'ogauhse_0.5_hours.xlsx'\n",
    "        vectors_now = ogauhsemotion_vectors_dict\n",
    "        method_now = 'OGAU+HSE'\n",
    "\n",
    "    elif 'HSE_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + 'hsemotion_0.5_hours.xlsx'\n",
    "        vectors_now = hsemotion_vectors_dict\n",
    "        method_now = 'HSEmotion'\n",
    "\n",
    "    elif 'ALL_' in RESULTS_PREFIX:\n",
    "        spreadsheet_path = FEATURE_LABEL_PATH + 'all_0.5_hours.xlsx'\n",
    "        vectors_now = all_vectors_dict\n",
    "        method_now = 'ALL(OF+OG+HSE)'\n",
    "\n",
    "    # Let's put each setting in its own folder!\n",
    "    os.makedirs(RESULTS_PATH_BASE + RESULTS_PREFIX, exist_ok=True)\n",
    "    results_prefix_unmodified = RESULTS_PREFIX\n",
    "    RESULTS_PREFIX = RESULTS_PREFIX + '/' + RESULTS_PREFIX\n",
    "\n",
    "    # Loop through metrics (Anxiety, Depression, Mood, etc.)\n",
    "    for metric in all_metrics:\n",
    "        print('METRIC NOW: ', metric)\n",
    "        if do_lasso:\n",
    "            alpha_now = best_alphas_lasso[results_prefix_unmodified].get(metric, 1.0)  # Use the specific alpha for the metric\n",
    "        elif do_ridge:\n",
    "            alpha_now = best_alphas_ridge[results_prefix_unmodified].get(metric, 1.0)  # Use the specific alpha for the metric\n",
    "        else:\n",
    "            # Neither lasso nor ridge, so alpha is irrelevant\n",
    "            alpha_now = 1.0\n",
    "\n",
    "        vectors_return, y = extractOneMetric(metric, vectors_now=vectors_now, remove_outliers=REMOVE_OUTLIERS)\n",
    "        scores, preds, y, models = linRegOneMetric(vectors_return, y, do_lasso=do_lasso, do_ridge=do_ridge, alpha=alpha_now)\n",
    "        scores_r, preds_r, _, models_r = linRegOneMetric(vectors_return, y, randShuffle=True, alpha=alpha_now)\n",
    "\n",
    "        # make scatterplots\n",
    "        randShuffleR, _, _ = plot_scatterplots(preds_r, y, f'{metric} Random Shuffle', RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_scatterRand{FILE_ENDING}')\n",
    "        r_list, p_list, scatterFig = plot_scatterplots(preds, y, metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_scatterplots{FILE_ENDING}', randShuffleR=randShuffleR)\n",
    "\n",
    "        # Determine our best time radius for this metric based on Pearson's R\n",
    "        best_time_radius = list(scores.keys())[np.argmax(r_list)]\n",
    "        best_mse_list = scores[best_time_radius]\n",
    "        best_avg_mse = np.mean(scores[best_time_radius])\n",
    "        best_pearson_r = r_list[np.argmax(r_list)]\n",
    "\n",
    "        # bar plot for pearson r\n",
    "        rPlotFig = make_r_barplot(r_list, list(scores.keys()), metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_R{FILE_ENDING}', method_now=method_now)\n",
    "\n",
    "        # make MSE plot\n",
    "        MSEPlotFig = make_mse_boxplot(scores, metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_MSE{FILE_ENDING}', method_now=method_now)\n",
    "\n",
    "        # Feature ablation\n",
    "        # feat_ab_scores, feat_ab_prs = featureAblate(vectors_return[best_time_radius], y, do_lasso=do_lasso, do_ridge=do_ridge)\n",
    "\n",
    "        # top_indices, featAbMSEFig = plotFeatAbMSEs(feat_ab_scores, best_mse_list, metric, best_time_radius, savepath=RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_featAblate_MSEs{FILE_ENDING}', spreadsheet_path=spreadsheet_path)\n",
    "        # plotFeatAbPRs(feat_ab_prs, best_pearson_r, metric, best_time_radius, savepath=RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_featAblate_R{FILE_ENDING}', spreadsheet_path=spreadsheet_path)\n",
    "\n",
    "        # extract just ONE scatterplot (the best pearson's R) and save it individually\n",
    "        plt.rcParams['lines.markersize'] = 9\n",
    "        _, _, bestScatterFig = plot_predictions(y, preds[best_time_radius], randShuffleR=randShuffleR[np.argmax(r_list)], ax=None, time_rad=best_time_radius, metric=metric)\n",
    "        bestScatterFig.savefig(RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_bestScatter{FILE_ENDING}', bbox_inches='tight')\n",
    "\n",
    "        # Plot top n features vs. self-reported scores\n",
    "        # PLOT_NOW = 3\n",
    "        # plot_feat_scatterplots(vectors_array=vectors_return[best_time_radius], y=y, feat_ind_list=top_indices[:PLOT_NOW], metric=metric, savepath=RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_topFeats{FILE_ENDING}', spreadsheet_path=spreadsheet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
