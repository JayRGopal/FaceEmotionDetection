{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D57PMLHEVlOm"
   },
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frVOpCRsQFEX"
   },
   "outputs": [],
   "source": [
    "PAT_NOW = \"S23_199\"\n",
    "PAT_SHORT_NAME = PAT_NOW[0:1] + PAT_NOW[3:] # e.g. \"S_199\"\n",
    "\n",
    "VIDEO_TIMESTAMPS_FILE_PATH = f'/home/klab/NAS/Analysis/AudioFacialEEG/Behavioral Labeling/videoDateTimes/VideoDatetimes{PAT_SHORT_NAME[-4:]}.xlsx'\n",
    "\n",
    "VERBAL_PAIN_SCORES_FILE_PATH = '/home/klab/NAS/Analysis/AudioFacialEEG/Behavioral Labeling/VerbalPainScores/verbalPainScores.xlsx'\n",
    "\n",
    "\n",
    "OPENFACE_OUTPUT_DIRECTORY = f'/home/klab/NAS/Analysis/outputs_OpenFace/{PAT_NOW}/'\n",
    "COMBINED_OUTPUT_DIRECTORY = f'/home/klab/NAS/Analysis/outputs_Combined/{PAT_NOW}/'\n",
    "\n",
    "RUNTIME_VAR_PATH = '/home/klab/NAS/Analysis/AudioFacialEEG/Runtime_Vars/'\n",
    "RESULTS_PATH_BASE = f'/home/klab/NAS/Analysis/AudioFacialEEG/Results/{PAT_SHORT_NAME}/'\n",
    "FEATURE_VIS_PATH = f'/home/klab/NAS/Analysis/AudioFacialEEG/Feature_Visualization/{PAT_SHORT_NAME}/'\n",
    "FEATURE_LABEL_PATH = '/home/klab/NAS/Analysis/AudioFacialEEG/Feature_Labels/'\n",
    "QC_PATH = '/home/klab/NAS/Analysis/AudioFacialEEG/Quality_Control/'\n",
    "EPHYS_CONCORDANCE_PATH = f'/home/klab/NAS/Analysis/AudioFacialEEG/EphysProb/{PAT_NOW}-ephysPrediction.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "91MVRRaOj-FH"
   },
   "outputs": [],
   "source": [
    "EMO_FEATURE_SETTING = 2\n",
    "\n",
    "# 0 - Our Custom AU --> Emotions, with all emotions\n",
    "# 1 - Our Custom AU --> Emotions, with just OpenDBM's emotions\n",
    "# 2 - OpenDBM's AU--> Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "b_KxZhrQtcmv"
   },
   "outputs": [],
   "source": [
    "STATS_FEATURE_SETTING = 3\n",
    "\n",
    "# 0 - Our new features (including autocorrelation, kurtosis, etc.)\n",
    "# 1 - Our new features, excluding extras like autocorrelation and kurtosis\n",
    "# 2 - Just pres_pct\n",
    "# 3 - Our new features, excluding extras. Do NOT threshold AUs before computing metrics. HSE gets 5 event features. OGAU gets num events and presence percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "VIc4rrMwD8N9"
   },
   "outputs": [],
   "source": [
    "NORMALIZE_DATA = 0\n",
    "\n",
    "# 0 - No time series normalization\n",
    "# 1 - Yes time series normalization (for each time window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU72QcAcVyLy"
   },
   "source": [
    "# Installs & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "LRX3feHKIR1z"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Ignore all warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "d6_t0DlpTFmi"
   },
   "outputs": [],
   "source": [
    "# SAVE VARIABLES\n",
    "import pickle\n",
    "\n",
    "def get_var_name(our_variable):\n",
    "    namespace = globals()\n",
    "    for name, obj in namespace.items():\n",
    "        if obj is our_variable:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "# Save the dictionary to a file using pickle\n",
    "def save_var(our_variable, RUNTIME_VAR_PATH=RUNTIME_VAR_PATH, forced_name=None):\n",
    "  if forced_name is None:\n",
    "    name_now = get_var_name(our_variable)\n",
    "  else:\n",
    "    name_now = forced_name\n",
    "\n",
    "  with open(RUNTIME_VAR_PATH + f'{name_now}.pkl', 'wb') as file:\n",
    "      pickle.dump(our_variable, file)\n",
    "\n",
    "def load_var(variable_name, RUNTIME_VAR_PATH=RUNTIME_VAR_PATH):\n",
    "  # Load from the file\n",
    "  with open(RUNTIME_VAR_PATH + f'{variable_name}.pkl', 'rb') as file:\n",
    "      return pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-N5cJiTpuyVr"
   },
   "source": [
    "# Video Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "QoONs-gXu0NZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read data from the Excel file and put it in a dataframe\n",
    "worksheet_name = f'VideoDatetimes{PAT_SHORT_NAME[-4:]}'\n",
    "df_videoTimestamps = pd.read_excel(VIDEO_TIMESTAMPS_FILE_PATH, sheet_name=worksheet_name)\n",
    "\n",
    "df_videoTimestamps['Filename'] = df_videoTimestamps['Filename'].str.replace('.m2t', '')\n",
    "\n",
    "# Additional processing specific to patient 'S_199'\n",
    "if PAT_SHORT_NAME == 'S_199':\n",
    "    # There's no H01 video, so let's drop that filename\n",
    "    df_videoTimestamps = df_videoTimestamps.drop(211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "M7WfI-u-PR2h"
   },
   "outputs": [],
   "source": [
    "# Check for any missing videos!\n",
    "\n",
    "def print_difference(list1, list2):\n",
    "    for item in list1:\n",
    "        if item not in list2:\n",
    "            print(item)\n",
    "\n",
    "filenames_master_list = list(df_videoTimestamps['Filename'].values)\n",
    "filenames_we_have = [i[:-4] for i in os.listdir(COMBINED_OUTPUT_DIRECTORY)]\n",
    "\n",
    "print_difference(filenames_master_list, filenames_we_have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "cqMfPe79vFRv",
    "outputId": "515318a2-9243-4d29-f920-ade22cf87517"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>VideoStart</th>\n",
       "      <th>VideoEnd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>3332YX00</td>\n",
       "      <td>2023-04-09 11:04:43</td>\n",
       "      <td>2023-04-09 12:04:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>3332YX01</td>\n",
       "      <td>2023-04-09 12:04:43</td>\n",
       "      <td>2023-04-09 13:04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>3332YY00</td>\n",
       "      <td>2023-04-09 13:04:43</td>\n",
       "      <td>2023-04-09 14:04:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>3332Z100</td>\n",
       "      <td>2023-04-09 14:14:03</td>\n",
       "      <td>2023-04-09 15:12:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>48220H00</td>\n",
       "      <td>2023-03-31 15:35:06</td>\n",
       "      <td>2023-03-31 16:35:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Filename          VideoStart            VideoEnd\n",
       "206  3332YX00 2023-04-09 11:04:43 2023-04-09 12:04:43\n",
       "207  3332YX01 2023-04-09 12:04:43 2023-04-09 13:04:35\n",
       "208  3332YY00 2023-04-09 13:04:43 2023-04-09 14:04:42\n",
       "209  3332Z100 2023-04-09 14:14:03 2023-04-09 15:12:06\n",
       "210  48220H00 2023-03-31 15:35:06 2023-03-31 16:35:06"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_videoTimestamps[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIwgzAGpg7yS"
   },
   "source": [
    "# Verbal Pain Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "8LoNlJvxg9mn"
   },
   "outputs": [],
   "source": [
    "# Read data from the Excel file and put it in a dataframe\n",
    "worksheet_name = f'painScores_{PAT_SHORT_NAME[0:1] + PAT_SHORT_NAME[2:]}'\n",
    "df_verbalPain = pd.read_excel(VERBAL_PAIN_SCORES_FILE_PATH, sheet_name=worksheet_name)\n",
    "\n",
    "df_verbalPain = df_verbalPain.replace('', np.nan).replace(' ', np.nan).fillna(value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zs1NoMoIg9p5"
   },
   "outputs": [],
   "source": [
    "# Add the :00 at the end for seconds\n",
    "df_verbalPain['Date Time'] = pd.to_datetime(df_verbalPain['Date Time'])\n",
    "df_verbalPain['Date Time'] = df_verbalPain['Date Time'].dt.strftime('%d-%b-%Y %H:%M:%S')\n",
    "\n",
    "# Filtering df_verbalPain based on conditions\n",
    "df_verbalPain = df_verbalPain[\n",
    "    (pd.to_datetime(df_verbalPain['Date Time']) >= pd.to_datetime(df_videoTimestamps['VideoStart']).min().strftime('%d-%b-%Y %H:%M:%S')) &\n",
    "    (pd.to_datetime(df_verbalPain['Date Time']) <= pd.to_datetime(df_videoTimestamps['VideoEnd']).max().strftime('%d-%b-%Y %H:%M:%S'))\n",
    "]\n",
    "\n",
    "df_verbalPain = df_verbalPain.reset_index(drop=True)\n",
    "\n",
    "# Reformatting the 'Date Time' column to match df_moodTracking\n",
    "df_verbalPain['Date Time'] = pd.to_datetime(df_verbalPain['Date Time'])\n",
    "df_verbalPain['Date Time'] = df_verbalPain['Date Time'].dt.strftime('%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "# Rename Date Time to Datetime\n",
    "df_verbalPain = df_verbalPain.rename(columns={'Date Time': 'Datetime'})\n",
    "df_verbalPain = df_verbalPain.rename(columns={'Pain Scores': 'Pain'})\n",
    "\n",
    "df_verbalPain = df_verbalPain.groupby('Datetime', as_index=False).mean()\n",
    "\n",
    "# Deleting rows where 'Pain' column is NaN or empty\n",
    "df_verbalPain = df_verbalPain[df_verbalPain['Pain'].notna() & (df_verbalPain['Pain'] != '')]\n",
    "df_verbalPain.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "df_verbalPain_original = df_verbalPain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "VOBh0AffhMyn",
    "outputId": "814a36e3-7a4d-45e6-c94d-7c04e07b6b3d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Pain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03/31/2023 18:15:00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03/31/2023 18:30:00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03/31/2023 18:45:00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03/31/2023 19:00:00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03/31/2023 19:15:00</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>04/09/2023 09:00:00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>04/09/2023 11:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>04/09/2023 13:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>04/09/2023 15:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>04/09/2023 15:12:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Datetime  Pain\n",
       "0   03/31/2023 18:15:00   8.0\n",
       "1   03/31/2023 18:30:00   8.0\n",
       "2   03/31/2023 18:45:00   8.0\n",
       "3   03/31/2023 19:00:00   8.0\n",
       "4   03/31/2023 19:15:00  10.0\n",
       "..                  ...   ...\n",
       "89  04/09/2023 09:00:00   4.0\n",
       "90  04/09/2023 11:00:00   0.0\n",
       "91  04/09/2023 13:00:00   0.0\n",
       "92  04/09/2023 15:00:00   0.0\n",
       "93  04/09/2023 15:12:00   0.0\n",
       "\n",
       "[94 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_verbalPain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOrT1X_9WU5X",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# OpenFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "YV8_wmALY-hV"
   },
   "outputs": [],
   "source": [
    "# DICTIONARY OF SEPARATE DFS\n",
    "\n",
    "def get_dict_openface(output_dir):\n",
    "  # Create an empty dictionary to hold the DataFrames\n",
    "  dfs_openface = {}\n",
    "\n",
    "  # Get a list of all the CSV files in the directory\n",
    "  csv_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.csv')])\n",
    "\n",
    "  # list of columns to keep\n",
    "  columns_to_keep = ['frame', ' timestamp', ' success',\n",
    "                    ' AU01_r',\n",
    "                    ' AU02_r',\n",
    "                    ' AU04_r',\n",
    "                    ' AU05_r',\n",
    "                    ' AU06_r',\n",
    "                    ' AU07_r',\n",
    "                    ' AU09_r',\n",
    "                    ' AU10_r',\n",
    "                    ' AU12_r',\n",
    "                    ' AU14_r',\n",
    "                    ' AU15_r',\n",
    "                    ' AU17_r',\n",
    "                    ' AU20_r',\n",
    "                    ' AU23_r',\n",
    "                    ' AU25_r',\n",
    "                    ' AU26_r',\n",
    "                    ' AU45_r',\n",
    "                    ' AU01_c',\n",
    "                    ' AU02_c',\n",
    "                    ' AU04_c',\n",
    "                    ' AU05_c',\n",
    "                    ' AU06_c',\n",
    "                    ' AU07_c',\n",
    "                    ' AU09_c',\n",
    "                    ' AU10_c',\n",
    "                    ' AU12_c',\n",
    "                    ' AU14_c',\n",
    "                    ' AU15_c',\n",
    "                    ' AU17_c',\n",
    "                    ' AU20_c',\n",
    "                    ' AU23_c',\n",
    "                    ' AU25_c',\n",
    "                    ' AU26_c',\n",
    "                    ' AU45_c']\n",
    "\n",
    "  # Loop through the CSV files\n",
    "  for csv_file in csv_files:\n",
    "      # Load data into a pandas df\n",
    "      csv_file_path = os.path.join(output_dir, csv_file)\n",
    "      df_temp = pd.read_csv(csv_file_path)\n",
    "\n",
    "      # keep every 6th row such that it's 5 fps!\n",
    "      X = 6\n",
    "      df_temp = df_temp[df_temp.index % X == 0]\n",
    "\n",
    "      # filter DataFrame to keep only columns in list\n",
    "      df_temp = df_temp.loc[:, columns_to_keep]\n",
    "\n",
    "      # fix column names to not have leading or trailing spaces!\n",
    "      df_temp = df_temp.rename(columns=lambda x: x.strip())\n",
    "\n",
    "      # Store the DataFrame in the dictionary with the csv file name as the key\n",
    "      # remove the '.csv' by doing csv_file[:-4]\n",
    "      dfs_openface[csv_file[:-4]] = df_temp\n",
    "      del df_temp\n",
    "\n",
    "  return dfs_openface\n",
    "\n",
    "\n",
    "def only_successful_frames(df):\n",
    "    # get frames where AU/emotion detection was successful!\n",
    "    return df[df['success'] == 1]\n",
    "\n",
    "def apply_function_to_dict(dictionary, func, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply a function to each DataFrame in a dictionary and return a modified copy of the dictionary.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): The dictionary containing DataFrames.\n",
    "        func (function): The function to apply to each DataFrame.\n",
    "        **kwargs: Additional keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        dict: A modified copy of the dictionary with the function applied to each DataFrame.\n",
    "    \"\"\"\n",
    "    return {key: func(df, **kwargs) for key, df in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTOXk7be8YSH"
   },
   "outputs": [],
   "source": [
    "dfs_openface = get_dict_openface(OPENFACE_OUTPUT_DIRECTORY)\n",
    "dfs_openface = apply_function_to_dict(dfs_openface, only_successful_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkiuU4NVeACU"
   },
   "outputs": [],
   "source": [
    "# SAVE THE OPENFACE DICTIONARY\n",
    "\n",
    "save_var(dfs_openface, forced_name=f'dfs_openface_{PAT_SHORT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Py2RgmTAekZS"
   },
   "outputs": [],
   "source": [
    "# LOAD THE OPENFACE DICTIONARY\n",
    "\n",
    "dfs_openface = load_var(f'dfs_openface_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTcrVT0z0RzO",
    "outputId": "9c6706c4-b256-4d14-f5dc-36b8735f821d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variables in RAM: 0.14008331298828125 MB\n"
     ]
    }
   ],
   "source": [
    "# RAM CHECK\n",
    "\n",
    "import sys\n",
    "sumsize = 0\n",
    "for i in list(globals().keys()):\n",
    "  size = sys.getsizeof(globals()[i])\n",
    "  # print(i, ': ', size)\n",
    "  sumsize = sumsize + size\n",
    "\n",
    "print(f'Total variables in RAM: {sumsize / (1024 ** 2)} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzAOLb_jSWk0"
   },
   "outputs": [],
   "source": [
    "# CLEAR UP RAM\n",
    "\n",
    "for key in list(globals().keys()):\n",
    "    if (key.startswith('_i') and key != '_ih') or (key.startswith('_') and key[1:].isdigit()):\n",
    "        del globals()[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "wkgxRTcLvkw-",
    "outputId": "ca053124-77f0-4285-eafc-5d08168b02b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-fa74b8e4-011e-4332-9c54-3c1395fe4633\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>success</th>\n",
       "      <th>AU01_r</th>\n",
       "      <th>AU02_r</th>\n",
       "      <th>AU04_r</th>\n",
       "      <th>AU05_r</th>\n",
       "      <th>AU06_r</th>\n",
       "      <th>AU07_r</th>\n",
       "      <th>AU09_r</th>\n",
       "      <th>...</th>\n",
       "      <th>AU10_c</th>\n",
       "      <th>AU12_c</th>\n",
       "      <th>AU14_c</th>\n",
       "      <th>AU15_c</th>\n",
       "      <th>AU17_c</th>\n",
       "      <th>AU20_c</th>\n",
       "      <th>AU23_c</th>\n",
       "      <th>AU25_c</th>\n",
       "      <th>AU26_c</th>\n",
       "      <th>AU45_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>169</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>1315</td>\n",
       "      <td>43.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>1429</td>\n",
       "      <td>47.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>4975</td>\n",
       "      <td>165.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>4987</td>\n",
       "      <td>166.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107994</th>\n",
       "      <td>107995</td>\n",
       "      <td>3599.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108000</th>\n",
       "      <td>108001</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108006</th>\n",
       "      <td>108007</td>\n",
       "      <td>3600.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108012</th>\n",
       "      <td>108013</td>\n",
       "      <td>3600.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108018</th>\n",
       "      <td>108019</td>\n",
       "      <td>3600.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940 rows × 37 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa74b8e4-011e-4332-9c54-3c1395fe4633')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-fa74b8e4-011e-4332-9c54-3c1395fe4633 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-fa74b8e4-011e-4332-9c54-3c1395fe4633');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-bdf2776f-ef1f-467a-82dc-20518e29c4d6\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bdf2776f-ef1f-467a-82dc-20518e29c4d6')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-bdf2776f-ef1f-467a-82dc-20518e29c4d6 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "         frame  timestamp  success  AU01_r  AU02_r  AU04_r  AU05_r  AU06_r  \\\n",
       "168        169        5.6        1    0.00    0.37    0.40    0.06    0.38   \n",
       "1314      1315       43.8        1    1.61    2.99    2.44    0.35    0.01   \n",
       "1428      1429       47.6        1    0.35    1.81    1.45    0.00    0.00   \n",
       "4974      4975      165.8        1    0.58    2.43    1.21    1.45    1.05   \n",
       "4986      4987      166.2        1    0.38    2.60    0.00    0.00    0.00   \n",
       "...        ...        ...      ...     ...     ...     ...     ...     ...   \n",
       "107994  107995     3599.8        1    0.00    0.00    0.57    0.00    0.43   \n",
       "108000  108001     3600.0        1    0.09    0.12    0.44    0.55    0.46   \n",
       "108006  108007     3600.2        1    0.00    0.00    0.53    0.54    0.32   \n",
       "108012  108013     3600.4        1    0.00    0.01    0.50    0.30    0.60   \n",
       "108018  108019     3600.6        1    0.00    0.31    0.70    0.16    0.65   \n",
       "\n",
       "        AU07_r  AU09_r  ...  AU10_c  AU12_c  AU14_c  AU15_c  AU17_c  AU20_c  \\\n",
       "168       0.00    0.10  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1314      1.09    0.00  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1428      0.08    0.00  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4974      0.12    0.00  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4986      0.34    0.00  ...     1.0     0.0     0.0     0.0     1.0     0.0   \n",
       "...        ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "107994    0.00    0.00  ...     1.0     0.0     0.0     0.0     0.0     0.0   \n",
       "108000    0.22    0.04  ...     1.0     1.0     0.0     0.0     1.0     0.0   \n",
       "108006    0.64    0.06  ...     1.0     1.0     0.0     0.0     0.0     0.0   \n",
       "108012    0.47    0.36  ...     1.0     1.0     0.0     0.0     0.0     0.0   \n",
       "108018    0.64    0.33  ...     1.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "        AU23_c  AU25_c  AU26_c  AU45_c  \n",
       "168        0.0     0.0     0.0     0.0  \n",
       "1314       0.0     0.0     0.0     0.0  \n",
       "1428       0.0     0.0     0.0     0.0  \n",
       "4974       0.0     0.0     0.0     0.0  \n",
       "4986       0.0     0.0     0.0     0.0  \n",
       "...        ...     ...     ...     ...  \n",
       "107994     0.0     0.0     0.0     0.0  \n",
       "108000     0.0     0.0     0.0     0.0  \n",
       "108006     0.0     0.0     0.0     0.0  \n",
       "108012     0.0     0.0     0.0     0.0  \n",
       "108018     0.0     0.0     0.0     0.0  \n",
       "\n",
       "[1940 rows x 37 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_openface['3332W200']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "vXD64i_f8az8",
    "outputId": "e4890b15-116c-4bf1-b918-3ba6c004e1a3",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0.0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6c2d55ad5806>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs_openface\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'3332W200'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_c$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Calculate the percentage of zeros for each key in the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpercentages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs_openface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Calculate the average percentage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-6c2d55ad5806>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs_openface\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'3332W200'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_c$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Calculate the percentage of zeros for each key in the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpercentages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs_openface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Calculate the average percentage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# List to store the average percentages\n",
    "avg_percentages = []\n",
    "\n",
    "# Iterate over each column ending with '_c'\n",
    "for column in dfs_openface['3332W200'].filter(regex='_c$').columns:\n",
    "    # Calculate the percentage of zeros for each key in the dictionary\n",
    "    percentages = [df[column].value_counts(normalize=True)[0] * 100 for df in dfs_openface.values()]\n",
    "\n",
    "    # Calculate the average percentage\n",
    "    avg_percentage = np.mean(percentages)\n",
    "    avg_percentages.append(avg_percentage)\n",
    "\n",
    "# Create the bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(dfs_openface['3332W200'].filter(regex='_c$').columns, avg_percentages, width=0.6)  # Adjust width as desired\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Columns')\n",
    "ax.set_ylabel('Average Percentage of Zeros')\n",
    "ax.set_title(f'Average Percentage of Zeros for Classification Columns, Threshold = Default')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Display the plot\n",
    "plt.savefig(RESULTS_PATH_BASE + f'openface_threshold_default.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t52_30WQzYeh"
   },
   "source": [
    "# HSEmotion & OpenGraphAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "aUg5iqkRtIJX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_dict(output_dir, file_now='outputs_hse.csv', filterOutLR=True):\n",
    "\n",
    "  # Initialize an empty dictionary to store the dataframes\n",
    "  df_dict = {}\n",
    "\n",
    "  # Loop through the subfolders in alphabetical order\n",
    "  for subfolder_name in sorted(os.listdir(output_dir)):\n",
    "\n",
    "    # Check if the subfolder contains CSV files\n",
    "    subfolder_path = os.path.join(output_dir, subfolder_name)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "      continue\n",
    "\n",
    "    # Load the first CSV file in the subfolder into a dataframe\n",
    "    csv_file_path = os.path.join(subfolder_path, file_now)\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "      continue\n",
    "\n",
    "    try:\n",
    "      df_temp = pd.read_csv(csv_file_path)\n",
    "    except:\n",
    "      df_temp = pd.DataFrame(columns=['frame', 'timestamp', 'success', 'AU1', 'AU2', 'AU4', 'AU5', 'AU6', 'AU7', 'AU9',\n",
    "       'AU10', 'AU11', 'AU12', 'AU13', 'AU14', 'AU15', 'AU16', 'AU17', 'AU18',\n",
    "       'AU19', 'AU20', 'AU22', 'AU23', 'AU24', 'AU25', 'AU26', 'AU27', 'AU32',\n",
    "       'AU38', 'AU39'])\n",
    "\n",
    "\n",
    "    # OpenGraphAU - we are filtering out L and R!\n",
    "    if filterOutLR:\n",
    "      df_temp = df_temp.filter(regex='^(?!AUL|AUR)')\n",
    "\n",
    "    # Add the dataframe to the dictionary with the subfolder name as the key\n",
    "    # We do [:-4] to remove '.mp4' from the end of the string\n",
    "    df_dict[subfolder_name[:-4]] = df_temp\n",
    "\n",
    "  return df_dict\n",
    "\n",
    "def create_binary_columns(df, threshold):\n",
    "    # adds classification columns to opengraphAU\n",
    "    for col in df.columns:\n",
    "        if col.startswith('AU'):\n",
    "            # Add _c to the column name for the new column\n",
    "            new_col_name = col + '_c'\n",
    "            # Apply the binary classification to the new column\n",
    "            df[new_col_name] = df[col].apply(lambda x: 1 if x >= threshold else 0)\n",
    "            # Add _r to the original column name\n",
    "            df.rename(columns={col: col + '_r'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def remove_columns_ending_with_r(df):\n",
    "    columns_to_drop = [col for col in df.columns if col.endswith('_r')]\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "def only_successful_frames(df):\n",
    "    # get frames where AU/emotion detection was successful!\n",
    "    return df[df['success'] == 1]\n",
    "\n",
    "\n",
    "def apply_function_to_dict(dictionary, func, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply a function to each DataFrame in a dictionary and return a modified copy of the dictionary.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): The dictionary containing DataFrames.\n",
    "        func (function): The function to apply to each DataFrame.\n",
    "        **kwargs: Additional keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        dict: A modified copy of the dictionary with the function applied to each DataFrame.\n",
    "    \"\"\"\n",
    "    return {key: func(df, **kwargs) for key, df in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "mEea0RmkACqS"
   },
   "outputs": [],
   "source": [
    "dfs_hsemotion = get_dict(COMBINED_OUTPUT_DIRECTORY, file_now='outputs_hse.csv')\n",
    "dfs_hsemotion = apply_function_to_dict(dfs_hsemotion, only_successful_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQjRLc6E7st0",
    "outputId": "97c0e872-abb8-425f-9c59-bb78b8ae6bd9"
   },
   "outputs": [],
   "source": [
    "dfs_opengraphau = get_dict(COMBINED_OUTPUT_DIRECTORY, file_now='outputs_ogau.csv')\n",
    "dfs_opengraphau = apply_function_to_dict(dfs_opengraphau, only_successful_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "EA_FYs-8e12m"
   },
   "outputs": [],
   "source": [
    "# SAVE THE HSEMOTION AND OPENGRAPHAU DICTIONARIES\n",
    "\n",
    "save_var(dfs_hsemotion, forced_name=f'dfs_hsemotion_{PAT_SHORT_NAME}')\n",
    "\n",
    "save_var(dfs_opengraphau, forced_name=f'dfs_opengraphau_{PAT_SHORT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c_gdTS1fAoE"
   },
   "outputs": [],
   "source": [
    "# LOAD THE HSEMOTION AND OPENGRAPHAU DICTIONARIES\n",
    "\n",
    "dfs_hsemotion = load_var(f'dfs_hsemotion_{PAT_SHORT_NAME}')\n",
    "\n",
    "dfs_opengraphau = load_var(f'dfs_opengraphau_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjDp31PcyrBD"
   },
   "source": [
    "# Select Specific Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rrCWvN5Kmjr"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "rgRoAaIsyuFY"
   },
   "outputs": [],
   "source": [
    "def get_data_within_duration(dfs_dict, df_video_timestamps, datetime, duration):\n",
    "    # Takes in:\n",
    "    # dfs_dict -- a dictionary of dataframes containing csv data from one of the pipelines\n",
    "    # df_video_timestamps -- the VideoDateTimes_199 csv\n",
    "    # datetime -- a pd.datetime value to center our extraction\n",
    "    # duration -- a duration (in minutes) BEFORE the datetime to extract\n",
    "\n",
    "    # Outputs:\n",
    "    # One dataframe with all rows we want, with timestamps converted into correct datetimes\n",
    "    start_datetime = datetime - pd.Timedelta(minutes=duration)\n",
    "    end_datetime = datetime\n",
    "\n",
    "    relevant_keys = df_video_timestamps.loc[(pd.to_datetime(df_video_timestamps['VideoEnd']) >= start_datetime) &\n",
    "                                            (pd.to_datetime(df_video_timestamps['VideoStart']) <= end_datetime), 'Filename'].values\n",
    "\n",
    "    relevant_dfs = []\n",
    "    for key in relevant_keys:\n",
    "        if key in dfs_dict:\n",
    "            video_start = pd.to_datetime(df_video_timestamps.loc[df_video_timestamps['Filename'] == key, 'VideoStart'].values[0])\n",
    "            video_end = pd.to_datetime(df_video_timestamps.loc[df_video_timestamps['Filename'] == key, 'VideoEnd'].values[0])\n",
    "            time_mask = ((dfs_dict[key]['timestamp'] >= (start_datetime - video_start).total_seconds()) &\n",
    "                         (dfs_dict[key]['timestamp'] <= (end_datetime - video_start).total_seconds()))\n",
    "            df = dfs_dict[key].loc[time_mask].copy()\n",
    "            df['timestamp'] = video_start + pd.to_timedelta(df['timestamp'], unit='s')\n",
    "            relevant_dfs.append(df)\n",
    "\n",
    "    if relevant_dfs:\n",
    "        df_combined = pd.concat(relevant_dfs, ignore_index=True, sort=False)\n",
    "        df_combined = df_combined.drop(columns='frame')\n",
    "\n",
    "        return df_combined\n",
    "\n",
    "    print(f\"MAJOR ERROR! ZERO RELEVANT DFS!! DATETIME: {datetime}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def get_radius_dict(TIME_RADIUS_IN_MINUTES, INPUT_DF, df_videoTimestamps, df_moodTracking, takeAll=True):\n",
    "  # takes in the:\n",
    "  # --time radius,\n",
    "  # --input dataframe dict (e.g. is it from OpenFace? HSEmotion?)\n",
    "  # --df with video timestamps\n",
    "  # --df with mood tracking patient reports\n",
    "  # --takeAll - are we taking all reports, or filtering out values w/o mood (e.g. anxiety)? True = no filtering\n",
    "\n",
    "  # returns dictionary of timestamp : df with relevant frames\n",
    "\n",
    "  # We'll make a dictionary, with the relevant df for each datetime we have a report\n",
    "  radius_df_dict = {}\n",
    "  for oneIndex in range(len(df_moodTracking)):\n",
    "    # Let's make sure there's a value collected (or takeAll = True)!\n",
    "    if takeAll:\n",
    "      dt_now = get_moodTracking_datetime(oneIndex, df_moodTracking=df_moodTracking)\n",
    "      filtered_df = get_data_within_duration(INPUT_DF, df_videoTimestamps, dt_now, TIME_RADIUS_IN_MINUTES)\n",
    "      radius_df_dict[dt_now] = filtered_df\n",
    "    else:\n",
    "      val_now = df_moodTracking[oneIndex:oneIndex+1]['Anxiety'][oneIndex]\n",
    "      if isinstance(val_now, str):\n",
    "        # Value was collected\n",
    "        dt_now = get_moodTracking_datetime(oneIndex, df_moodTracking=df_moodTracking)\n",
    "        filtered_df = get_data_within_duration(INPUT_DF, df_videoTimestamps, dt_now, TIME_RADIUS_IN_MINUTES)\n",
    "        radius_df_dict[dt_now] = filtered_df\n",
    "      else:\n",
    "        # No value collected!\n",
    "        print('No value for Anxiety for index ', oneIndex, f'corresponding to {get_moodTracking_datetime(oneIndex, df_moodTracking=df_moodTracking)}')\n",
    "  return radius_df_dict\n",
    "\n",
    "def generate_number_list(start, interval, count):\n",
    "    number_list = [start + i * interval for i in range(count)]\n",
    "    return number_list\n",
    "\n",
    "def get_moodTracking_datetime(index, df_moodTracking):\n",
    "  temp_var = pd.to_datetime(pd.to_datetime(df_moodTracking[index:index+1]['Datetime']).dt.strftime('%d-%b-%Y %H:%M:%S'))\n",
    "  return pd.Timestamp(temp_var[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TfTqzxUavwP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i7NiqirKqfs"
   },
   "source": [
    "## Binary Pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JY_iu9F9mKc7",
    "outputId": "5a431c3b-19ae-4ad6-fb2e-677df40a094d"
   },
   "outputs": [],
   "source": [
    "# BINARY PAIN SCORES\n",
    "\n",
    "# Reset verbal pain DF\n",
    "df_verbalPain = df_verbalPain_original\n",
    "\n",
    "takeAll = True # we are taking all patient reports\n",
    "\n",
    "# start and interval are in minutes\n",
    "#TIME_RADIUS_LIST = generate_number_list(start=30, interval=30, count=8)\n",
    "TIME_RADIUS_LIST = [1, 2, 3, 4, 5, 10] + generate_number_list(start=30, interval=30, count=16)\n",
    "#TIME_RADIUS_LIST = [1, 2, 3, 4, 5]\n",
    "\n",
    "ENABLE_OPENFACE = False\n",
    "\n",
    "if ENABLE_OPENFACE:\n",
    "  openface_radius_dict = {}\n",
    "hsemotion_radius_dict = {}\n",
    "opengraphau_radius_dict = {}\n",
    "\n",
    "for i in TIME_RADIUS_LIST:\n",
    "  if ENABLE_OPENFACE:\n",
    "    openface_radius_now = get_radius_dict(i, dfs_openface, df_videoTimestamps, df_verbalPain, takeAll=takeAll)\n",
    "  hsemotion_radius_now = get_radius_dict(i, dfs_hsemotion, df_videoTimestamps, df_verbalPain, takeAll=takeAll)\n",
    "  opengraphau_radius_now = get_radius_dict(i, dfs_opengraphau, df_videoTimestamps, df_verbalPain, takeAll=takeAll)\n",
    "\n",
    "  if i == TIME_RADIUS_LIST[0]:\n",
    "    # Remove invalid timestamps (no video data)\n",
    "    invalid_timestamps = []\n",
    "    if ENABLE_OPENFACE:\n",
    "      for ts_now in list(openface_radius_now.keys()):\n",
    "        if openface_radius_now[ts_now].empty:\n",
    "          invalid_timestamps.append(ts_now)\n",
    "    for ts_now in list(opengraphau_radius_now.keys()):\n",
    "      if opengraphau_radius_now[ts_now].empty:\n",
    "        invalid_timestamps.append(ts_now)\n",
    "    for ts_now in list(hsemotion_radius_now.keys()):\n",
    "      if hsemotion_radius_now[ts_now].empty:\n",
    "        invalid_timestamps.append(ts_now)\n",
    "\n",
    "    # Remove duplicates\n",
    "    invalid_timestamps = list(set(invalid_timestamps))\n",
    "\n",
    "    # Convert to datetimes\n",
    "    invalid_timestamps = pd.to_datetime(invalid_timestamps)\n",
    "\n",
    "    df_verbalPain = df_verbalPain[~pd.to_datetime(df_verbalPain['Datetime']).isin(invalid_timestamps)]\n",
    "    df_verbalPain = df_verbalPain.reset_index(drop=True)\n",
    "\n",
    "    for timestamp in invalid_timestamps:\n",
    "      if ENABLE_OPENFACE:\n",
    "        if timestamp in openface_radius_now:\n",
    "            del openface_radius_now[timestamp]\n",
    "      if timestamp in hsemotion_radius_now:\n",
    "          del hsemotion_radius_now[timestamp]\n",
    "      if timestamp in opengraphau_radius_now:\n",
    "          del opengraphau_radius_now[timestamp]\n",
    "\n",
    "\n",
    "  if ENABLE_OPENFACE:\n",
    "    openface_radius_dict[f'{i}'] = openface_radius_now\n",
    "  hsemotion_radius_dict[f'{i}'] = hsemotion_radius_now\n",
    "  opengraphau_radius_dict[f'{i}'] = opengraphau_radius_now\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Pg6iTljrnZs"
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "df_verbalPain['Pain'] = pd.to_numeric(df_verbalPain['Pain'], errors='coerce')\n",
    "df_verbalPain['Pain'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculating the median of the 'Pain' column\n",
    "median_pain = df_verbalPain['Pain'].median()\n",
    "\n",
    "# Converting 'Pain' values to 0 or 1 based on the median\n",
    "df_verbalPain['Pain'] = (df_verbalPain['Pain'] >= median_pain).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "9oQNHgUdTfmC",
    "outputId": "3ea1c805-6adf-4f85-853d-521886cd7915"
   },
   "outputs": [],
   "source": [
    "df_verbalPain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6Z8-SJWmKdI"
   },
   "outputs": [],
   "source": [
    "# SAVE VARIABLES - BINARY PAIN SCORES\n",
    "\n",
    "#save_var(openface_radius_dict, forced_name=f'openface_radius_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "save_var(hsemotion_radius_dict, forced_name=f'hsemotion_radius_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "save_var(opengraphau_radius_dict, forced_name=f'opengraphau_radius_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "save_var(df_verbalPain, forced_name=f'df_verbalPain_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61y41Oo4mKdI"
   },
   "outputs": [],
   "source": [
    "# LOAD VARIABLES - BINARY PAIN SCORES\n",
    "\n",
    "#openface_radius_dict = load_var(f'openface_radius_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "hsemotion_radius_dict = load_var(f'hsemotion_radius_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "opengraphau_radius_dict = load_var(f'opengraphau_radius_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "df_verbalPain = load_var(f'df_verbalPain_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3GZCcHA-jaR",
    "outputId": "8546296e-5aa7-4fd5-91cf-9468b2a4acd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([Timestamp('2023-04-01 10:41:00'), Timestamp('2023-04-01 13:26:00'), Timestamp('2023-04-01 14:51:00'), Timestamp('2023-04-02 12:28:00'), Timestamp('2023-04-02 13:33:00'), Timestamp('2023-04-03 12:35:00'), Timestamp('2023-04-03 14:23:00'), Timestamp('2023-04-03 16:00:00'), Timestamp('2023-04-04 10:15:00'), Timestamp('2023-04-04 12:36:00'), Timestamp('2023-04-04 15:31:00'), Timestamp('2023-04-04 16:05:00'), Timestamp('2023-04-05 09:42:00'), Timestamp('2023-04-05 14:34:00'), Timestamp('2023-04-05 22:00:00'), Timestamp('2023-04-06 11:33:00'), Timestamp('2023-04-06 14:05:00'), Timestamp('2023-04-06 18:38:00'), Timestamp('2023-04-06 20:30:00')])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsemotion_radius_dict['60'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "YiGJoUmQAuEr",
    "outputId": "7eb1fcd8-a5bb-4e4f-cff8-3216811551b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-fc905ce9-4dd3-41ec-9b70-76d726a500e6\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>success</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Happiness</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-01 13:51:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.423498</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>0.339198</td>\n",
       "      <td>0.207993</td>\n",
       "      <td>0.001749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-01 13:51:00.200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.621612</td>\n",
       "      <td>0.023388</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>0.221857</td>\n",
       "      <td>0.127470</td>\n",
       "      <td>0.001496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-01 13:51:00.400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.460258</td>\n",
       "      <td>0.028619</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.215181</td>\n",
       "      <td>0.287028</td>\n",
       "      <td>0.002693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-01 13:51:00.600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.402869</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>0.255569</td>\n",
       "      <td>0.325414</td>\n",
       "      <td>0.000875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-01 13:51:00.800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.423747</td>\n",
       "      <td>0.018580</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.245403</td>\n",
       "      <td>0.297931</td>\n",
       "      <td>0.003618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34990</th>\n",
       "      <td>2023-04-01 15:50:59.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680746</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.164397</td>\n",
       "      <td>0.150494</td>\n",
       "      <td>0.000789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34991</th>\n",
       "      <td>2023-04-01 15:50:59.200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702113</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.010603</td>\n",
       "      <td>0.187238</td>\n",
       "      <td>0.083686</td>\n",
       "      <td>0.006443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34992</th>\n",
       "      <td>2023-04-01 15:50:59.400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605561</td>\n",
       "      <td>0.005314</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.098516</td>\n",
       "      <td>0.006741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34993</th>\n",
       "      <td>2023-04-01 15:50:59.600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.413575</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.020848</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>0.343801</td>\n",
       "      <td>0.156234</td>\n",
       "      <td>0.028441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34994</th>\n",
       "      <td>2023-04-01 15:50:59.800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.611558</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.011189</td>\n",
       "      <td>0.294188</td>\n",
       "      <td>0.058272</td>\n",
       "      <td>0.013688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34995 rows × 9 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc905ce9-4dd3-41ec-9b70-76d726a500e6')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-fc905ce9-4dd3-41ec-9b70-76d726a500e6 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-fc905ce9-4dd3-41ec-9b70-76d726a500e6');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                    timestamp  success     Anger   Disgust      Fear  \\\n",
       "0     2023-04-01 13:51:00.000      1.0  0.423498  0.021201  0.000888   \n",
       "1     2023-04-01 13:51:00.200      1.0  0.621612  0.023388  0.000651   \n",
       "2     2023-04-01 13:51:00.400      1.0  0.460258  0.028619  0.003752   \n",
       "3     2023-04-01 13:51:00.600      1.0  0.402869  0.009147  0.000898   \n",
       "4     2023-04-01 13:51:00.800      1.0  0.423747  0.018580  0.002886   \n",
       "...                       ...      ...       ...       ...       ...   \n",
       "34990 2023-04-01 15:50:59.000      1.0  0.680746  0.001486  0.000841   \n",
       "34991 2023-04-01 15:50:59.200      1.0  0.702113  0.004676  0.005242   \n",
       "34992 2023-04-01 15:50:59.400      1.0  0.605561  0.005314  0.005634   \n",
       "34993 2023-04-01 15:50:59.600      1.0  0.413575  0.007948  0.020848   \n",
       "34994 2023-04-01 15:50:59.800      1.0  0.611558  0.004269  0.006836   \n",
       "\n",
       "       Happiness   Neutral   Sadness  Surprise  \n",
       "0       0.005473  0.339198  0.207993  0.001749  \n",
       "1       0.003526  0.221857  0.127470  0.001496  \n",
       "2       0.002469  0.215181  0.287028  0.002693  \n",
       "3       0.005228  0.255569  0.325414  0.000875  \n",
       "4       0.007836  0.245403  0.297931  0.003618  \n",
       "...          ...       ...       ...       ...  \n",
       "34990   0.001248  0.164397  0.150494  0.000789  \n",
       "34991   0.010603  0.187238  0.083686  0.006443  \n",
       "34992   0.009335  0.268900  0.098516  0.006741  \n",
       "34993   0.029153  0.343801  0.156234  0.028441  \n",
       "34994   0.011189  0.294188  0.058272  0.013688  \n",
       "\n",
       "[34995 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsemotion_radius_dict['60'][get_moodTracking_datetime(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b2NLAbBrOKb"
   },
   "source": [
    "# Feature Extraction 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gneOt-s3QrmF"
   },
   "source": [
    "## Emotion Processing Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGCl95VtQrmF"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def binarize_cols(df, threshold=0.5):\n",
    "  new_df = df.copy()\n",
    "  emotions = [col for col in new_df.columns if col not in ['frame', 'success', 'timestamp']]\n",
    "\n",
    "  for emotion in emotions:\n",
    "      new_df[f'{emotion}_Raw'] = new_df[emotion]\n",
    "      new_df[f'{emotion}_Binary'] = (new_df[f'{emotion}_Raw'] >= threshold).astype(int)\n",
    "\n",
    "  new_df = new_df.drop(columns=emotions, inplace=False)\n",
    "\n",
    "  return new_df\n",
    "\n",
    "\n",
    "def fill_empty_dfs(dictionary):\n",
    "  # when we do emotion processing, some dfs will have ZERO successful frames,\n",
    "  # leading to ZERO events, and an empty df.\n",
    "  # we need to fill the empty dfs with a df with all 0s\n",
    "\n",
    "  non_empty_dfs = [df for df in dictionary.values() if not df.empty]\n",
    "\n",
    "  if not non_empty_dfs:\n",
    "      return dictionary  # Return the original dictionary if all DataFrames are empty\n",
    "\n",
    "  non_empty_df = non_empty_dfs[0]  # Choose the first non-empty DataFrame as replacement\n",
    "\n",
    "  modified_dictionary = {}\n",
    "  for key, df in dictionary.items():\n",
    "      if df.empty:\n",
    "          modified_df = pd.DataFrame(0, index=non_empty_df.index, columns=non_empty_df.columns)\n",
    "          # Preserve string columns from non-empty DataFrame\n",
    "          for column in non_empty_df.columns:\n",
    "              if non_empty_df[column].dtype == object:\n",
    "                  modified_df[column] = non_empty_df[column]\n",
    "      else:\n",
    "          modified_df = df.copy()\n",
    "\n",
    "      modified_dictionary[key] = modified_df\n",
    "\n",
    "  return modified_dictionary\n",
    "\n",
    "def analyze_emotion_events_v2(df, max_frame_gap=10, event_minimum_num_frames=1, method='HSE'):\n",
    "    # Emotions to analyze\n",
    "    emotions_raw = [col for col in df.columns if col not in ['frame', 'success', 'timestamp']]\n",
    "    # Removing \"_Raw\" or \"_Binary\" from each string\n",
    "    processed_strings = [s.replace(\"_Raw\", \"\").replace(\"_Binary\", \"\") for s in emotions_raw]\n",
    "    # Eliminating duplicates\n",
    "    emotions = list(set(processed_strings))\n",
    "\n",
    "    # Create DataFrame for results\n",
    "    if STATS_FEATURE_SETTING == 0:\n",
    "        results_df = pd.DataFrame(index=['avg_event_length', 'avg_event_duration', 'total_num_events', 'avg_probability', 'std', 'skewness', 'kurtosis', 'autocorrelation', 'pres_pct'])\n",
    "    elif STATS_FEATURE_SETTING == 1 or (STATS_FEATURE_SETTING == 3 and method == 'HSE'):\n",
    "        results_df = pd.DataFrame(index=['avg_event_length', 'total_num_events', 'avg_probability', 'std', 'pres_pct'])\n",
    "    elif STATS_FEATURE_SETTING == 2:\n",
    "        results_df = pd.DataFrame(index=['pres_pct'])\n",
    "    elif STATS_FEATURE_SETTING == 3 and (method == 'OGAU' or method=='OF'):\n",
    "        results_df = pd.DataFrame(index=['pres_pct', 'total_num_events'])\n",
    "\n",
    "\n",
    "    def detect_events(emotion_binary_col):\n",
    "        probThreshold = 0.5 # irrelevant because it's a binary column\n",
    "        minInterval = max_frame_gap\n",
    "        minDuration = event_minimum_num_frames\n",
    "\n",
    "        probBinary = emotion_binary_col > probThreshold\n",
    "\n",
    "        # Using np.diff to find changes in the binary array\n",
    "        changes = np.diff(probBinary.astype(int))\n",
    "\n",
    "        # Identify start (1) and stop (-1) points\n",
    "        starts = np.where(changes == 1)[0] + 1  # +1 to correct the index shift caused by diff\n",
    "        stops = np.where(changes == -1)[0] + 1\n",
    "\n",
    "        # Adjust for edge cases\n",
    "        if probBinary.iloc[0]:\n",
    "            starts = np.insert(starts, 0, 0)\n",
    "        if probBinary.iloc[-1]:\n",
    "            stops = np.append(stops, len(probBinary))\n",
    "\n",
    "        # Merge close events and filter by duration\n",
    "        events = []\n",
    "        for start, stop in zip(starts, stops):\n",
    "\n",
    "            # Construct the event considering only indices where probBinary is 1\n",
    "            event = np.arange(start, stop)[probBinary[start:stop].values]\n",
    "\n",
    "            # Check if there is a previous event to potentially merge with\n",
    "            if events and event.size > 0 and events[-1][-1] >= start - minInterval:\n",
    "                # Merge with the previous event\n",
    "                events[-1] = np.unique(np.concatenate([events[-1], event]))\n",
    "            elif event.size >= event_minimum_num_frames:\n",
    "                events.append(event)\n",
    "\n",
    "        # Filter events by minimum duration\n",
    "        valid_events = [event for event in events if len(event) >= minDuration]\n",
    "\n",
    "        return valid_events\n",
    "\n",
    "    for emotion in emotions:\n",
    "        # Identify events\n",
    "        emotion_binary_col = df[f'{emotion}_Binary']\n",
    "        emotion_presence = df[f'{emotion}_Binary'].sum()\n",
    "        pres_pct = emotion_presence / len(df) * 100  # Percentage of frames where emotion is present\n",
    "        events = detect_events(emotion_binary_col)\n",
    "\n",
    "        if not(STATS_FEATURE_SETTING == 2):\n",
    "            # Calculate features for each event\n",
    "            if events:\n",
    "                event_lengths = [len(event) for event in events]\n",
    "                event_durations = [event[-1] - event[0] + 1 for event in events]\n",
    "                probabilities = [df.loc[event, f'{emotion}_Raw'].values for event in events]\n",
    "                probabilities_flattened = np.concatenate(probabilities)\n",
    "\n",
    "                avg_event_length = np.mean(event_lengths)\n",
    "                avg_event_duration = np.mean(event_durations)\n",
    "                total_num_events = len(events)\n",
    "                avg_probability = np.mean(probabilities_flattened)\n",
    "                std_dev = np.std(probabilities_flattened)\n",
    "                skewness_val = skew(probabilities_flattened)\n",
    "                kurtosis_val = kurtosis(probabilities_flattened)\n",
    "                autocorr = acf(probabilities_flattened, fft=True, nlags=1)[1] if len(probabilities_flattened) > 1 else 0\n",
    "            else:\n",
    "                avg_event_length = 0\n",
    "                avg_event_duration = 0\n",
    "                total_num_events = 0\n",
    "                avg_probability = 0\n",
    "                std_dev = 0\n",
    "                skewness_val = 0\n",
    "                kurtosis_val = 0\n",
    "                autocorr = 0\n",
    "\n",
    "        # Add results to the DataFrame\n",
    "        if STATS_FEATURE_SETTING == 0:\n",
    "            results_df[emotion] = [avg_event_length, avg_event_duration, total_num_events, avg_probability, std_dev, skewness_val, kurtosis_val, autocorr, pres_pct]\n",
    "        elif STATS_FEATURE_SETTING == 1 or (STATS_FEATURE_SETTING == 3 and method == 'HSE'):\n",
    "            results_df[emotion] = [avg_event_length, total_num_events, avg_probability, std_dev, pres_pct]\n",
    "        elif STATS_FEATURE_SETTING == 2:\n",
    "            results_df[emotion] = [pres_pct]\n",
    "        elif STATS_FEATURE_SETTING == 3 and (method == 'OGAU' or method=='OF'):\n",
    "            results_df[emotion] = [pres_pct, total_num_events]\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    results_df.fillna(0, inplace=True)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EU7WhJIQrmG",
    "outputId": "a1665e71-a911-4391-ec34-832bcecd68dc"
   },
   "outputs": [],
   "source": [
    "# dictionary to store results from each different time window we test!\n",
    "hsemotion_emo_stats_dict = {}\n",
    "\n",
    "for time_radius, hsemotion_radius_now in hsemotion_radius_dict.items():\n",
    "  print('Time Radius: ', time_radius)\n",
    "  if NORMALIZE_DATA == 0:\n",
    "    THRESHOLD = 0.4\n",
    "  elif NORMALIZE_DATA == 1:\n",
    "    THRESHOLD = 2.5\n",
    "  hsemotion_radius_binarized = apply_function_to_dict(hsemotion_radius_now, binarize_cols, threshold=THRESHOLD)\n",
    "  hsemotion_emo_stats = apply_function_to_dict(hsemotion_radius_binarized, analyze_emotion_events_v2, max_frame_gap=10, event_minimum_num_frames=12, method='HSE')\n",
    "  hsemotion_emo_stats_fixed = fill_empty_dfs(hsemotion_emo_stats)\n",
    "  hsemotion_emo_stats_dict[time_radius] = hsemotion_emo_stats_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lx43Ki9MAI0",
    "outputId": "49ca4855-afa9-4f57-824f-f843df3a4f6d"
   },
   "outputs": [],
   "source": [
    "# OPENGRAPHAU AU EVENTS\n",
    "\n",
    "# dictionary to store results from each different time window we test!\n",
    "opengraphau_au_events_stats_dict = {}\n",
    "\n",
    "for time_radius, opengraphau_radius_now in opengraphau_radius_dict.items():\n",
    "  print('Time Radius: ', time_radius)\n",
    "  opengraphau_radius_binarized = apply_function_to_dict(opengraphau_radius_now, binarize_cols, threshold=THRESHOLD)\n",
    "  opengraphau_au_events_stats = apply_function_to_dict(opengraphau_radius_binarized, analyze_emotion_events_v2, max_frame_gap=10, event_minimum_num_frames=12, method='OGAU')\n",
    "  opengraphau_au_events_stats_fixed = fill_empty_dfs(opengraphau_au_events_stats)\n",
    "  opengraphau_au_events_stats_dict[time_radius] = opengraphau_au_events_stats_fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkcRVBK7CZaJ"
   },
   "source": [
    "# Make Vectors for Each Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFkY8uARYsGL"
   },
   "source": [
    "## Vectors for AU and emotion classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1afeC95tCzgK"
   },
   "outputs": [],
   "source": [
    "## Dictionary of list of relevant dictionaries\n",
    "openface_dict_list_dict = {}\n",
    "\n",
    "for key in openface_au_derived_dict.keys():\n",
    "  openface_dict_list_dict[key] = [openface_au_derived_dict[key], openface_emo_stats_dict[key], openface_ee_derived_dict[key], openface_oe_derived_dict[key]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xa9VoJpoIz1A"
   },
   "outputs": [],
   "source": [
    "opengraphau_dict_list_dict = {}\n",
    "\n",
    "for key in opengraphau_au_events_stats_dict.keys():\n",
    "  #opengraphau_dict_list_dict[key] = [opengraphau_au_derived_dict[key], opengraphau_ee_derived_dict[key]]\n",
    "  opengraphau_dict_list_dict[key] = [opengraphau_au_events_stats_dict[key]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jmlv3wwIz7e"
   },
   "outputs": [],
   "source": [
    "hsemotion_dict_list_dict = {}\n",
    "\n",
    "for key in hsemotion_emo_stats_dict.keys():\n",
    "  #hsemotion_dict_list_dict[key] = [hsemotion_emo_stats_dict[key], hsemotion_ee_derived_dict[key]]\n",
    "  #hsemotion_dict_list_dict[key] = [hsemotion_ee_derived_dict[key]]\n",
    "  hsemotion_dict_list_dict[key] = [hsemotion_emo_stats_dict[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4DmMSlLa_GC"
   },
   "outputs": [],
   "source": [
    "def partial_combine_dictionaries(dict1, dict2):\n",
    "    # Takes element one (i.e. the AU matrix) from dict1, and all of dict2 (i.e. HSEmotion)\n",
    "    combined_dict = {}\n",
    "\n",
    "    for key in dict1:\n",
    "        combined_dict[key] = [dict1[key][0]] + dict2[key]\n",
    "\n",
    "    return combined_dict\n",
    "\n",
    "def full_combine_dictionaries(dict_list):\n",
    "    combined_dict = {}\n",
    "\n",
    "    for key in dict_list[0]:\n",
    "        combined_dict[key] = []\n",
    "        for j in dict_list:\n",
    "          combined_dict[key] = combined_dict[key] + j[key]\n",
    "\n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GNy0xOKbEBj"
   },
   "outputs": [],
   "source": [
    "#ofauhsemotion_dict_list_dict = partial_combine_dictionaries(openface_dict_list_dict, hsemotion_dict_list_dict)\n",
    "\n",
    "ogauhsemotion_dict_list_dict = partial_combine_dictionaries(opengraphau_dict_list_dict, hsemotion_dict_list_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k53vTvAQcMAf"
   },
   "outputs": [],
   "source": [
    "all_dict_list_dict = full_combine_dictionaries([openface_dict_list_dict, opengraphau_dict_list_dict, hsemotion_dict_list_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pBKMDGo4I1Q"
   },
   "outputs": [],
   "source": [
    "# SAVE VARIABLES - BINARY PAIN\n",
    "\n",
    "#save_var(openface_dict_list_dict, forced_name=f'openface_dict_list_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "save_var(opengraphau_dict_list_dict, forced_name=f'opengraphau_dict_list_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "save_var(hsemotion_dict_list_dict, forced_name=f'hsemotion_dict_list_dict_pain_{PAT_SHORT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsmJODbw4I1R"
   },
   "outputs": [],
   "source": [
    "# SAVE VARIABLES - BINARY PAIN\n",
    "\n",
    "save_var(ofauhsemotion_dict_list_dict, forced_name=f'ofauhsemotion_dict_list_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "save_var(ogauhsemotion_dict_list_dict, forced_name=f'ogauhsemotion_dict_list_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "save_var(all_dict_list_dict, forced_name=f'all_dict_list_dict_pain_{PAT_SHORT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qyfu6zz44I1R"
   },
   "outputs": [],
   "source": [
    "# LOAD VARIABLES - BINARY PAIN\n",
    "\n",
    "#openface_dict_list_dict = load_var(f'openface_dict_list_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "opengraphau_dict_list_dict = load_var(f'opengraphau_dict_list_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "#hsemotion_dict_list_dict = load_var(f'hsemotion_dict_list_dict_pain_{PAT_SHORT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sk0nN-o74I1R"
   },
   "outputs": [],
   "source": [
    "# LOAD VARIABLES - BINARY PAIN\n",
    "\n",
    "ofauhsemotion_dict_list_dict = load_var(f'ofauhsemotion_dict_list_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "ogauhsemotion_dict_list_dict = load_var(f'ogauhsemotion_dict_list_dict_pain_{PAT_SHORT_NAME}')\n",
    "\n",
    "all_dict_list_dict = load_var(f'all_dict_list_dict_pain_{PAT_SHORT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cxl0hxv2z0TG"
   },
   "outputs": [],
   "source": [
    "# LOAD LABELS - BINARY PAIN\n",
    "\n",
    "df_verbalPain = load_var(f'df_verbalPain_{PAT_SHORT_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcZea69MJ9Lw"
   },
   "outputs": [],
   "source": [
    "def flatten_dataframes_dict(dataframes_list):\n",
    "    # Initialize an empty dictionary to store the flattened data for each key\n",
    "    flattened_data_dict = {}\n",
    "\n",
    "    # Define the columns to ignore\n",
    "    ignore_columns = ['success', 'timestamp', 'AU', 'emotion']\n",
    "\n",
    "    for dataframes_dict in dataframes_list:\n",
    "       for key, df in dataframes_dict.items():\n",
    "          # Filter out the columns to be ignored\n",
    "          filtered_df = df.drop(columns=[col for col in ignore_columns if col in df.columns])\n",
    "\n",
    "          # Flatten the data by converting each DataFrame into a 1D array\n",
    "          flattened_array = filtered_df.select_dtypes(include=[np.number, int, float, complex, \\\n",
    "                                                                pd.Int64Dtype(), pd.Float64Dtype(), pd.Int32Dtype(), \\\n",
    "                                                                pd.Float32Dtype()]).values.flatten()\n",
    "\n",
    "          # Convert the flattened array to NumPy array and store it in the dictionary\n",
    "          if key in flattened_data_dict:\n",
    "              flattened_data_dict[key] = np.concatenate((flattened_data_dict[key], flattened_array))\n",
    "          else:\n",
    "              flattened_data_dict[key] = np.array(flattened_array)\n",
    "\n",
    "    return flattened_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_kZiz0dKDcm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "openface_vectors_dict = {}\n",
    "\n",
    "for key, openface_dict_list_now in openface_dict_list_dict.items():\n",
    "  openface_vectors_dict[key] = flatten_dataframes_dict(openface_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2P1ntu5JF9A"
   },
   "outputs": [],
   "source": [
    "opengraphau_vectors_dict = {}\n",
    "\n",
    "for key, opengraphau_dict_list_now in opengraphau_dict_list_dict.items():\n",
    "  opengraphau_vectors_dict[key] = flatten_dataframes_dict(opengraphau_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEBJtszEJGPD"
   },
   "outputs": [],
   "source": [
    "hsemotion_vectors_dict = {}\n",
    "\n",
    "for key, hsemotion_dict_list_now in hsemotion_dict_list_dict.items():\n",
    "  hsemotion_vectors_dict[key] = flatten_dataframes_dict(hsemotion_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJQe4v4abTIe"
   },
   "outputs": [],
   "source": [
    "ofauhsemotion_vectors_dict = {}\n",
    "\n",
    "for key, ofauhsemotion_dict_list_now in ofauhsemotion_dict_list_dict.items():\n",
    "  ofauhsemotion_vectors_dict[key] = flatten_dataframes_dict(ofauhsemotion_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QhobZ1wbTL2"
   },
   "outputs": [],
   "source": [
    "ogauhsemotion_vectors_dict = {}\n",
    "\n",
    "for key, ogauhsemotion_dict_list_now in ogauhsemotion_dict_list_dict.items():\n",
    "  ogauhsemotion_vectors_dict[key] = flatten_dataframes_dict(ogauhsemotion_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dn_OpS2jcZnF"
   },
   "outputs": [],
   "source": [
    "all_vectors_dict = {}\n",
    "\n",
    "for key, all_dict_list_now in all_dict_list_dict.items():\n",
    "  all_vectors_dict[key] = flatten_dataframes_dict(all_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeN1ukKDLHVo",
    "outputId": "f8c5de74-61ef-4deb-8479-4eeca8eec32d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openface_vectors_dict['60'][get_moodTracking_datetime(0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L60VuGulO29n",
    "outputId": "40b75eab-17e8-4bd7-a16b-5f5ee9d401b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34,)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opengraphau_vectors_dict['60'][get_moodTracking_datetime(0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bKBfxsR8O4OI",
    "outputId": "91eb178b-eaab-4bae-cfcb-0ea6fc89de55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77,)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsemotion_vectors_dict['60'][get_moodTracking_datetime(0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2xNZSwmbfLM",
    "outputId": "d96afe03-2e6f-4b5c-b8da-fe229a4f36c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofauhsemotion_vectors_dict['60'][get_moodTracking_datetime(0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cuRt5B5GbfOi",
    "outputId": "790b1b64-5d24-469f-f9f1-f48c71d2893f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ogauhsemotion_vectors_dict['60'][get_moodTracking_datetime(0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XduQPeAQce5X",
    "outputId": "09ec5d8f-3904-48af-90d5-bf550b745d3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vectors_dict['60'][get_moodTracking_datetime(0)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9apyQQMfIso"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "So5hSPmHfIv7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gx6_bCNQV6uD"
   },
   "source": [
    "## Labels - datetime conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5hRicOaOQVtq"
   },
   "outputs": [],
   "source": [
    "def ts_to_str(timestamp):\n",
    "    return timestamp.strftime('%-m/%-d/%Y %H:%M:%S')\n",
    "\n",
    "def str_to_ts(string_now):\n",
    "  temp_var = pd.to_datetime(pd.to_datetime(string_now).strftime('%d-%b-%Y %H:%M:%S'))\n",
    "  return pd.Timestamp(temp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NpvfE0TfXduU",
    "outputId": "49c48362-c5f5-4327-e331-5128d408971a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'4/2/2023 18:40:00'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_to_str(list(openface_vectors_dict['60'].keys())[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJvxb39jc9Pg",
    "outputId": "c4c13a2f-1785-40f4-c8b5-7279dc4fc66f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-04-06 20:30:00')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_to_ts('4/6/2023 20:30:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLtMi6TUvm9K"
   },
   "source": [
    "## Save to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "6Es85PqYzSht"
   },
   "outputs": [],
   "source": [
    "def ts_to_str_save(timestamp):\n",
    "    # shorter version bc xlsxwriter sheet name char limit\n",
    "    return timestamp.strftime('%-m_%-d %H_%M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "ntLtIcQow-03"
   },
   "outputs": [],
   "source": [
    "## Save our vectors to excel sheets!\n",
    "\n",
    "def get_dict_name(dictionary):\n",
    "    namespace = globals()\n",
    "    for name, obj in namespace.items():\n",
    "        if isinstance(obj, dict) and obj is dictionary:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def save_dicts_to_excel(dict_list, output_path):\n",
    "  # Create an Excel writer object\n",
    "  writer = pd.ExcelWriter(output_path, engine='xlsxwriter')\n",
    "\n",
    "  # Iterate over the keys in the dictionaries\n",
    "  for key in dict_list[0].keys():\n",
    "      # Write each dataframe to a separate sheet with the corresponding key as the sheet name\n",
    "      for enum, dict_now in enumerate(dict_list):\n",
    "        name_var = f'Matrix_{enum}'\n",
    "        sheet_name_starter = f'{ts_to_str_save(key)}_{name_var}'\n",
    "        dict_now[key].to_excel(writer, sheet_name=sheet_name_starter[:31])\n",
    "\n",
    "  # Save the Excel file\n",
    "  writer.close()\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtF5RndQv9qj"
   },
   "source": [
    "### Multi-hour features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-zpJEZSyU2S"
   },
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(FEATURE_VIS_PATH, exist_ok=True)\n",
    "\n",
    "for i in opengraphau_dict_list_dict.keys():\n",
    "  #save_dicts_to_excel(openface_dict_list_dict[i], FEATURE_VIS_PATH + f'openface_pain_{PAT_SHORT_NAME}_{int(i)}_minutes.xlsx')\n",
    "  save_dicts_to_excel(opengraphau_dict_list_dict[i], FEATURE_VIS_PATH + f'opengraphau_pain_{PAT_SHORT_NAME}_{int(i)}_minutes.xlsx')\n",
    "  save_dicts_to_excel(hsemotion_dict_list_dict[i], FEATURE_VIS_PATH + f'hsemotion_pain_{PAT_SHORT_NAME}_{int(i)}_minutes.xlsx')\n",
    "  save_dicts_to_excel(ogauhsemotion_dict_list_dict[i], FEATURE_VIS_PATH + f'ogauhse_pain_{PAT_SHORT_NAME}_{int(i)}_minutes.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgTxVzgzgoI2"
   },
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOJEGd81yFRH"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Wcz3A-wgp7o"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(input_num):\n",
    "    \"\"\"\n",
    "    Sets the random seed for both the random and numpy libraries.\n",
    "\n",
    "    Parameters:\n",
    "    input_num (int): The seed number to set.\n",
    "    \"\"\"\n",
    "    random.seed(input_num)\n",
    "    np.random.seed(input_num)\n",
    "\n",
    "\n",
    "def cross_validate_model_stratified(model, X, y, n_folds, random_state=42):\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    auroc_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    y_test_all = []\n",
    "    y_proba_all = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        y_test_all = y_test_all + list(y_test)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_proba_all = y_proba_all + list(y_proba)\n",
    "\n",
    "    mean_acc = np.mean(accuracy_scores)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_all, y_proba_all)\n",
    "    mean_auroc = roc_auc_score(y_test_all, y_proba_all)\n",
    "\n",
    "    roc_data = list(zip(fpr, tpr))\n",
    "\n",
    "    return mean_auroc, mean_acc, roc_data\n",
    "\n",
    "\n",
    "def average_xy_pairs(list_of_x_lists, list_of_y_lists):\n",
    "    \"\"\"\n",
    "    Averages over x-y pairings from lists of lists of x and y values.\n",
    "\n",
    "    Parameters:\n",
    "    list_of_x_lists (list of list of float): List of lists containing x values.\n",
    "    list_of_y_lists (list of list of float): List of lists containing y values, corresponding to x values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A pair of lists (x_list, y_list) containing averaged x and corresponding y values.\n",
    "    \"\"\"\n",
    "    # Create a standard x-axis from 0 to 1\n",
    "    standard_x = np.linspace(0, 1, 100)\n",
    "\n",
    "    # Initialize a list to store interpolated y values\n",
    "    interpolated_y_values = []\n",
    "\n",
    "    # Iterate over each pair of x and y lists\n",
    "    for x_list, y_list in zip(list_of_x_lists, list_of_y_lists):\n",
    "        # Interpolate y values for the standard x-axis\n",
    "        interpolated_y = np.interp(standard_x, x_list, y_list)\n",
    "        interpolated_y_values.append(interpolated_y)\n",
    "\n",
    "    # Calculate the average of interpolated y values\n",
    "    average_y = np.mean(interpolated_y_values, axis=0)\n",
    "\n",
    "    # Return the standard x-axis and the average y values\n",
    "    return standard_x, average_y\n",
    "\n",
    "def prepare_data(datetime_features_dict, df, cross_val=True):\n",
    "    # Convert datetime_features_dict to DataFrame\n",
    "    feature_df = pd.DataFrame.from_dict(datetime_features_dict, orient='index')\n",
    "    feature_df.index.name = 'Datetime'\n",
    "\n",
    "    # Convert Datetime column in df to datetime type\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "    # Concatenate feature DataFrame with df\n",
    "    merged_df = pd.concat([df.set_index('Datetime'), feature_df], axis=1, join='inner')\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = merged_df.drop(['Pain'], axis=1)\n",
    "    y = merged_df['Pain']\n",
    "\n",
    "    y = y.astype(int)\n",
    "\n",
    "    if cross_val:\n",
    "      return X, y\n",
    "    else:\n",
    "      # Split the data into training and testing sets\n",
    "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)\n",
    "\n",
    "      return X_train, X_test, y_train, y_test\n",
    "\n",
    "def get_metrics_binClass_no_alpha_search(vectors_dict, labels_df, save_path, title, verbose=False):\n",
    "  auroc_dict = {}\n",
    "  acc_dict = {}\n",
    "\n",
    "  for key in vectors_dict.keys():\n",
    "    num_minutes = round(float(key))\n",
    "    save_path_plotting = save_path[:-4] + f'_{num_minutes}.png'\n",
    "    if verbose:\n",
    "      print(f'CURRENT WINDOW: {num_minutes} minutes')\n",
    "\n",
    "    results_prep = prepare_data(vectors_dict[key], labels_df, cross_val=True)\n",
    "\n",
    "    X, y = results_prep\n",
    "\n",
    "    # Fix nan issue\n",
    "    missing_indices = X[X.isnull().any(axis=1)].index\n",
    "    X = X.dropna()\n",
    "    y = y.drop(missing_indices)\n",
    "    X = X.values\n",
    "    y = y.values.astype(float)\n",
    "    NUM_OVERALL_LOOPS = 10\n",
    "    fprs_all = []\n",
    "    tprs_all = []\n",
    "    aurocs_all = []\n",
    "    accs_all = []\n",
    "\n",
    "    for loop_now in range(NUM_OVERALL_LOOPS):\n",
    "\n",
    "      # Train Logistic Regression model via 10-fold CV\n",
    "      alpha = 0.1\n",
    "      logreg_model = LogisticRegression(penalty='l1', C=alpha, solver='liblinear')\n",
    "\n",
    "      avg_auroc, avg_accuracy, roc_data = cross_validate_model_stratified(logreg_model, X, y, n_folds=10, random_state=loop_now)\n",
    "\n",
    "      # Update dicts\n",
    "      accs_all.append(avg_accuracy)\n",
    "      aurocs_all.append(avg_auroc)\n",
    "\n",
    "      fprs = []\n",
    "      tprs = []\n",
    "\n",
    "      for fpr, tpr in roc_data:\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "\n",
    "      fprs_all.append(fprs)\n",
    "      tprs_all.append(tprs)\n",
    "\n",
    "      plt.plot(fprs, tprs, color='lightblue', lw=2, label='ROC (AUROC = {:.2f})'.format(avg_auroc))\n",
    "\n",
    "    averaged_x, averaged_y = average_xy_pairs(fprs_all, tprs_all)\n",
    "\n",
    "    roc_data = list(zip(averaged_x, averaged_y))\n",
    "    mean_mean_auroc = np.mean(aurocs_all)\n",
    "    mean_mean_acc = np.mean(accs_all)\n",
    "\n",
    "    roc_csv_path = save_path[:-4] + f'_AvgROC_{num_minutes}.csv'\n",
    "    with open(roc_csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['False Positive Rate', 'True Positive Rate'])\n",
    "        for fpr, tpr in roc_data:\n",
    "            writer.writerow([fpr, tpr])\n",
    "\n",
    "    plt.plot(averaged_x, averaged_y, color='blue', lw=2, label='Overall ROC (AUROC = {:.2f})'.format(mean_mean_auroc))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Random Chance')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Average Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Save the plot\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path_plotting, bbox_inches='tight')\n",
    "    if verbose:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Add to dicts\n",
    "    auroc_dict[key] = mean_mean_auroc\n",
    "    acc_dict[key] = mean_mean_acc\n",
    "\n",
    "  return auroc_dict, acc_dict\n",
    "\n",
    "\n",
    "def get_metrics_binClass(vectors_dict, labels_df, shuffled_labels_df, save_path, title, cross_val=True, verbose=False):\n",
    "    # Does a Nested CV alpha search\n",
    "    # Does 10-times 10-fold CV\n",
    "    # Returns AUROCs and Accuracies for each time window\n",
    "\n",
    "    auroc_dict = {}\n",
    "    acc_dict = {}\n",
    "    auroc_dict_shuf = {}\n",
    "    acc_dict_shuf = {}\n",
    "\n",
    "    # Define the range of alphas for the LASSO regression\n",
    "    alpha_range = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "    for key in vectors_dict.keys():\n",
    "        num_minutes = round(float(key))\n",
    "        save_path_plotting = save_path[:-4] + f'_{num_minutes}.png'\n",
    "        if verbose:\n",
    "            print(f'CURRENT WINDOW: {num_minutes} minutes')\n",
    "\n",
    "        X, y = prepare_data(vectors_dict[key], labels_df, cross_val=cross_val)\n",
    "        _, y_shuf = prepare_data(vectors_dict[key], shuffled_labels_df, cross_val=cross_val)\n",
    "\n",
    "        # Fix nan issue\n",
    "        missing_indices = X[X.isnull().any(axis=1)].index\n",
    "        X = X.dropna()\n",
    "        y = y.drop(missing_indices)\n",
    "        y_shuf = y_shuf.drop(missing_indices)\n",
    "        X = X.values\n",
    "        y = y.values.astype(float)\n",
    "        y_shuf = y_shuf.values.astype(float)\n",
    "\n",
    "        NUM_OVERALL_LOOPS = 10\n",
    "        all_avg_accs = []\n",
    "        all_avg_aurocs = []\n",
    "        all_avg_accs_shuf = []\n",
    "        all_avg_aurocs_shuf = []\n",
    "        fpr_list_of_lists = []\n",
    "        tpr_list_of_lists = []\n",
    "        fpr_list_of_lists_shuf = []\n",
    "        tpr_list_of_lists_shuf = []\n",
    "\n",
    "        for loop_now in range(NUM_OVERALL_LOOPS):\n",
    "\n",
    "          # Outer cross-validation\n",
    "          outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=loop_now)\n",
    "\n",
    "          accuracy_scores = []\n",
    "          y_test_all = []\n",
    "          y_prob_all = []\n",
    "\n",
    "          accuracy_scores_shuf = []\n",
    "          y_test_all_shuf = []\n",
    "          y_prob_all_shuf = []\n",
    "\n",
    "          for train_idx, test_idx in outer_cv.split(X, y):\n",
    "              X_train, X_test = X[train_idx], X[test_idx]\n",
    "              y_train, y_test = y[train_idx], y[test_idx]\n",
    "              y_train_shuf, y_test_shuf = y_shuf[train_idx], y_shuf[test_idx]\n",
    "\n",
    "              # Define the model with LASSO penalty for the inner CV\n",
    "              model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "              # Set up GridSearchCV for the inner CV\n",
    "              # Use regular (non-shuffled) labels for the alpha search!\n",
    "              grid = GridSearchCV(estimator=model, param_grid={'C': alpha_range}, scoring='roc_auc', cv=10)\n",
    "              grid.fit(X_train, y_train)\n",
    "\n",
    "              # Best model from inner CV\n",
    "              logreg_model = LogisticRegression(penalty='l1', C=grid.best_params_['C'], solver='liblinear')\n",
    "              logreg_model.fit(X_train, y_train)\n",
    "\n",
    "              # Shuffled best model (using same alpha as the non-shuffled model)\n",
    "              logreg_model_shuf = LogisticRegression(penalty='l1', C=grid.best_params_['C'], solver='liblinear')\n",
    "              logreg_model_shuf.fit(X_train, y_train_shuf)\n",
    "\n",
    "              # Evaluate the best model on the test set\n",
    "              y_pred = logreg_model.predict(X_test)\n",
    "              accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "              y_prob = logreg_model.predict_proba(X_test)[:, 1]\n",
    "              y_test_all = y_test_all + list(y_test)\n",
    "              y_prob_all = y_prob_all + list(y_prob)\n",
    "\n",
    "              # Eval best shuffled model on shuffled test set\n",
    "              y_pred_shuf = logreg_model_shuf.predict(X_test)\n",
    "              accuracy_scores_shuf.append(accuracy_score(y_test_shuf, y_pred_shuf))\n",
    "\n",
    "              y_prob_shuf = logreg_model_shuf.predict_proba(X_test)[:, 1]\n",
    "              y_test_all_shuf = y_test_all_shuf + list(y_test_shuf)\n",
    "              y_prob_all_shuf = y_prob_all_shuf + list(y_prob_shuf)\n",
    "\n",
    "\n",
    "          # Calculate average AUROC and accuracy across outer folds\n",
    "          avg_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "          # Calculate average AUROC and accuracy across outer folds - shuffled\n",
    "          avg_accuracy_shuf = np.mean(accuracy_scores_shuf)\n",
    "\n",
    "          # Get one ROC curve and AUROC\n",
    "          fpr, tpr, thresholds = roc_curve(y_test_all, y_prob_all)\n",
    "          mean_auroc = roc_auc_score(y_test_all, y_prob_all)\n",
    "\n",
    "          # Get one ROC curve and AUROC - shuffled\n",
    "          fpr_shuf, tpr_shuf, thresholds_shuf = roc_curve(y_test_all_shuf, y_prob_all_shuf)\n",
    "          mean_auroc_shuf = roc_auc_score(y_test_all_shuf, y_prob_all_shuf)\n",
    "\n",
    "          # Append accuracy and auroc\n",
    "          all_avg_aurocs.append(mean_auroc)\n",
    "          all_avg_accs.append(avg_accuracy)\n",
    "\n",
    "          # Append accuracy and auroc - shuffled\n",
    "          all_avg_aurocs_shuf.append(mean_auroc_shuf)\n",
    "          all_avg_accs_shuf.append(avg_accuracy_shuf)\n",
    "\n",
    "          # TPR + FPR\n",
    "          fpr_list_of_lists.append(fpr)\n",
    "          tpr_list_of_lists.append(tpr)\n",
    "          #plt.plot(fpr, tpr, color='lightblue', lw=1, label='ROC (AUROC = {:.2f})'.format(mean_auroc))\n",
    "\n",
    "          # TPR + FPR - shuffled\n",
    "          fpr_list_of_lists_shuf.append(fpr_shuf)\n",
    "          tpr_list_of_lists_shuf.append(tpr_shuf)\n",
    "          #plt.plot(fpr_shuf, tpr_shuf, color='grey', lw=1, label='Shuffled ROC (AUROC = {:.2f})'.format(mean_auroc_shuf))\n",
    "\n",
    "\n",
    "        # Plot averaged x and y\n",
    "        averaged_x, averaged_y = average_xy_pairs(fpr_list_of_lists, tpr_list_of_lists)\n",
    "        roc_data = list(zip(averaged_x, averaged_y))\n",
    "        mean_mean_auroc = np.mean(all_avg_aurocs)\n",
    "        mean_mean_acc = np.mean(all_avg_accs)\n",
    "\n",
    "        plt.plot(averaged_x, averaged_y, color='blue', lw=2, label='Avg ROC (AUROC = {:.2f})'.format(mean_mean_auroc))\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Random Chance')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Average Receiver Operating Characteristic (ROC) Curve')\n",
    "\n",
    "        # Plot averaged x and y - shuffled\n",
    "        averaged_x_shuf, averaged_y_shuf = average_xy_pairs(fpr_list_of_lists_shuf, tpr_list_of_lists_shuf)\n",
    "        roc_data_shuf = list(zip(averaged_x_shuf, averaged_y_shuf))\n",
    "        mean_mean_auroc_shuf = np.mean(all_avg_aurocs_shuf)\n",
    "        mean_mean_acc_shuf = np.mean(all_avg_accs_shuf)\n",
    "\n",
    "        plt.plot(averaged_x_shuf, averaged_y_shuf, color='darkgrey', lw=2, label='Shuffled Avg ROC (AUROC = {:.2f})'.format(mean_mean_auroc_shuf))\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        # Save the plot\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path_plotting, bbox_inches='tight')\n",
    "        if verbose:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Update dicts\n",
    "        auroc_dict[key] = mean_mean_auroc\n",
    "        acc_dict[key] = mean_mean_acc\n",
    "\n",
    "        auroc_dict_shuf[key] = mean_mean_auroc_shuf\n",
    "        acc_dict_shuf[key] = mean_mean_acc_shuf\n",
    "\n",
    "\n",
    "        # Save ROC\n",
    "        roc_csv_path = save_path[:-4] + f'_AvgROC_{num_minutes}.csv'\n",
    "        with open(roc_csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['False Positive Rate', 'True Positive Rate'])\n",
    "            for fpr, tpr in roc_data:\n",
    "                writer.writerow([fpr, tpr])\n",
    "\n",
    "        # Save ROC\n",
    "        roc_csv_path_shuf = save_path[:-4] + f'_shuffled_AvgROC_{num_minutes}.csv'\n",
    "        with open(roc_csv_path_shuf, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['False Positive Rate', 'True Positive Rate'])\n",
    "            for fpr, tpr in roc_data_shuf:\n",
    "                writer.writerow([fpr, tpr])\n",
    "\n",
    "    return auroc_dict, acc_dict, auroc_dict_shuf, acc_dict_shuf\n",
    "\n",
    "\n",
    "def plot_auroc_dict(auroc_dict, save_path, title_prefix, verbose=False):\n",
    "    plt.figure(figsize=(8, 6))  # Create a new figure\n",
    "    time_windows = []\n",
    "    aurocs = []\n",
    "\n",
    "    # Keep time window in minutes!\n",
    "    for window, auroc in auroc_dict.items():\n",
    "        time_windows.append(float(window))\n",
    "        aurocs.append(auroc)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.plot(time_windows, aurocs, marker='o')\n",
    "    plt.xlabel('Time Window (minutes)')\n",
    "    plt.ylabel('AUROC')\n",
    "    plt.title(f'{title_prefix}: AUROC vs. Time Window')\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    if verbose:\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "def plot_acc_dict(acc_dict, save_path, title_prefix, verbose=False):\n",
    "    plt.figure(figsize=(8, 6))  # Create a new figure\n",
    "    time_windows = []\n",
    "    accs = []\n",
    "\n",
    "    # Keep time window in minutes!\n",
    "    for window, acc_now in acc_dict.items():\n",
    "        time_windows.append(float(window))\n",
    "        accs.append(acc_now)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.plot(time_windows, accs, marker='o')\n",
    "    plt.xlabel('Time Window (minutes)')\n",
    "    plt.ylabel('Test Set Accuracy')\n",
    "    plt.title(f'{title_prefix}: Accuracy vs. Time Window')\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    if verbose:\n",
    "      plt.show()\n",
    "\n",
    "def key_outputs_to_csv(auroc_dict, save_path):\n",
    "  # Find the key with the highest value\n",
    "  best_key = max(auroc_dict, key=auroc_dict.get)\n",
    "  highest_auroc = auroc_dict[best_key]\n",
    "\n",
    "  # Convert the key to a number\n",
    "  best_time_window = int(best_key)\n",
    "\n",
    "  # Write to CSV\n",
    "  with open(save_path, 'w', newline='') as csvfile:\n",
    "      fieldnames = ['Highest AUROC', 'Best Time Window']\n",
    "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "      writer.writeheader()\n",
    "      writer.writerow({'Highest AUROC': highest_auroc, 'Best Time Window': best_time_window})\n",
    "\n",
    "\n",
    "def save_auroc_dict_to_csv(auroc_dict, save_path):\n",
    "    \"\"\"\n",
    "    Save the AUROC values to a CSV file.\n",
    "\n",
    "    :param auroc_dict: Dictionary with time windows as keys and AUROC values as values.\n",
    "    :param save_path: The path to save the CSV file.\n",
    "    \"\"\"\n",
    "    with open(save_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Time Window', 'AUROC'])  # Writing header\n",
    "\n",
    "        for key, value in auroc_dict.items():\n",
    "            writer.writerow([key, value])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzc3k42wyHv7"
   },
   "source": [
    "## Core Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aHeuj256pt9O",
    "outputId": "513a3831-45fd-43b7-86de-af973c715ad1"
   },
   "outputs": [],
   "source": [
    "\n",
    "pipeline_dict = {\n",
    "    #'HSE': (hsemotion_vectors_dict, 'HSE_metrics.png', 'HSEmotion'),\n",
    "    #'OFAU': (openface_vectors_dict, 'OFAU_metrics.png', 'OpenFace'),\n",
    "    'OGAU': (opengraphau_vectors_dict, 'OGAU_metrics.png', 'OpenGraphAU'),\n",
    "    #'OFAUHSE': (ofauhsemotion_vectors_dict, 'OFAUHSE_metrics.png', 'OFAU + HSE'),\n",
    "    #'OGAUHSE': (ogauhsemotion_vectors_dict, 'OGAUHSE_metrics.png', 'OGAU + HSE'),\n",
    "    #'ALL': (all_vectors_dict, 'ALL_metrics.png', 'ALL (OF + OG + HSE)')\n",
    "}\n",
    "\n",
    "# SHUFFLED LABELS\n",
    "def shuffle_pain_column(df):\n",
    "    # Make a copy of the DataFrame to avoid modifying the original one\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Shuffle the 'Pain' column\n",
    "    np.random.shuffle(df_copy['Pain'].values)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "set_seed(10)\n",
    "shuffle_verbalPain = shuffle_pain_column(df_verbalPain)\n",
    "\n",
    "ideal_windows = {}\n",
    "\n",
    "for pipeline_label, (vectors_dict, save_file, title_prefix) in pipeline_dict.items():\n",
    "  auroc_dict, acc_dict, auroc_dict_shuf, acc_dict_shuf  = get_metrics_binClass(vectors_dict, df_verbalPain, shuffle_verbalPain, save_path=RESULTS_PATH_BASE + \"PAIN_LASSO/\" + save_file, title=f'{title_prefix}: Automated Pain Detection', verbose=True)\n",
    "  key_outputs_to_csv(auroc_dict, save_path=RESULTS_PATH_BASE + \"PAIN_LASSO/\" + save_file[:-4] + '.csv')\n",
    "  save_auroc_dict_to_csv(auroc_dict, save_path=RESULTS_PATH_BASE + \"PAIN_LASSO/\" + 'AUROC_dict.csv')\n",
    "  save_auroc_dict_to_csv(auroc_dict_shuf, save_path=RESULTS_PATH_BASE + \"PAIN_LASSO/\" + 'AUROC_shuffled_dict.csv')\n",
    "  ideal_windows[pipeline_label] = max(auroc_dict, key=auroc_dict.get)\n",
    "  plot_auroc_dict(auroc_dict, save_path=RESULTS_PATH_BASE + \"PAIN_LASSO/\" + save_file, title_prefix=title_prefix, verbose=True)\n",
    "  plot_acc_dict(acc_dict, save_path=RESULTS_PATH_BASE + \"PAIN_LASSO/\" + save_file[:-4] + '_acc.png', title_prefix=title_prefix, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiLt88slucPo"
   },
   "source": [
    "## Most Important AUs (OpenGraphAU Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jsfg1l2f3ZYi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "def analyze_au_pain(aus_one_time_window, df_verbalPain, save_path):\n",
    "    # Splitting the timestamps into pain = 0 and pain = 1\n",
    "    pain_0_times = df_verbalPain[df_verbalPain['Pain'] == 0]['Datetime']\n",
    "    pain_1_times = df_verbalPain[df_verbalPain['Pain'] == 1]['Datetime']\n",
    "\n",
    "    # Initialize dictionaries to hold pres_pct lists\n",
    "    pres_pct_0 = {}\n",
    "    pres_pct_1 = {}\n",
    "\n",
    "    # Loop through each timestamp and categorize the pres_pct into pain 0 or 1\n",
    "    for time, df in aus_one_time_window.items():\n",
    "        if time in pain_0_times.values:\n",
    "            for au in [i for i in df.columns if 'AU' in i]:\n",
    "                if int(au.replace('AU', '')) not in pres_pct_0:\n",
    "                    pres_pct_0[int(au.replace('AU', ''))] = []\n",
    "                pres_pct_0[int(au.replace('AU', ''))].append(df[au]['pres_pct'])\n",
    "        elif time in pain_1_times.values:\n",
    "            for au in [i for i in df.columns if 'AU' in i]:\n",
    "                if int(au.replace('AU', '')) not in pres_pct_1:\n",
    "                    pres_pct_1[int(au.replace('AU', ''))] = []\n",
    "                pres_pct_1[int(au.replace('AU', ''))].append(df[au]['pres_pct'])\n",
    "\n",
    "    # Performing t-tests and calculating averages\n",
    "    p_values = {}\n",
    "    avg_pres_pct_0 = {}\n",
    "    avg_pres_pct_1 = {}\n",
    "    for au in pres_pct_0.keys():\n",
    "        avg_pres_pct_0[au] = np.mean(pres_pct_0[au])\n",
    "        avg_pres_pct_1[au] = np.mean(pres_pct_1[au])\n",
    "        p_values[au] = stats.ttest_ind(pres_pct_0[au], pres_pct_1[au]).pvalue\n",
    "\n",
    "    # Calculating SEM\n",
    "    sem_pres_pct_0 = {}\n",
    "    sem_pres_pct_1 = {}\n",
    "    for au in pres_pct_0.keys():\n",
    "        sem_pres_pct_0[au] = np.std(pres_pct_0[au]) / np.sqrt(len(pres_pct_0[au]))\n",
    "        sem_pres_pct_1[au] = np.std(pres_pct_1[au]) / np.sqrt(len(pres_pct_1[au]))\n",
    "\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))  # Wider figure\n",
    "    for idx, au in enumerate(sorted(pres_pct_0.keys())):\n",
    "        ax.bar(idx - 0.2, avg_pres_pct_0[au], width=0.4, color='blue', label='Low Pain' if idx == 0 else \"\", yerr=sem_pres_pct_0[au], capsize=5)\n",
    "        ax.bar(idx + 0.2, avg_pres_pct_1[au], width=0.4, color='red', label='High Pain' if idx == 0 else \"\", yerr=sem_pres_pct_1[au], capsize=5)\n",
    "        if p_values[au] < 0.05:\n",
    "            #ax.plot([idx - 0.2, idx + 0.2], [max(avg_pres_pct_0[au], avg_pres_pct_1[au]) + 2, max(avg_pres_pct_0[au], avg_pres_pct_1[au]) + 2], color='black')\n",
    "            #ax.plot([idx - 0.2, idx + 0.2], [max(avg_pres_pct_0[au], avg_pres_pct_1[au]) + 2, max(avg_pres_pct_0[au], avg_pres_pct_1[au]) + 2], color='black', linewidth=2)\n",
    "            ax.scatter([idx - 0.2, idx, idx + 0.2], [max(avg_pres_pct_0[au], avg_pres_pct_1[au]) + 7.5]*3, color='black', marker='*', s=25)\n",
    "\n",
    "    ax.set_xticks(range(len(pres_pct_0)))\n",
    "    ax.set_xticklabels(sorted(pres_pct_0.keys()), fontsize=16)\n",
    "    ax.set_yticklabels(ax.get_yticks(), fontsize=18)\n",
    "    ax.set_ylabel('Average Presence (%)', fontsize=24)\n",
    "    ax.set_xlabel('Facial Action Unit (AU)', fontsize=24)\n",
    "    plt.title('Comparing Facial Action Units During Low vs. High Pain', fontsize=26)\n",
    "    ax.legend(fontsize=24)\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return p_values, avg_pres_pct_0, avg_pres_pct_1, sem_pres_pct_0, sem_pres_pct_1\n",
    "\n",
    "\n",
    "def save_dicts_to_csv(dict1, dict2, dict3, error_dict1, error_dict2, save_path):\n",
    "    \"\"\"\n",
    "    Save five dictionaries to a CSV file.\n",
    "\n",
    "    :param dict1: Dictionary for 'Low Pain Presence Percent'\n",
    "    :param dict2: Dictionary for 'High Pain Presence Percent'\n",
    "    :param dict3: Dictionary for 'P value'\n",
    "    :param error_dict1: Dictionary for 'SEM Low Pain'\n",
    "    :param error_dict2: Dictionary for 'SEM High Pain'\n",
    "    :param save_path: The path to save the CSV file\n",
    "    \"\"\"\n",
    "    fieldnames = ['AU', 'Low Pain Presence Percent', 'SEM Low Pain', 'High Pain Presence Percent', 'SEM High Pain', 'P value']\n",
    "\n",
    "    with open(save_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Assuming all dictionaries have the same keys\n",
    "        for key in dict1:\n",
    "            writer.writerow({\n",
    "                'AU': key,\n",
    "                'Low Pain Presence Percent': dict1[key],\n",
    "                'SEM Low Pain': error_dict1[key],\n",
    "                'High Pain Presence Percent': dict2[key],\n",
    "                'SEM High Pain': error_dict2[key],\n",
    "                'P value': dict3[key]\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cC3pFDx2jrt6"
   },
   "outputs": [],
   "source": [
    "pipeline_label = 'OGAU'\n",
    "dict_list_dict = opengraphau_dict_list_dict\n",
    "save_file_prefix = 'OGAU_AU_comparison'\n",
    "title_prefix = 'OpenGraphAU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MkpllXYmitmn",
    "outputId": "218d2497-4512-4a30-ab98-956b3d8e51dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ideal_window in dict_list_dict.keys():\n",
    "  save_file = f'{save_file_prefix}_{ideal_window}.png'\n",
    "  aus_one_time_window = dict_list_dict[ideal_window][0] # the first list element is the au derived dict!\n",
    "  barplot_save_path = RESULTS_PATH_BASE + \"PAIN_LASSO/\" + save_file\n",
    "  p_values, avg_pres_pct_0, avg_pres_pct_1, sem_pres_pct_0, sem_pres_pct_1 = analyze_au_pain(aus_one_time_window, df_verbalPain, barplot_save_path)\n",
    "  save_dicts_to_csv(avg_pres_pct_0, avg_pres_pct_1, p_values, sem_pres_pct_0, sem_pres_pct_1, save_path=RESULTS_PATH_BASE + \"PAIN_LASSO/\" + f'Significant_AUs_{ideal_window}_minutes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ephys Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(input_num):\n",
    "    \"\"\"\n",
    "    Sets the random seed for both the random and numpy libraries.\n",
    "\n",
    "    Parameters:\n",
    "    input_num (int): The seed number to set.\n",
    "    \"\"\"\n",
    "    random.seed(input_num)\n",
    "    np.random.seed(input_num)\n",
    "\n",
    "\n",
    "def prepare_data(datetime_features_dict, df, cross_val=True):\n",
    "    # Convert datetime_features_dict to DataFrame\n",
    "    feature_df = pd.DataFrame.from_dict(datetime_features_dict, orient='index')\n",
    "    feature_df.index.name = 'Datetime'\n",
    "\n",
    "    # Convert Datetime column in df to datetime type\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "    # Concatenate feature DataFrame with df\n",
    "    merged_df = pd.concat([df.set_index('Datetime'), feature_df], axis=1, join='inner')\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = merged_df.drop(['Pain'], axis=1)\n",
    "    y = merged_df['Pain']\n",
    "\n",
    "    y = y.astype(int)\n",
    "\n",
    "    if cross_val:\n",
    "      return X, y\n",
    "    else:\n",
    "      # Split the data into training and testing sets\n",
    "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)\n",
    "\n",
    "      return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_model_all_data(vectors_dict, labels_df, verbose=False):\n",
    "    # Define the range of alphas for the LASSO regression\n",
    "    alpha_range = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "    for key in vectors_dict.keys():\n",
    "        num_minutes = round(float(key))\n",
    "        if verbose:\n",
    "            print(f'CURRENT WINDOW: {num_minutes} minutes')\n",
    "    \n",
    "        results_prep = prepare_data(vectors_dict[key], labels_df, cross_val=True)\n",
    "    \n",
    "        X, y = results_prep\n",
    "    \n",
    "        # Fix nan issue\n",
    "        missing_indices = X[X.isnull().any(axis=1)].index\n",
    "        X = X.dropna()\n",
    "        y = y.drop(missing_indices)\n",
    "        X = X.values\n",
    "        y = y.values.astype(float)\n",
    "\n",
    "        model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "        # Set up GridSearchCV for the CV\n",
    "        grid = GridSearchCV(estimator=model, param_grid={'C': alpha_range}, scoring='roc_auc', cv=10)\n",
    "        grid.fit(X, y)\n",
    "\n",
    "        # Best model from CV\n",
    "        logreg_model = LogisticRegression(penalty='l1', C=grid.best_params_['C'], solver='liblinear')\n",
    "        logreg_model.fit(X, y)\n",
    "        save_var(logreg_model, forced_name=f'concordance_model_{PAT_SHORT_NAME}_{num_minutes}_mins')\n",
    "\n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_dict = {\n",
    "    #'HSE': (hsemotion_vectors_dict, 'HSE_metrics.png', 'HSEmotion'),\n",
    "    #'OFAU': (openface_vectors_dict, 'OFAU_metrics.png', 'OpenFace'),\n",
    "    'OGAU': (opengraphau_vectors_dict, 'OGAU_metrics.png', 'OpenGraphAU'),\n",
    "    #'OFAUHSE': (ofauhsemotion_vectors_dict, 'OFAUHSE_metrics.png', 'OFAU + HSE'),\n",
    "    #'OGAUHSE': (ogauhsemotion_vectors_dict, 'OGAUHSE_metrics.png', 'OGAU + HSE'),\n",
    "    #'ALL': (all_vectors_dict, 'ALL_metrics.png', 'ALL (OF + OG + HSE)')\n",
    "}\n",
    "\n",
    "set_seed(10)\n",
    "\n",
    "for pipeline_label, (vectors_dict, save_file, title_prefix) in pipeline_dict.items():\n",
    "  train_model_all_data(vectors_dict, df_verbalPain, verbose=True)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "temp_EPHYS_CONCORDANCE_PATH = f'/home/klab/NAS/Analysis/AudioFacialEEG/EphysProb/{PAT_NOW}-ephysPrediction-v2.csv'\n",
    "\n",
    "df_ephysConcord = pd.read_csv(temp_EPHYS_CONCORDANCE_PATH)\n",
    "\n",
    "# Rename the columns\n",
    "df_ephysConcord.columns = ['Datetime', 'ephysPredProb']\n",
    "\n",
    "# Convert the 'Datetime' column to the specified format\n",
    "df_ephysConcord['Datetime'] = pd.to_datetime(df_ephysConcord['Datetime']).dt.strftime('%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "# Convert the 'ephysPredProb' column to float\n",
    "df_ephysConcord['ephysPredProb'] = df_ephysConcord['ephysPredProb'].astype(float)\n",
    "\n",
    "df_ephysConcord_original = df_ephysConcord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAJOR ERROR! ZERO RELEVANT DFS!! DATETIME: 2023-03-31 19:39:00\n",
      "MAJOR ERROR! ZERO RELEVANT DFS!! DATETIME: 2023-03-31 19:39:00\n"
     ]
    }
   ],
   "source": [
    "# BINARY PAIN SCORES\n",
    "\n",
    "# Reset ephysConcordance DF\n",
    "df_ephysConcord = df_ephysConcord_original\n",
    "\n",
    "if ENABLE_OPENFACE:\n",
    "  openface_radius_dict_concord = {}\n",
    "hsemotion_radius_dict_concord = {}\n",
    "opengraphau_radius_dict_concord = {}\n",
    "\n",
    "for i in TIME_RADIUS_LIST:\n",
    "  if ENABLE_OPENFACE:\n",
    "    openface_radius_now_concord = get_radius_dict(i, dfs_openface, df_videoTimestamps, df_ephysConcord, takeAll=takeAll)\n",
    "  hsemotion_radius_now_concord = get_radius_dict(i, dfs_hsemotion, df_videoTimestamps, df_ephysConcord, takeAll=takeAll)\n",
    "  opengraphau_radius_now_concord = get_radius_dict(i, dfs_opengraphau, df_videoTimestamps, df_ephysConcord, takeAll=takeAll)\n",
    "\n",
    "  if i == TIME_RADIUS_LIST[0]:\n",
    "    # Remove invalid timestamps (no video data)\n",
    "    invalid_timestamps = []\n",
    "    if ENABLE_OPENFACE:\n",
    "      for ts_now in list(openface_radius_now_concord.keys()):\n",
    "        if openface_radius_now_concord[ts_now].empty:\n",
    "          invalid_timestamps.append(ts_now)\n",
    "    for ts_now in list(opengraphau_radius_now_concord.keys()):\n",
    "      if opengraphau_radius_now_concord[ts_now].empty:\n",
    "        invalid_timestamps.append(ts_now)\n",
    "    for ts_now in list(hsemotion_radius_now_concord.keys()):\n",
    "      if hsemotion_radius_now_concord[ts_now].empty:\n",
    "        invalid_timestamps.append(ts_now)\n",
    "\n",
    "    # Remove duplicates\n",
    "    invalid_timestamps = list(set(invalid_timestamps))\n",
    "\n",
    "    # Convert to datetimes\n",
    "    invalid_timestamps = pd.to_datetime(invalid_timestamps)\n",
    "\n",
    "    df_ephysConcord = df_ephysConcord[~pd.to_datetime(df_ephysConcord['Datetime']).isin(invalid_timestamps)]\n",
    "    df_ephysConcord = df_ephysConcord.reset_index(drop=True)\n",
    "\n",
    "    for timestamp in invalid_timestamps:\n",
    "      if ENABLE_OPENFACE:\n",
    "        if timestamp in openface_radius_now_concord:\n",
    "            del openface_radius_now_concord[timestamp]\n",
    "      if timestamp in hsemotion_radius_now_concord:\n",
    "          del hsemotion_radius_now_concord[timestamp]\n",
    "      if timestamp in opengraphau_radius_now_concord:\n",
    "          del opengraphau_radius_now_concord[timestamp]\n",
    "\n",
    "\n",
    "  if ENABLE_OPENFACE:\n",
    "    openface_radius_dict_concord[f'{i}'] = openface_radius_now_concord\n",
    "  hsemotion_radius_dict_concord[f'{i}'] = hsemotion_radius_now_concord\n",
    "  opengraphau_radius_dict_concord[f'{i}'] = opengraphau_radius_now_concord\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Radius:  1\n",
      "Time Radius:  2\n",
      "Time Radius:  3\n",
      "Time Radius:  4\n",
      "Time Radius:  5\n",
      "Time Radius:  10\n",
      "Time Radius:  30\n",
      "Time Radius:  60\n",
      "Time Radius:  90\n",
      "Time Radius:  120\n",
      "Time Radius:  150\n",
      "Time Radius:  180\n",
      "Time Radius:  210\n",
      "Time Radius:  240\n"
     ]
    }
   ],
   "source": [
    "# dictionary to store results from each different time window we test!\n",
    "hsemotion_emo_stats_dict_concord = {}\n",
    "\n",
    "for time_radius, hsemotion_radius_now in hsemotion_radius_dict_concord.items():\n",
    "  print('Time Radius: ', time_radius)\n",
    "  if NORMALIZE_DATA == 0:\n",
    "    THRESHOLD = 0.4\n",
    "  elif NORMALIZE_DATA == 1:\n",
    "    THRESHOLD = 2.5\n",
    "  hsemotion_radius_binarized = apply_function_to_dict(hsemotion_radius_now, binarize_cols, threshold=THRESHOLD)\n",
    "  hsemotion_emo_stats = apply_function_to_dict(hsemotion_radius_binarized, analyze_emotion_events_v2, max_frame_gap=10, event_minimum_num_frames=12, method='HSE')\n",
    "  hsemotion_emo_stats_fixed = fill_empty_dfs(hsemotion_emo_stats)\n",
    "  hsemotion_emo_stats_dict_concord[time_radius] = hsemotion_emo_stats_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Radius:  1\n",
      "Time Radius:  2\n",
      "Time Radius:  3\n",
      "Time Radius:  4\n",
      "Time Radius:  5\n",
      "Time Radius:  10\n",
      "Time Radius:  30\n",
      "Time Radius:  60\n",
      "Time Radius:  90\n",
      "Time Radius:  120\n",
      "Time Radius:  150\n",
      "Time Radius:  180\n",
      "Time Radius:  210\n",
      "Time Radius:  240\n"
     ]
    }
   ],
   "source": [
    "# OPENGRAPHAU AU EVENTS\n",
    "\n",
    "# dictionary to store results from each different time window we test!\n",
    "opengraphau_au_events_stats_dict_concord = {}\n",
    "\n",
    "for time_radius, opengraphau_radius_now in opengraphau_radius_dict_concord.items():\n",
    "  print('Time Radius: ', time_radius)\n",
    "  opengraphau_radius_binarized = apply_function_to_dict(opengraphau_radius_now, binarize_cols, threshold=THRESHOLD)\n",
    "  opengraphau_au_events_stats = apply_function_to_dict(opengraphau_radius_binarized, analyze_emotion_events_v2, max_frame_gap=10, event_minimum_num_frames=12, method='OGAU')\n",
    "  opengraphau_au_events_stats_fixed = fill_empty_dfs(opengraphau_au_events_stats)\n",
    "  opengraphau_au_events_stats_dict_concord[time_radius] = opengraphau_au_events_stats_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opengraphau_dict_list_dict_concord = {}\n",
    "\n",
    "for key in opengraphau_au_events_stats_dict_concord.keys():\n",
    "  opengraphau_dict_list_dict_concord[key] = [opengraphau_au_events_stats_dict_concord[key]]\n",
    "\n",
    "hsemotion_dict_list_dict_concord = {}\n",
    "\n",
    "for key in hsemotion_emo_stats_dict_concord.keys():\n",
    "  hsemotion_dict_list_dict_concord[key] = [hsemotion_emo_stats_dict_concord[key]]\n",
    "\n",
    "ogauhsemotion_dict_list_dict_concord = partial_combine_dictionaries(opengraphau_dict_list_dict_concord, hsemotion_dict_list_dict_concord)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2P1ntu5JF9A"
   },
   "outputs": [],
   "source": [
    "opengraphau_vectors_dict_concord = {}\n",
    "\n",
    "for key, opengraphau_dict_list_now in opengraphau_dict_list_dict_concord.items():\n",
    "  opengraphau_vectors_dict_concord[key] = flatten_dataframes_dict(opengraphau_dict_list_now)\n",
    "\n",
    "hsemotion_vectors_dict_concord = {}\n",
    "\n",
    "for key, hsemotion_dict_list_now in hsemotion_dict_list_dict_concord.items():\n",
    "  hsemotion_vectors_dict_concord[key] = flatten_dataframes_dict(hsemotion_dict_list_now)\n",
    "\n",
    "ogauhsemotion_vectors_dict_concord = {}\n",
    "\n",
    "for key, ogauhsemotion_dict_list_now in ogauhsemotion_dict_list_dict_concord.items():\n",
    "  ogauhsemotion_vectors_dict_concord[key] = flatten_dataframes_dict(ogauhsemotion_dict_list_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def prepare_data_ephys(datetime_features_dict, df, cross_val=True):\n",
    "    # Convert datetime_features_dict to DataFrame\n",
    "    feature_df = pd.DataFrame.from_dict(datetime_features_dict, orient='index')\n",
    "    feature_df.index.name = 'Datetime'\n",
    "\n",
    "    # Convert Datetime column in df to datetime type\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "    # Concatenate feature DataFrame with df\n",
    "    merged_df = pd.concat([df.set_index('Datetime'), feature_df], axis=1, join='inner')\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = merged_df.drop(['ephysPredProb'], axis=1)\n",
    "    y = merged_df['ephysPredProb']\n",
    "\n",
    "    y = y.astype(float)\n",
    "\n",
    "    if cross_val:\n",
    "      return X, y\n",
    "    else:\n",
    "      # Split the data into training and testing sets\n",
    "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=25)\n",
    "\n",
    "      return X_train, X_test, y_train, y_test\n",
    "\n",
    "def eval_models_on_datetimes(vectors_dict, labels_df, results_save_path, csv_save_path, verbose=False):\n",
    "    results = []\n",
    "    cv_predictions = {\"Datetime\": None, \"ephysPredProb\": None}\n",
    "\n",
    "    for key in vectors_dict.keys():\n",
    "        num_minutes = round(float(key))\n",
    "        if verbose:\n",
    "            print(f'CURRENT WINDOW: {num_minutes} minutes')\n",
    "    \n",
    "        results_prep = prepare_data_ephys(vectors_dict[key], labels_df, cross_val=True)\n",
    "    \n",
    "        X, y = results_prep\n",
    "    \n",
    "        # Fix nan issue\n",
    "        missing_indices = X[X.isnull().any(axis=1)].index\n",
    "        X = X.dropna()\n",
    "        y = y.drop(missing_indices)\n",
    "\n",
    "        if cv_predictions[\"Datetime\"] is None:\n",
    "            cv_predictions[\"Datetime\"] = X.index\n",
    "            cv_predictions[\"ephysPredProb\"] = y\n",
    "        \n",
    "        X = X.values\n",
    "        y = y.values.astype(float)\n",
    "        \n",
    "        logreg_model = load_var(f'concordance_model_{PAT_SHORT_NAME}_{num_minutes}_mins')\n",
    "\n",
    "        # Use the model to predict probabilities\n",
    "        y_pred_prob = logreg_model.predict_proba(X)[:, 1]\n",
    "\n",
    "        # Compute Pearson's R\n",
    "        pearson_r, _ = pearsonr(y, y_pred_prob)\n",
    "\n",
    "        # Append the results\n",
    "        results.append({'Window_Size_In_Minutes': num_minutes, 'Pearson_R_With_Ephys': pearson_r})\n",
    "\n",
    "        # Store predictions for CSV\n",
    "        cv_predictions[f'CVPred_{num_minutes}_minutes'] = y_pred_prob\n",
    "    \n",
    "    # Save the results to a CSV file\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(results_save_path, index=False)\n",
    "\n",
    "    # Prepare and save the additional CSV\n",
    "    csv_df = pd.DataFrame(cv_predictions)\n",
    "    csv_df.to_csv(csv_save_path, index=False)\n",
    "\n",
    "    return results_df, csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_dict = {\n",
    "    #'HSE': (hsemotion_vectors_dict, 'HSE_metrics.png', 'HSEmotion'),\n",
    "    #'OFAU': (openface_vectors_dict, 'OFAU_metrics.png', 'OpenFace'),\n",
    "    'OGAU': (opengraphau_vectors_dict_concord, 'OGAU_metrics.png', 'OpenGraphAU'),\n",
    "    #'OFAUHSE': (ofauhsemotion_vectors_dict, 'OFAUHSE_metrics.png', 'OFAU + HSE'),\n",
    "    #'OGAUHSE': (ogauhsemotion_vectors_dict, 'OGAUHSE_metrics.png', 'OGAU + HSE'),\n",
    "    #'ALL': (all_vectors_dict, 'ALL_metrics.png', 'ALL (OF + OG + HSE)')\n",
    "}\n",
    "\n",
    "set_seed(10)\n",
    "\n",
    "for pipeline_label, (vectors_dict, save_file, title_prefix) in pipeline_dict.items():\n",
    "  save_path = RESULTS_PATH_BASE + \"PAIN_LASSO/\" +  title_prefix + \"_ephysConcord.csv\"\n",
    "  results_df, csv_df = eval_models_on_datetimes(vectors_dict, df_ephysConcord, results_save_path=save_path, csv_save_path=save_path[:-4] + '_full.csv', verbose=True)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
