{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D57PMLHEVlOm"
      },
      "source": [
        "# Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frVOpCRsQFEX"
      },
      "outputs": [],
      "source": [
        "PAT_NOW = \"S11\"\n",
        "\n",
        "OUTPATIENT_MOOD_SHEET_ID = '1BxSevmO5hhyATB2yxYQWbW-7o5SsLFIoICgNq-BqyWs'\n",
        "\n",
        "COMBINED_OUTPUT_DIRECTORY = f'/content/drive/MyDrive/Stanford/AudioFacialEEG/Outputs/Outpatient/{PAT_NOW}/'\n",
        "\n",
        "RUNTIME_VAR_PATH = '/content/drive/MyDrive/Stanford/AudioFacialEEG/Runtime_Vars/'\n",
        "RESULTS_PATH_BASE = f'/content/drive/MyDrive/Stanford/AudioFacialEEG/Results/Outpatient/{PAT_NOW}/'\n",
        "FEATURE_VIS_PATH = f'/content/drive/MyDrive/Stanford/AudioFacialEEG/Feature_Visualization/Outpatient/{PAT_NOW}/'\n",
        "FEATURE_LABEL_PATH = '/content/drive/MyDrive/Stanford/AudioFacialEEG/Feature_Labels/'\n",
        "QC_PATH = '/content/drive/MyDrive/Stanford/AudioFacialEEG/Quality_Control/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMO_FEATURE_SETTING = 2\n",
        "\n",
        "# 0 - Our Custom AU --> Emotions, with all emotions\n",
        "# 1 - Our Custom AU --> Emotions, with just OpenDBM's emotions\n",
        "# 2 - OpenDBM's AU--> Emotions"
      ],
      "metadata": {
        "id": "91MVRRaOj-FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STATS_FEATURE_SETTING = 3\n",
        "\n",
        "# 0 - Our new features (including autocorrelation, kurtosis, etc.)\n",
        "# 1 - Our new features, excluding extras like autocorrelation and kurtosis\n",
        "# 2 - Just pres_pct\n",
        "# 3 - Our new features, excluding extras. Do NOT threshold AUs before computing metrics. HSE gets 5 event features. OGAU gets num events and presence percent."
      ],
      "metadata": {
        "id": "b_KxZhrQtcmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NORMALIZE_DATA = 0\n",
        "\n",
        "# 0 - No time series normalization\n",
        "# 1 - Yes time series normalization (for each time window)"
      ],
      "metadata": {
        "id": "VIc4rrMwD8N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU72QcAcVyLy"
      },
      "source": [
        "# Installs & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZpFPBvsPdNe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7QHCYhjQTQ0"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "import pandas as pd\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.auth import default\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRX3feHKIR1z"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "# Ignore all warnings\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kKGUjhTqo-3"
      },
      "source": [
        "# Runtime Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6_t0DlpTFmi"
      },
      "outputs": [],
      "source": [
        "# SAVE VARIABLES\n",
        "import pickle\n",
        "\n",
        "def get_var_name(our_variable):\n",
        "    namespace = globals()\n",
        "    for name, obj in namespace.items():\n",
        "        if obj is our_variable:\n",
        "            return name\n",
        "    return None\n",
        "\n",
        "# Save the dictionary to a file using pickle\n",
        "def save_var(our_variable, RUNTIME_VAR_PATH=RUNTIME_VAR_PATH, forced_name=None):\n",
        "  if forced_name is None:\n",
        "    name_now = get_var_name(our_variable)\n",
        "  else:\n",
        "    name_now = forced_name\n",
        "\n",
        "  with open(RUNTIME_VAR_PATH + f'{name_now}.pkl', 'wb') as file:\n",
        "      pickle.dump(our_variable, file)\n",
        "\n",
        "def load_var(variable_name, RUNTIME_VAR_PATH=RUNTIME_VAR_PATH):\n",
        "  # Load from the file\n",
        "  with open(RUNTIME_VAR_PATH + f'{variable_name}.pkl', 'rb') as file:\n",
        "      return pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r22R0VLV4Gl"
      },
      "source": [
        "# Outpatient Mood Tracking Sheet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_bCqQTwV0cr"
      },
      "outputs": [],
      "source": [
        "# Mood_Tracking Sheet ID\n",
        "sheet_id = OUTPATIENT_MOOD_SHEET_ID\n",
        "\n",
        "## read data and put it in a dataframe\n",
        "spreadsheet = gc.open_by_key(sheet_id)\n",
        "\n",
        "wks = spreadsheet.worksheet(f'{PAT_NOW}')\n",
        "\n",
        "data = wks.get_all_values()\n",
        "headers = data.pop(0)\n",
        "\n",
        "df = pd.DataFrame(data, columns=headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IyDMQz7PvrR"
      },
      "outputs": [],
      "source": [
        "## Preprocess the mood tracking sheet\n",
        "\n",
        "# Replace the P_number mood headers with just the mood\n",
        "# df.columns = df.columns.str.replace('P[0-9]+ ', '')\n",
        "\n",
        "# Properly deal with the missing values\n",
        "df = df.replace('', np.nan).replace(' ', np.nan).fillna(value=np.nan)\n",
        "\n",
        "df_moodTracking = df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_moodTracking.loc[df_moodTracking['Follow-up Time'] == 'Screening', 'MADRS'] = df_moodTracking.apply(\n",
        "    lambda row: df_moodTracking[(df_moodTracking['Follow-up Time'] == 'Baseline')]['MADRS'].values[0]\n",
        "    if pd.isna(row['MADRS']) and row['Follow-up Time'] == 'Screening' else row['MADRS'], axis=1)"
      ],
      "metadata": {
        "id": "20LO991XzNAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows where we don't have a video file\n",
        "df_moodTracking = df_moodTracking.dropna(subset=['Filename']).reset_index(drop=True)\n",
        "\n",
        "# Remove rows where we have a video file, but no MADRS score\n",
        "df_moodTracking = df_moodTracking.dropna(subset=['MADRS']).reset_index(drop=True)\n",
        "\n",
        "# Remove duplicate Filenames\n",
        "df_moodTracking = df_moodTracking.drop_duplicates(subset=['Filename']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "12v2satGzkoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "def normalize_columns(df, method=1):\n",
        "    # Create a copy of the DataFrame\n",
        "    normalized_df = df.copy()\n",
        "\n",
        "    # Get the column names excluding 'Datetime'\n",
        "    columns_to_normalize = [col for col in normalized_df.columns if col != 'Filename' and col != 'Follow-up Time']\n",
        "\n",
        "    if method == 1:\n",
        "        # No scaling or normalization\n",
        "        pass\n",
        "\n",
        "    elif method == 2:\n",
        "        # MinMax scaling to range 0 to 10\n",
        "        scaler = MinMaxScaler(feature_range=(0, 10))\n",
        "        normalized_df[columns_to_normalize] = scaler.fit_transform(normalized_df[columns_to_normalize])\n",
        "\n",
        "    elif method == 3:\n",
        "        # MinMax scaling to range 0 to 1\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        normalized_df[columns_to_normalize] = scaler.fit_transform(normalized_df[columns_to_normalize])\n",
        "\n",
        "    elif method == 4:\n",
        "        # Log scaling\n",
        "        normalized_df[columns_to_normalize] = normalized_df[columns_to_normalize].astype(float)\n",
        "        normalized_df[columns_to_normalize] = np.log1p(normalized_df[columns_to_normalize])\n",
        "\n",
        "    elif method == 5:\n",
        "        # Standard normalization (Z-score normalization)\n",
        "        scaler = StandardScaler()\n",
        "        normalized_df[columns_to_normalize] = scaler.fit_transform(normalized_df[columns_to_normalize])\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid method. Choose a value between 1 and 5.\")\n",
        "\n",
        "    return normalized_df"
      ],
      "metadata": {
        "id": "XLMWo6cmiiE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_moodTracking = normalize_columns(df_moodTracking, method=3)"
      ],
      "metadata": {
        "id": "4pkPsvQxixAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_moodTracking"
      ],
      "metadata": {
        "id": "cU4FBkYWzktU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t52_30WQzYeh"
      },
      "source": [
        "# HSEmotion & OpenGraphAU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUg5iqkRtIJX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def get_dict(output_dir, file_now='outputs_hse.csv', filterOutLR=True):\n",
        "\n",
        "  # Initialize an empty dictionary to store the dataframes\n",
        "  df_dict = {}\n",
        "\n",
        "  # Loop through the subfolders in alphabetical order\n",
        "  for subfolder_name in sorted(os.listdir(output_dir)):\n",
        "\n",
        "    # Check if the subfolder contains CSV files\n",
        "    subfolder_path = os.path.join(output_dir, subfolder_name)\n",
        "    if not os.path.isdir(subfolder_path):\n",
        "      continue\n",
        "\n",
        "    # Load the first CSV file in the subfolder into a dataframe\n",
        "    csv_file_path = os.path.join(subfolder_path, file_now)\n",
        "    if not os.path.isfile(csv_file_path):\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      df_temp = pd.read_csv(csv_file_path)\n",
        "    except:\n",
        "      df_temp = pd.DataFrame(columns=['frame', 'timestamp', 'success', 'AU1', 'AU2', 'AU4', 'AU5', 'AU6', 'AU7', 'AU9',\n",
        "       'AU10', 'AU11', 'AU12', 'AU13', 'AU14', 'AU15', 'AU16', 'AU17', 'AU18',\n",
        "       'AU19', 'AU20', 'AU22', 'AU23', 'AU24', 'AU25', 'AU26', 'AU27', 'AU32',\n",
        "       'AU38', 'AU39'])\n",
        "\n",
        "\n",
        "    # OpenGraphAU - we are filtering out L and R!\n",
        "    if filterOutLR:\n",
        "      df_temp = df_temp.filter(regex='^(?!AUL|AUR)')\n",
        "\n",
        "    # Add the dataframe to the dictionary with the subfolder name as the key\n",
        "    df_dict[subfolder_name] = df_temp\n",
        "\n",
        "  return df_dict\n",
        "\n",
        "def create_binary_columns(df, threshold):\n",
        "    # adds classification columns to opengraphAU\n",
        "    for col in df.columns:\n",
        "        if col.startswith('AU'):\n",
        "            # Add _c to the column name for the new column\n",
        "            new_col_name = col + '_c'\n",
        "            # Apply the binary classification to the new column\n",
        "            df[new_col_name] = df[col].apply(lambda x: 1 if x >= threshold else 0)\n",
        "            # Add _r to the original column name\n",
        "            df.rename(columns={col: col + '_r'}, inplace=True)\n",
        "    return df\n",
        "\n",
        "def remove_columns_ending_with_r(df):\n",
        "    columns_to_drop = [col for col in df.columns if col.endswith('_r')]\n",
        "    df_new = df.drop(columns=columns_to_drop, inplace=False)\n",
        "    return df_new\n",
        "\n",
        "def only_successful_frames(df):\n",
        "    # get frames where AU/emotion detection was successful!\n",
        "    return df[df['success'] == 1]\n",
        "\n",
        "\n",
        "def take_time_subset(df, num_mins=10, frames_per_second=5):\n",
        "    # Takes the first num_mins minutes of data we have from a dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "    num_wanted_frames = num_mins * 60 * frames_per_second\n",
        "    df = df[:num_wanted_frames]\n",
        "    return df\n",
        "\n",
        "def take_time_subsets(df, num_mins=10, frames_per_second=5):\n",
        "    # Reset the index of the dataframe\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Calculate the number of frames for each interval\n",
        "    interval_frame_count = num_mins * 60 * frames_per_second\n",
        "\n",
        "    # Calculate the total number of intervals\n",
        "    total_frames = len(df)\n",
        "    total_intervals = (total_frames + interval_frame_count - 1) // interval_frame_count\n",
        "\n",
        "    # Create a list to store each interval DataFrame\n",
        "    intervals = []\n",
        "\n",
        "    # Slice the DataFrame into intervals and append to the list\n",
        "    for i in range(total_intervals):\n",
        "        start = i * interval_frame_count\n",
        "        end = min((i + 1) * interval_frame_count, total_frames)\n",
        "        interval_df = df[start:end]\n",
        "        intervals.append(interval_df)\n",
        "\n",
        "    return intervals\n",
        "\n",
        "\n",
        "def apply_function_to_dict(dictionary, func, **kwargs):\n",
        "    \"\"\"\n",
        "    Apply a function to each DataFrame in a dictionary and return a modified copy of the dictionary.\n",
        "\n",
        "    Args:\n",
        "        dictionary (dict): The dictionary containing DataFrames.\n",
        "        func (function): The function to apply to each DataFrame.\n",
        "        **kwargs: Additional keyword arguments to pass to the function.\n",
        "\n",
        "    Returns:\n",
        "        dict: A modified copy of the dictionary with the function applied to each DataFrame.\n",
        "    \"\"\"\n",
        "    return {key: func(df, **kwargs) for key, df in dictionary.items()}\n",
        "\n",
        "def apply_function_to_dict_list(dictionary, func, **kwargs):\n",
        "    \"\"\"\n",
        "    Apply a function to each DataFrame in a dictionary where values are LISTS of dfs and return a modified copy of the dictionary.\n",
        "\n",
        "    Args:\n",
        "        dictionary (dict): The dictionary containing DataFrames.\n",
        "        func (function): The function to apply to each DataFrame.\n",
        "        **kwargs: Additional keyword arguments to pass to the function.\n",
        "\n",
        "    Returns:\n",
        "        dict: A modified copy of the dictionary with the function applied to each DataFrame.\n",
        "    \"\"\"\n",
        "    return {key: [func(df, **kwargs) for df in df_list] for key, df_list in dictionary.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEea0RmkACqS"
      },
      "outputs": [],
      "source": [
        "dfs_hsemotion = get_dict(COMBINED_OUTPUT_DIRECTORY, file_now='outputs_hse.csv')\n",
        "dfs_hsemotion = apply_function_to_dict(dfs_hsemotion, only_successful_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQjRLc6E7st0"
      },
      "outputs": [],
      "source": [
        "dfs_opengraphau = get_dict(COMBINED_OUTPUT_DIRECTORY, file_now='outputs_ogau.csv')\n",
        "dfs_opengraphau = apply_function_to_dict(dfs_opengraphau, only_successful_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA_FYs-8e12m"
      },
      "outputs": [],
      "source": [
        "# SAVE THE HSEMOTION AND OPENGRAPHAU DICTIONARIES\n",
        "\n",
        "save_var(dfs_hsemotion, forced_name=f'dfs_hsemotion_outpatient_raw_{PAT_NOW}')\n",
        "\n",
        "save_var(dfs_opengraphau, forced_name=f'dfs_opengraphau_outpatient_raw_{PAT_NOW}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c_gdTS1fAoE"
      },
      "outputs": [],
      "source": [
        "# LOAD THE HSEMOTION AND OPENGRAPHAU DICTIONARIES\n",
        "\n",
        "dfs_hsemotion = load_var(f'dfs_hsemotion_outpatient_raw_{PAT_NOW}')\n",
        "\n",
        "dfs_opengraphau = load_var(f'dfs_opengraphau_outpatient_raw_{PAT_NOW}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through and print the shapes\n",
        "for key in dfs_opengraphau.keys():\n",
        "  print(key, dfs_opengraphau[key].shape)"
      ],
      "metadata": {
        "id": "DUgg-tWbD0gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make them into lists of X minutes of valid data from each video\n",
        "\n",
        "TIME_INTERVALS = [5, 10]\n",
        "\n",
        "dfs_opengraphau_times = {}\n",
        "dfs_hsemotion_times = {}\n",
        "\n",
        "for interval_now in TIME_INTERVALS:\n",
        "  dfs_opengraphau_now = apply_function_to_dict(dfs_opengraphau, take_time_subsets, num_mins=interval_now)\n",
        "  dfs_opengraphau_times[interval_now] = dfs_opengraphau_now\n",
        "  dfs_hsemotion_now = apply_function_to_dict(dfs_hsemotion, take_time_subsets, num_mins=interval_now)\n",
        "  dfs_hsemotion_times[interval_now] = dfs_hsemotion_now"
      ],
      "metadata": {
        "id": "6mOSFkHtFvXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug"
      ],
      "metadata": {
        "id": "ivt7KCq6iFAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK FOR EMPTY CSVs IN ALL FOLDERS\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Replace this with your specific root path\n",
        "root_path = COMBINED_OUTPUT_DIRECTORY\n",
        "\n",
        "# Loop through all subdirectories in the root path\n",
        "for subdir, dirs, files in os.walk(root_path):\n",
        "    for file in files:\n",
        "        # Check if the current file is 'outputs_ogau.csv'\n",
        "        if file == 'outputs_ogau.csv' or file == 'outputs_hse.csv':\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            try:\n",
        "                # Attempt to read the csv file\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                # Check if the DataFrame is empty\n",
        "                if df.empty:\n",
        "                    print(f\"Empty CSV in folder: {subdir}\")\n",
        "                # Check if the DataFrame has no columns\n",
        "                elif df.shape[1] == 0:\n",
        "                    print(f\"No columns in CSV in folder: {subdir}\")\n",
        "                # Check if the DataFrame has only header but no rows\n",
        "                elif df.shape[0] == 0:\n",
        "                    print(f\"Only header, no rows in CSV in folder: {subdir}\")\n",
        "            except pd.errors.EmptyDataError:\n",
        "                # This exception is raised if the CSV is empty/no columns\n",
        "                print(f\"CSV file is empty or has no columns in folder: {subdir}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while processing file {file_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "XH_WYGCviGVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d29KucDj8nhK"
      },
      "source": [
        "# Feature Extraction 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jeNfFA9Fo3z"
      },
      "source": [
        "## AU --> Emotion & Lower/Upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6z8PGgWp5Bm"
      },
      "outputs": [],
      "source": [
        "# Define emotion to AU mapping\n",
        "\n",
        "# OpenDBM:\n",
        "emo_AUs = {'Happiness': [6, 12],\n",
        "           'Sadness': [1, 4, 15],\n",
        "           'Surprise': [1, 2, 5, 26],\n",
        "           'Fear': [1, 2, 4, 5, 7, 20, 26],\n",
        "           'Anger': [4, 5, 7, 23],\n",
        "           'Disgust': [9, 15, 16],\n",
        "           'Contempt': [12, 14]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVaChzOTx522"
      },
      "outputs": [],
      "source": [
        "# Define AU to lower/upper\n",
        "\n",
        "# OpenDBM:\n",
        "AU_lower = [12, 15, 26, 20, 23, 14]\n",
        "AU_upper = [6, 1, 4, 2, 5, 7, 9]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnTMYRufFs6t"
      },
      "source": [
        "## Emotion Processing - HSEmotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujhYO5vxCY8S"
      },
      "outputs": [],
      "source": [
        "def only_successful_frames(df):\n",
        "    # get frames where AU/emotion detection was successful!\n",
        "    return df[df['success'] == 1]\n",
        "\n",
        "def filter_max_emotion(df, threshold):\n",
        "\n",
        "    if df.empty:\n",
        "      return df\n",
        "\n",
        "    # create a copy of the input df\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # success\n",
        "    df_copy = only_successful_frames(df_copy)\n",
        "\n",
        "    # find the maximum emotion value for each frame\n",
        "    cols_to_check = [col for col in df_copy.columns if col not in ['frame', 'success', 'timestamp']]\n",
        "    max_emotion = df_copy[cols_to_check].max(axis=1)\n",
        "\n",
        "    # set all non-max emotion values to 0 for each frame\n",
        "    for col in cols_to_check:\n",
        "      df_copy.loc[df_copy[col] < max_emotion, col] = 0\n",
        "\n",
        "    # filter for frames where max emotion is above threshold\n",
        "    df_filtered = df_copy[max_emotion > threshold]\n",
        "\n",
        "    return df_filtered\n",
        "\n",
        "def add_event_column(df, time_thresh, min_event_length):\n",
        "    \"\"\"\n",
        "    Adds an 'event' column to a dataframe, assigning an event ID to each frame based on the specified time threshold and\n",
        "    event length.\n",
        "\n",
        "    Args:\n",
        "        df (pandas DataFrame): A dataframe with video frames as rows and emotion features as columns. The dataframe\n",
        "            should have a 'timestamp' column containing the number of seconds since the video started for each frame.\n",
        "        time_thresh (int or float): The time threshold (in seconds) to use when defining events. Any two consecutive\n",
        "            frames with a time difference greater than this threshold will belong to different events.\n",
        "        min_event_length (int): The minimum length of an event. Events shorter than this length will be filtered out.\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame: A new dataframe with an additional 'event' column, leaving the input dataframe unchanged.\n",
        "    \"\"\"\n",
        "\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Make a copy of the dataframe\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Sort the dataframe by timestamp\n",
        "    df_copy = df_copy.sort_values('timestamp')\n",
        "\n",
        "    # Compute the time difference between consecutive frames\n",
        "    time_diff = df_copy['timestamp'].diff()\n",
        "\n",
        "    # Initialize event ID and current event emotion\n",
        "    global event_id, current_emotion\n",
        "    event_id = 1\n",
        "    current_emotion = None\n",
        "\n",
        "    # Initialize the event column with zeros\n",
        "    df_copy['event'] = 0\n",
        "\n",
        "    # Define emotion columns\n",
        "    emotion_cols = [col for col in df_copy.columns if col not in ['frame', 'success', 'timestamp']]\n",
        "\n",
        "    def assign_event_id(row, emotion_cols):\n",
        "        global event_id, current_emotion\n",
        "\n",
        "        this_frame_emotion = emotion_cols[np.nanargmax(row[emotion_cols].values)]\n",
        "        if pd.isnull(current_emotion) or (time_diff.loc[row.name] > time_thresh) or (this_frame_emotion != current_emotion):\n",
        "            event_id += 1\n",
        "            current_emotion = this_frame_emotion\n",
        "\n",
        "        return event_id\n",
        "\n",
        "    df_copy['event'] = df_copy.apply(lambda row: assign_event_id(row, emotion_cols), axis=1)\n",
        "\n",
        "    # Filter out events with length less than min_event_length\n",
        "    event_counts = df_copy['event'].value_counts()\n",
        "    short_events = event_counts[event_counts < min_event_length].index\n",
        "    df_copy = df_copy[~df_copy['event'].isin(short_events)]\n",
        "\n",
        "    # Factorize event column\n",
        "    df_copy['event'] = pd.factorize(df_copy['event'])[0]\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "def calculate_emotion_statistics(df):\n",
        "    \"\"\"\n",
        "    Calculates statistics for each emotion in a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pandas DataFrame): The DataFrame containing emotion values and event information.\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame: The DataFrame with statistics for each emotion.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if df.empty:\n",
        "      return df\n",
        "\n",
        "    # List of emotions (assuming they are the columns in the dataframe)\n",
        "    emotions = [col for col in df.columns if col not in ['frame', 'success', 'timestamp', 'event']]\n",
        "\n",
        "    # Initialize an empty dictionary to store the computed statistics\n",
        "    stats = {'emotion': [], 'num_events': [], 'avg_intensity': [], 'avg_event_length': [], 'avg_event_duration': [],\n",
        "             'std': [], 'skewness': [], 'kurtosis': [], 'autocorrelation': []}\n",
        "\n",
        "    for emotion in emotions:\n",
        "        # Filter the dataframe for rows with non-zero values for the current emotion\n",
        "        filtered_df = df[df[emotion] != 0]\n",
        "\n",
        "        # Calculate the statistics for the emotion\n",
        "        num_events = filtered_df['event'].nunique()\n",
        "        avg_intensity = filtered_df[emotion].mean()\n",
        "        avg_event_length = filtered_df.groupby('event', group_keys=True).size().mean()\n",
        "        avg_event_duration = filtered_df.groupby('event', group_keys=True)['timestamp'].apply(lambda x: (x.max() - x.min())).mean()\n",
        "\n",
        "        # Additional features\n",
        "        std = filtered_df[emotion].std()\n",
        "        skewness = filtered_df[emotion].skew()\n",
        "        kurtosis = filtered_df[emotion].kurtosis()\n",
        "        autocorrelation = np.corrcoef(filtered_df[emotion][:-1], filtered_df[emotion][1:])[0, 1]\n",
        "\n",
        "        # Add the statistics to the dictionary\n",
        "        stats['emotion'].append(emotion)\n",
        "        stats['num_events'].append(num_events)\n",
        "        stats['avg_intensity'].append(avg_intensity)\n",
        "        stats['avg_event_length'].append(avg_event_length)\n",
        "        stats['avg_event_duration'].append(avg_event_duration)\n",
        "        stats['std'].append(std)\n",
        "        stats['skewness'].append(skewness)\n",
        "        stats['kurtosis'].append(kurtosis)\n",
        "        stats['autocorrelation'].append(autocorrelation)\n",
        "\n",
        "    # Create a DataFrame from the dictionary of statistics\n",
        "    stats_df = pd.DataFrame(stats)\n",
        "    stats_df = stats_df.fillna(0)\n",
        "\n",
        "    return stats_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_empty_dfs(dictionary):\n",
        "  # when we do emotion processing, some dfs will have ZERO successful frames,\n",
        "  # leading to ZERO events, and an empty df.\n",
        "  # we need to fill the empty dfs with a df with all 0s\n",
        "\n",
        "  non_empty_dfs = [df for df in dictionary.values() if not df.empty]\n",
        "\n",
        "  if not non_empty_dfs:\n",
        "      return dictionary  # Return the original dictionary if all DataFrames are empty\n",
        "\n",
        "  non_empty_df = non_empty_dfs[0]  # Choose the first non-empty DataFrame as replacement\n",
        "\n",
        "  modified_dictionary = {}\n",
        "  for key, df in dictionary.items():\n",
        "      if df.empty:\n",
        "          modified_df = pd.DataFrame(0, index=non_empty_df.index, columns=non_empty_df.columns)\n",
        "          # Preserve string columns from non-empty DataFrame\n",
        "          for column in non_empty_df.columns:\n",
        "              if non_empty_df[column].dtype == object:\n",
        "                  modified_df[column] = non_empty_df[column]\n",
        "      else:\n",
        "          modified_df = df.copy()\n",
        "\n",
        "      modified_dictionary[key] = modified_df\n",
        "\n",
        "  return modified_dictionary"
      ],
      "metadata": {
        "id": "hwRS2uUqcHNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jZKlIodZ1G0"
      },
      "outputs": [],
      "source": [
        "hssemotion_fme = apply_function_to_dict(dfs_hsemotion, filter_max_emotion, threshold=0.5)\n",
        "hsemotion_aec = apply_function_to_dict(hssemotion_fme, add_event_column, time_thresh=10, min_event_length=1)\n",
        "hsemotion_emo_stats = apply_function_to_dict(hsemotion_aec, calculate_emotion_statistics)\n",
        "hsemotion_emo_stats = fill_empty_dfs(hsemotion_emo_stats)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_var(hsemotion_emo_stats, forced_name=f'hsemotion_emo_stats_outpatient_{PAT_NOW}')"
      ],
      "metadata": {
        "id": "XkSIdJ13FUym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ZqapEhJiAe"
      },
      "source": [
        "## Copying OpenDBM Feature Extraction\n",
        "\n",
        "We are manually coding this based on their documentation, with the goal of maximizing customization in the future. We've also applied their stats pipeline to other AU detectors beyond OpenFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijCyU-AVhAqR"
      },
      "source": [
        "### Convert AU to Emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8U9rA0HJYV1"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "def detect_emotions(df, method, emo_AUs, additional_filter=None):\n",
        "    # INPUT:\n",
        "    # df -- dataframe with AUs for each frame\n",
        "    # method -- must be 'OpenFace'\n",
        "    # emo_AUs -- the hash table\n",
        "    # additional_filter -- are we just doing lower half? upper half? This is None or a list of ints (which AUs to keep)\n",
        "\n",
        "    # OUTPUT:\n",
        "    # 3 datafrmes. Each has emotion values for each frame\n",
        "    # emo_hard, emo_soft, emo_binary (see OpenDBM docs for details)\n",
        "\n",
        "\n",
        "    if df.empty:\n",
        "      return (df, df, df)\n",
        "    # We start by mapping AUs to emotions for each of our two methods\n",
        "    # Using this mapping: https://aicure.github.io/open_dbm/docs/emotional-expressivity\n",
        "    if method == 'OpenFace':\n",
        "        columns = ['AU01_r','AU02_r', 'AU04_r', 'AU05_r', 'AU06_r', 'AU07_r', 'AU09_r', 'AU10_r',\n",
        "                    'AU12_r', 'AU14_r', 'AU15_r', 'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r',\n",
        "                    'AU26_r', 'AU45_r',\n",
        "                    'AU01_c',\n",
        "                    'AU02_c',\n",
        "                    'AU04_c',\n",
        "                    'AU05_c',\n",
        "                    'AU06_c',\n",
        "                    'AU07_c',\n",
        "                    'AU09_c',\n",
        "                    'AU10_c',\n",
        "                    'AU12_c',\n",
        "                    'AU14_c',\n",
        "                    'AU15_c',\n",
        "                    'AU17_c',\n",
        "                    'AU20_c',\n",
        "                    'AU23_c',\n",
        "                    'AU25_c',\n",
        "                    'AU26_c',\n",
        "                    'AU45_c']\n",
        "\n",
        "        # hash tables for presence and intensity\n",
        "        emo_AUs_presence = {}\n",
        "        emo_AUs_intensity = {}\n",
        "        for key in emo_AUs.keys(): # loop through emotion strings\n",
        "            new_values_r = [] # regression\n",
        "            new_values_c = [] # classification\n",
        "\n",
        "            for value in emo_AUs[key]:\n",
        "                if isinstance(value, int):\n",
        "                    AU_key_r = \"AU{:02d}_r\".format(value)\n",
        "                    AU_key_c = \"AU{:02d}_c\".format(value)\n",
        "                    if AU_key_r in columns:\n",
        "                        if additional_filter is not None:\n",
        "                          if value in additional_filter:\n",
        "                            new_values_r.append(AU_key_r)\n",
        "                        else:\n",
        "                          new_values_r.append(AU_key_r)\n",
        "                    if AU_key_c in columns:\n",
        "                        if additional_filter is not None:\n",
        "                          if value in additional_filter:\n",
        "                            new_values_c.append(AU_key_c)\n",
        "                        else:\n",
        "                          new_values_c.append(AU_key_c)\n",
        "            if new_values_r:\n",
        "                emo_AUs_intensity[key] = new_values_r\n",
        "            if new_values_c:\n",
        "                emo_AUs_presence[key] = new_values_c\n",
        "\n",
        "    # elif method == 'OpenGraphAU':\n",
        "    #     raise ValueError(\"Invalid method parameter. Method must be 'OpenFace'.\")\n",
        "        # columns = ['AU1', 'AU2', 'AU4', 'AU5', 'AU6', 'AU7', 'AU9',\n",
        "        #            'AU10', 'AU11', 'AU12', 'AU13', 'AU14', 'AU15', 'AU16', 'AU17',\n",
        "        #            'AU18', 'AU19', 'AU20', 'AU22', 'AU23', 'AU24', 'AU25', 'AU26', 'AU27', 'AU32',\n",
        "        #            'AU38', 'AU39']\n",
        "\n",
        "        # # add the classification columns!\n",
        "        # columns = [item for sublist in [[col+'_r', col+'_c'] for col in columns] for item in sublist]\n",
        "\n",
        "        # # hash tables for presence and intensity\n",
        "        # emo_AUs_presence = {}\n",
        "        # emo_AUs_intensity = {}\n",
        "        # for key in emo_AUs.keys():\n",
        "        #     new_values_r = []\n",
        "        #     new_values_c = []\n",
        "        #     for value in emo_AUs[key]:\n",
        "        #         if isinstance(value, int):\n",
        "        #             AU_key_r = f\"AU{value}_r\"\n",
        "        #             AU_key_c = f\"AU{value}_c\"\n",
        "        #             if AU_key_r in columns:\n",
        "        #                 if additional_filter is not None:\n",
        "        #                   if value in additional_filter:\n",
        "        #                     new_values_r.append(AU_key_r)\n",
        "        #                 else:\n",
        "        #                   new_values_r.append(AU_key_r)\n",
        "        #             if AU_key_c in columns:\n",
        "        #                 if additional_filter is not None:\n",
        "        #                   if value in additional_filter:\n",
        "        #                     new_values_c.append(AU_key_c)\n",
        "        #                 else:\n",
        "        #                   new_values_c.append(AU_key_c)\n",
        "        #     if new_values_r:\n",
        "        #         emo_AUs_intensity[key] = new_values_r\n",
        "        #     if new_values_c:\n",
        "        #         emo_AUs_presence[key] = new_values_c\n",
        "\n",
        "    else:\n",
        "        # if the method specified is not OpenFace or OpenGraphAU, raise an error (pipeline doesn't support others yet)\n",
        "        raise ValueError(\"Invalid method parameter. Method must be 'OpenFace'.\")\n",
        "\n",
        "    # Create an empty dictionary to store the emotion scores\n",
        "    emotion_scores_hard = {} # only non-zero if all AUs present\n",
        "    emotion_scores_soft = {} # average of AU intensities even if all not present\n",
        "    emotion_scores_binary = {} # 1 or 0: are all AUs present?\n",
        "\n",
        "    # Compute emotion scores for each emotion\n",
        "    for emotion in emo_AUs_presence.keys():\n",
        "        # Get the relevant columns for presence and intensity\n",
        "        presence_cols = emo_AUs_presence[emotion]\n",
        "        intensity_cols = emo_AUs_intensity[emotion]\n",
        "\n",
        "        # Compute the emotion score for each row in the dataframe\n",
        "        emotion_scores_hard[emotion] = df[intensity_cols].mean(axis=1) * df[presence_cols].all(axis=1)\n",
        "        emotion_scores_hard[emotion] = emotion_scores_hard[emotion].fillna(0)\n",
        "\n",
        "        emotion_scores_soft[emotion] = df[intensity_cols].mean(axis=1)\n",
        "        emotion_scores_soft[emotion] = emotion_scores_soft[emotion].fillna(0)\n",
        "\n",
        "        emotion_scores_binary[emotion] = df[presence_cols].all(axis=1)\n",
        "        emotion_scores_binary[emotion] = emotion_scores_binary[emotion].fillna(0)\n",
        "\n",
        "    # Create a new dataframe with the emotion scores\n",
        "    emotion_df_hard = pd.DataFrame(emotion_scores_hard)\n",
        "    emotion_df_soft = pd.DataFrame(emotion_scores_soft)\n",
        "    emotion_df_binary = pd.DataFrame(emotion_scores_binary)\n",
        "    emotion_df_binary = emotion_df_binary.replace({False: 0, True: 1})\n",
        "\n",
        "    # Let's add timestamp and success on\n",
        "    columns_of_interest = ['timestamp', 'success']\n",
        "    df_temp = df[columns_of_interest]\n",
        "\n",
        "    # Concatenate the columns from df2 with df1\n",
        "    emotion_df_hard = pd.concat([df_temp, emotion_df_hard], axis=1)\n",
        "    emotion_df_soft = pd.concat([df_temp, emotion_df_soft], axis=1)\n",
        "    emotion_df_binary = pd.concat([df_temp, emotion_df_binary], axis=1)\n",
        "\n",
        "    return emotion_df_hard, emotion_df_soft, emotion_df_binary\n",
        "\n",
        "\n",
        "\n",
        "def detect_emotions_og(df, method, emo_AUs, additional_filter=None):\n",
        "    # INPUT:\n",
        "    # df -- dataframe with AUs for each frame\n",
        "    # method -- must be 'OpenGraphAU'\n",
        "    # emo_AUs -- the hash table\n",
        "    # additional_filter -- are we just doing lower half? upper half? This is None or a list of ints (which AUs to keep)\n",
        "\n",
        "    # OUTPUT:\n",
        "    # 1 datafrme with emotion values for each frame\n",
        "    # emo_binary (see OpenDBM docs for details)\n",
        "\n",
        "\n",
        "    if df.empty:\n",
        "      return df\n",
        "    # We start by mapping AUs to emotions for each of our two methods\n",
        "    # Using this mapping: https://aicure.github.io/open_dbm/docs/emotional-expressivity\n",
        "\n",
        "\n",
        "    if method == 'OpenGraphAU':\n",
        "        columns = ['AU1', 'AU2', 'AU4', 'AU5', 'AU6', 'AU7', 'AU9',\n",
        "                   'AU10', 'AU11', 'AU12', 'AU13', 'AU14', 'AU15', 'AU16', 'AU17',\n",
        "                   'AU18', 'AU19', 'AU20', 'AU22', 'AU23', 'AU24', 'AU25', 'AU26', 'AU27', 'AU32',\n",
        "                   'AU38', 'AU39']\n",
        "\n",
        "        # add the classification columns!\n",
        "        columns = [item for sublist in [[col+'_r', col+'_c'] for col in columns] for item in sublist]\n",
        "\n",
        "        # hash tables for presence and intensity\n",
        "        emo_AUs_presence = {}\n",
        "        for key in emo_AUs.keys():\n",
        "            new_values_c = []\n",
        "            for value in emo_AUs[key]:\n",
        "                if isinstance(value, int):\n",
        "                    AU_key_c = f\"AU{value}_c\"\n",
        "\n",
        "                    if AU_key_c in columns:\n",
        "                        if additional_filter is not None:\n",
        "                          if value in additional_filter:\n",
        "                            new_values_c.append(AU_key_c)\n",
        "                        else:\n",
        "                          new_values_c.append(AU_key_c)\n",
        "            if new_values_c:\n",
        "                emo_AUs_presence[key] = new_values_c\n",
        "\n",
        "    else:\n",
        "        # if the method specified is not OpenFace or OpenGraphAU, raise an error (pipeline doesn't support others yet)\n",
        "        raise ValueError(\"Invalid method parameter. Method must be 'OpenGraphAU'.\")\n",
        "\n",
        "    # Create an empty dictionary to store the emotion scores\n",
        "    emotion_scores_binary = {} # 1 or 0: are all AUs present?\n",
        "\n",
        "    # Compute emotion scores for each emotion\n",
        "    for emotion in emo_AUs_presence.keys():\n",
        "        # Get the relevant columns for presence\n",
        "        presence_cols = emo_AUs_presence[emotion]\n",
        "\n",
        "        # Compute the emotion score for each row in the dataframe\n",
        "        emotion_scores_binary[emotion] = df[presence_cols].all(axis=1)\n",
        "        emotion_scores_binary[emotion] = emotion_scores_binary[emotion].fillna(0)\n",
        "\n",
        "    # Create a new dataframe with the emotion scores\n",
        "    emotion_df_binary = pd.DataFrame(emotion_scores_binary)\n",
        "    emotion_df_binary = emotion_df_binary.replace({False: 0, True: 1})\n",
        "\n",
        "    # Let's add timestamp and success on\n",
        "    columns_of_interest = ['timestamp', 'success']\n",
        "    df_temp = df[columns_of_interest]\n",
        "\n",
        "    # Concatenate the columns from df2 with df1\n",
        "    emotion_df_binary = pd.concat([df_temp, emotion_df_binary], axis=1)\n",
        "\n",
        "    return emotion_df_binary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2RyDtfUbkFe"
      },
      "outputs": [],
      "source": [
        "# Raw Variables for Emotional Expressivity!\n",
        "\n",
        "# key: (df_emohard, df_emosoft, df_emopres)\n",
        "\n",
        "openface_emoHardSoftPres = apply_function_to_dict(dfs_openface, detect_emotions, method='OpenFace', emo_AUs=emo_AUs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "opengraphau_emoPres = apply_function_to_dict(dfs_opengraphau, detect_emotions_og, method='OpenGraphAU', emo_AUs=emo_AUs)\n"
      ],
      "metadata": {
        "id": "LtqG8nfDBGHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-0Eqzje4OdE"
      },
      "outputs": [],
      "source": [
        "# This will help us get Raw Variables for Overall Expressivity!\n",
        "\n",
        "# key: (df_emohard, df_emosoft, df_emopres)\n",
        "\n",
        "\n",
        "openface_lowerHardSoftPres = apply_function_to_dict(dfs_openface, detect_emotions, method='OpenFace', emo_AUs=emo_AUs, additional_filter=AU_lower)\n",
        "openface_upperHardSoftPres = apply_function_to_dict(dfs_openface, detect_emotions, method='OpenFace', emo_AUs=emo_AUs, additional_filter=AU_upper)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opengraphau_lowerPres = apply_function_to_dict(dfs_opengraphau, detect_emotions_og, method='OpenGraphAU', emo_AUs=emo_AUs, additional_filter=AU_lower)\n",
        "opengraphau_upperPres = apply_function_to_dict(dfs_opengraphau, detect_emotions_og, method='OpenGraphAU', emo_AUs=emo_AUs, additional_filter=AU_upper)"
      ],
      "metadata": {
        "id": "p8iH_axvBCYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjInHNtNhZXd"
      },
      "source": [
        "### Apply Our HSEmotion Analysis to AU Detectors!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dux34pPqgSsW"
      },
      "outputs": [],
      "source": [
        "# Dictionary of just soft values\n",
        "openface_emoSoft = {key: val[1] for key, val in openface_emoHardSoftPres.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can't do this anymore since we're only using binary columns from opengraphAU!\n",
        "opengraphau_emoSoft = {key: val[1] for key, val in opengraphau_emoHardSoftPres.items()}\n"
      ],
      "metadata": {
        "id": "12M01MZPILF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-uhxDQHhf-a"
      },
      "outputs": [],
      "source": [
        "# We can't do this anymore since we're only using binary columns from opengraphAU!\n",
        "\n",
        "# OPENGRAPHAU\n",
        "\n",
        "opengraphau_fme = apply_function_to_dict(opengraphau_emoSoft, filter_max_emotion, threshold=0.5)\n",
        "opengraphau_aec = apply_function_to_dict(opengraphau_fme, add_event_column, time_thresh=10, min_event_length=1)\n",
        "opengraphau_emo_stats = apply_function_to_dict(opengraphau_aec, calculate_emotion_statistics)\n",
        "opengraphau_emo_stats = fill_empty_dfs(opengraphau_emo_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYVc6SnsmyL1"
      },
      "outputs": [],
      "source": [
        "# OPENFACE - affect/emotions (longer term)\n",
        "openface_emo_stats_dict = {}\n",
        "\n",
        "openface_fme = apply_function_to_dict(openface_emoSoft, filter_max_emotion, threshold=0.5)\n",
        "openface_aec = apply_function_to_dict(openface_fme, add_event_column, time_thresh=10, min_event_length=1)\n",
        "openface_emo_stats = apply_function_to_dict(openface_aec, calculate_emotion_statistics)\n",
        "openface_emo_stats = fill_empty_dfs(openface_emo_stats)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvxvof3FXOHG"
      },
      "source": [
        "### Action Units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCTZF0HDJnaR"
      },
      "outputs": [],
      "source": [
        "def rename_columns(df):\n",
        "    \"\"\"\n",
        "    Renames the columns in a DataFrame according to specified pattern.\n",
        "\n",
        "    Args:\n",
        "        df (pandas DataFrame): The DataFrame to rename columns.\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame: The DataFrame with renamed columns.\n",
        "    \"\"\"\n",
        "\n",
        "    # Copy the DataFrame\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Define the mapping for renaming columns\n",
        "    column_mapping = {\n",
        "        '_r': 'int',\n",
        "        '_c': 'pres'\n",
        "    }\n",
        "\n",
        "    # Function to rename the columns\n",
        "    def rename_column(column_name):\n",
        "        au_number = column_name[2:4]\n",
        "        if au_number.endswith('_'):\n",
        "          au_number = '0' + au_number[0:1]\n",
        "        suffix = column_name[-2:]\n",
        "        if suffix in column_mapping:\n",
        "            return f'fac_au{au_number}{column_mapping[suffix]}'\n",
        "        else:\n",
        "            return column_name\n",
        "\n",
        "    # Rename the columns in the copied DataFrame\n",
        "    df_copy = df_copy.rename(columns=rename_column)\n",
        "\n",
        "    return df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIyBBzD6VD2T"
      },
      "outputs": [],
      "source": [
        "def calculate_AU_statistics(df):\n",
        "    # Initialize an empty dictionary to store the computed statistics\n",
        "    stats = {'AU': [], 'pres_pct': [], 'int_mean': [], 'int_std': []}\n",
        "\n",
        "    # Iterate over the AU columns\n",
        "    for col in df.columns:\n",
        "        if col.startswith('fac_au') and ('pres' in col):\n",
        "            # Calculate the percentage of frames where AU is present\n",
        "            pres_pct = df[col].mean() * 100\n",
        "            # Extract the AU number\n",
        "            AU = col.split('au')[1][0:2]\n",
        "            # Calculate the mean and standard deviation of intensity for the AU\n",
        "            int_mean = df[f'fac_au{AU}int'].mean()\n",
        "            int_std = df[f'fac_au{AU}int'].std()\n",
        "\n",
        "            # Add the statistics to the dictionary\n",
        "            stats['AU'].append(AU)\n",
        "            stats['pres_pct'].append(pres_pct)\n",
        "            stats['int_mean'].append(int_mean)\n",
        "            stats['int_std'].append(int_std)\n",
        "\n",
        "    # Create a DataFrame from the dictionary of statistics\n",
        "    stats_df = pd.DataFrame(stats)\n",
        "\n",
        "    return stats_df\n",
        "\n",
        "def calculate_AU_statistics_og(df):\n",
        "    # Stats for ONLY binary columns!\n",
        "    # Initialize an empty dictionary to store the computed statistics\n",
        "    stats = {'AU': [], 'pres_pct': []}\n",
        "\n",
        "    # Iterate over the AU columns\n",
        "    for col in df.columns:\n",
        "        if col.startswith('fac_au') and ('pres' in col):\n",
        "            # Calculate the percentage of frames where AU is present\n",
        "            pres_pct = df[col].mean() * 100\n",
        "            # Extract the AU number\n",
        "            AU = col.split('au')[1][0:2]\n",
        "\n",
        "            # Add the statistics to the dictionary\n",
        "            stats['AU'].append(AU)\n",
        "            stats['pres_pct'].append(pres_pct)\n",
        "\n",
        "    # Create a DataFrame from the dictionary of statistics\n",
        "    stats_df = pd.DataFrame(stats)\n",
        "\n",
        "    return stats_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Elg7u1XDjv3X"
      },
      "outputs": [],
      "source": [
        "# Raw Variables!\n",
        "openface_radius_renamed = apply_function_to_dict(dfs_openface, rename_columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opengraphau_radius_renamed = apply_function_to_dict(dfs_opengraphau, rename_columns)\n"
      ],
      "metadata": {
        "id": "6wjYgSdZIhoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCYjKsxJkGP1"
      },
      "outputs": [],
      "source": [
        "# Derived Variables!\n",
        "\n",
        "openface_au_derived = apply_function_to_dict(openface_radius_renamed, calculate_AU_statistics)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opengraphau_au_derived = apply_function_to_dict(opengraphau_radius_renamed, calculate_AU_statistics_og)"
      ],
      "metadata": {
        "id": "G6MzjU03Ilv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqar5VjEXXo1"
      },
      "source": [
        "### Emotional Expressivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNQO9_omd8o3"
      },
      "outputs": [],
      "source": [
        "def calculate_emotion_express_statistics(tuple_to_unpack):\n",
        "    \"\"\"\n",
        "    Calculates statistics for each emotion in the given DataFrames.\n",
        "\n",
        "    Args:\n",
        "        tuple_to_unpack: 3-membered tuple that has:\n",
        "          df_emo_inthard (pandas DataFrame): DataFrame with emotion intensity (hard) values.\n",
        "          df_emo_intsoft (pandas DataFrame): DataFrame with emotion intensity (soft) values.\n",
        "          df_emo_pres (pandas DataFrame): DataFrame with emotion presence values.\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame: A DataFrame with statistics for each emotion.\n",
        "    \"\"\"\n",
        "    df_emo_inthard, df_emo_intsoft, df_emo_pres = tuple_to_unpack\n",
        "    stats = {'emotion': [], 'pres_pct': [], 'intsoft_mean': [], 'intsoft_std': [], 'inthard_mean': []}\n",
        "\n",
        "    emotions = [col for col in df_emo_inthard.columns if col not in ['frame', 'timestamp', 'success']]\n",
        "\n",
        "    for emotion in emotions:\n",
        "        pres_pct = (df_emo_pres[emotion] == 1).mean() * 100\n",
        "        intsoft_mean = df_emo_intsoft[emotion].mean()\n",
        "        intsoft_std = df_emo_intsoft[emotion].std()\n",
        "        inthard_mean = df_emo_inthard[emotion].mean()\n",
        "\n",
        "        stats['emotion'].append(emotion)\n",
        "        stats['pres_pct'].append(pres_pct)\n",
        "        stats['intsoft_mean'].append(intsoft_mean)\n",
        "        stats['intsoft_std'].append(intsoft_std)\n",
        "        stats['inthard_mean'].append(inthard_mean)\n",
        "\n",
        "    stats_df = pd.DataFrame(stats)\n",
        "    return stats_df\n",
        "\n",
        "def calculate_ee_stats_og(df_emo_pres):\n",
        "    \"\"\"\n",
        "    Calculates statistics for each emotion in the given DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df_emo_pres (pandas DataFrame): DataFrame with emotion presence values.\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame: A DataFrame with statistics for each emotion.\n",
        "    \"\"\"\n",
        "    stats = {'emotion': [], 'pres_pct': []}\n",
        "\n",
        "    emotions = [col for col in df_emo_pres.columns if col not in ['frame', 'timestamp', 'success']]\n",
        "\n",
        "    for emotion in emotions:\n",
        "        pres_pct = (df_emo_pres[emotion] == 1).mean() * 100\n",
        "\n",
        "\n",
        "        stats['emotion'].append(emotion)\n",
        "        stats['pres_pct'].append(pres_pct)\n",
        "\n",
        "    stats_df = pd.DataFrame(stats)\n",
        "    return stats_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch2W7fH3GHQf"
      },
      "outputs": [],
      "source": [
        "def calculate_ee_stats_hse(df, threshold):\n",
        "    \"\"\"\n",
        "    Calculates statistics for each emotion in the given DataFrame.\n",
        "\n",
        "    Args:\n",
        "    df with emotion intensities for every video frame\n",
        "    threshold for presence of emotion (i.e. 0.5)\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame: A DataFrame with statistics for each emotion.\n",
        "    \"\"\"\n",
        "    df_emo_intsoft = df\n",
        "    stats = {'emotion': [], 'pres_pct': [], 'intsoft_mean': [], 'intsoft_std': []}\n",
        "\n",
        "    emotions = [col for col in df_emo_intsoft.columns if col not in ['frame', 'timestamp', 'success']]\n",
        "\n",
        "    for emotion in emotions:\n",
        "        pres_pct = (df_emo_intsoft[emotion] >= threshold).mean() * 100\n",
        "        intsoft_mean = df_emo_intsoft[emotion].mean()\n",
        "        intsoft_std = df_emo_intsoft[emotion].std()\n",
        "\n",
        "        stats['emotion'].append(emotion)\n",
        "        stats['pres_pct'].append(pres_pct)\n",
        "        stats['intsoft_mean'].append(intsoft_mean)\n",
        "        stats['intsoft_std'].append(intsoft_std)\n",
        "\n",
        "    stats_df = pd.DataFrame(stats)\n",
        "    return stats_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw-ZfINxXcSO"
      },
      "outputs": [],
      "source": [
        "# Raw Variables for Emotional Expressivity were calculated above:\n",
        "# openface_emoHardSoftPres\n",
        "# opengraphau_emoHardSoftPres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPhTX8E0lWNf"
      },
      "outputs": [],
      "source": [
        "# Derived Variables for Emotional Expressivity\n",
        "openface_ee_derived = apply_function_to_dict(openface_emoHardSoftPres, calculate_emotion_express_statistics)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "opengraphau_ee_derived = apply_function_to_dict(opengraphau_emoPres, calculate_ee_stats_og)\n"
      ],
      "metadata": {
        "id": "SpqQMUX4B6hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hsemotion_ee_derived = apply_function_to_dict(dfs_hsemotion, calculate_ee_stats_hse, threshold=0.5)\n"
      ],
      "metadata": {
        "id": "FjWanNgEB6oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH3Cs6H61lan"
      },
      "source": [
        "### Overall Expressivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOFYuIJL1qML"
      },
      "outputs": [],
      "source": [
        "def compute_oe_raw_vars(regular_tuple, lower_tuple, upper_tuple):\n",
        "    # Takes in 3 3-membered tuples, each of which should be hardSoftPres\n",
        "    # regular, lower, upper\n",
        "\n",
        "    # Outputs one df with the raw variables for overall expressivity\n",
        "\n",
        "    df_emo_inthard, df_emo_intsoft, df_emo_pres = regular_tuple\n",
        "    df_emo_inthard_lower, df_emo_intsoft_lower, df_emo_pres_lower = lower_tuple\n",
        "    df_emo_inthard_upper, df_emo_intsoft_upper, df_emo_pres_upper = upper_tuple\n",
        "\n",
        "    # Calculate the average values for emo_intsoft and emo_inthard across all frames\n",
        "    avg_emo_intsoft = df_emo_intsoft.mean(axis=1)\n",
        "    avg_emo_inthard = df_emo_inthard.mean(axis=1)\n",
        "\n",
        "    # Calculate lower and upper averages across all frames\n",
        "    avg_emo_intsoft_lower = df_emo_intsoft_lower.mean(axis=1)\n",
        "    avg_emo_inthard_lower = df_emo_inthard_lower.mean(axis=1)\n",
        "    avg_emo_intsoft_upper = df_emo_intsoft_upper.mean(axis=1)\n",
        "    avg_emo_inthard_upper = df_emo_inthard_upper.mean(axis=1)\n",
        "\n",
        "    # Create a new dataframe with the computed statistics\n",
        "    stats_df = pd.DataFrame({'comintsoft': avg_emo_intsoft, 'cominthard': avg_emo_inthard,\n",
        "                             'comlowintsoft': avg_emo_intsoft_lower, 'comlowinthard': avg_emo_inthard_lower,\n",
        "                             'comuppintsoft': avg_emo_intsoft_upper, 'comuppinthard': avg_emo_inthard_upper,})\n",
        "\n",
        "    return stats_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH1M_yel8kWa"
      },
      "outputs": [],
      "source": [
        "def apply_function_to_dict_three(d1, d2, d3, func, **kwargs):\n",
        "    \"\"\"\n",
        "    Apply a function that takes in 3 dfs and return a modified dictionary\n",
        "\n",
        "    Args:\n",
        "        d1, d2, d3: The dictionaries containing DataFrames.\n",
        "        func (function): The function to apply to each DataFrame.\n",
        "        **kwargs: Additional keyword arguments to pass to the function.\n",
        "\n",
        "    Returns:\n",
        "        dict_final: A modified copy of the dictionary with the function applied to each DataFrame.\n",
        "    \"\"\"\n",
        "    dict_final = {}\n",
        "    for key in d1.keys():\n",
        "      dict_final[key] = func(d1[key], d2[key], d3[key], **kwargs)\n",
        "\n",
        "    return dict_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhzigaGI8X_g"
      },
      "outputs": [],
      "source": [
        "# Raw Variables for Overall Expressivity!\n",
        "\n",
        "#openface_oe_raw = apply_function_to_dict_three(openface_emoHardSoftPres, openface_lowerHardSoftPres, openface_upperHardSoftPres, compute_oe_raw_vars)\n",
        "#opengraphau_oe_raw = apply_function_to_dict_three(opengraphau_emoHardSoftPres, opengraphau_lowerHardSoftPres, opengraphau_upperHardSoftPres, compute_oe_raw_vars)\n",
        "\n",
        "\n",
        "\n",
        "openface_emo = openface_emoHardSoftPres\n",
        "openface_lower = openface_lowerHardSoftPres\n",
        "openface_upper = openface_upperHardSoftPres\n",
        "\n",
        "# Call the compute_oe_raw_vars function with the sampled items\n",
        "openface_oe_raw = apply_function_to_dict_three(openface_emo, openface_lower, openface_upper, compute_oe_raw_vars)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can't do this anymore! Probabilities are NOT intensities!\n",
        "\n",
        "opengraphau_emo = opengraphau_emoHardSoftPres\n",
        "opengraphau_lower = opengraphau_lowerHardSoftPres\n",
        "opengraphau_upper = opengraphau_upperHardSoftPres\n",
        "\n",
        "# Call the compute_oe_raw_vars function with the sampled items\n",
        "opengraphau_oe_raw = apply_function_to_dict_three(opengraphau_emo, opengraphau_lower, opengraphau_upper, compute_oe_raw_vars)"
      ],
      "metadata": {
        "id": "XFZd82J0B1pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig8iYi3v_KxG"
      },
      "outputs": [],
      "source": [
        "def calculate_oe_summary_statistics(df):\n",
        "    # Compute comintsoft_pct\n",
        "    comintsoft_pct = (df['comintsoft'] > 0).mean() * 100\n",
        "\n",
        "    # Compute comintsoft_mean and comintsoft_std\n",
        "    comintsoft_mean = df['comintsoft'].mean()\n",
        "    comintsoft_std = df['comintsoft'].std()\n",
        "\n",
        "    # Compute cominthard_mean and cominthard_std\n",
        "    cominthard_mean = df['cominthard'].mean()\n",
        "    cominthard_std = df['cominthard'].std()\n",
        "\n",
        "    # Compute comlowintsoft_pct\n",
        "    comlowintsoft_pct = (df['comlowintsoft'] > 0).mean() * 100\n",
        "\n",
        "    # Compute comlowintsoft_mean and comlowintsoft_std\n",
        "    comlowintsoft_mean = df['comlowintsoft'].mean()\n",
        "    comlowintsoft_std = df['comlowintsoft'].std()\n",
        "\n",
        "    # Compute comuppinthard_mean and comuppinthard_std\n",
        "    comuppinthard_mean = df['comuppinthard'].mean()\n",
        "    comuppinthard_std = df['comuppinthard'].std()\n",
        "\n",
        "    # Create a new DataFrame with the summary statistics\n",
        "    summary_df = pd.DataFrame({\n",
        "        'comintsoft_pct': [comintsoft_pct],\n",
        "        'comintsoft_mean': [comintsoft_mean],\n",
        "        'comintsoft_std': [comintsoft_std],\n",
        "        'cominthard_mean': [cominthard_mean],\n",
        "        'cominthard_std': [cominthard_std],\n",
        "        'comlowintsoft_pct': [comlowintsoft_pct],\n",
        "        'comlowintsoft_mean': [comlowintsoft_mean],\n",
        "        'comlowintsoft_std': [comlowintsoft_std],\n",
        "        'comuppinthard_mean': [comuppinthard_mean],\n",
        "        'comuppinthard_std': [comuppinthard_std]\n",
        "    })\n",
        "\n",
        "    return summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOTxb6GJBWfs"
      },
      "outputs": [],
      "source": [
        "# Derived Variables for Overall Expressivity!\n",
        "\n",
        "openface_oe_derived = apply_function_to_dict(openface_oe_raw, calculate_oe_summary_statistics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can't do this anymore since probabilities are not intensities!\n",
        "\n",
        "opengraphau_oe_derived = apply_function_to_dict(opengraphau_oe_raw, calculate_oe_summary_statistics)\n"
      ],
      "metadata": {
        "id": "jETLTmXEB-8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction 2.0"
      ],
      "metadata": {
        "id": "u8SkarQvEmXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew, kurtosis\n",
        "from statsmodels.tsa.stattools import acf\n",
        "\n",
        "def binarize_cols(df, threshold=0.5):\n",
        "  new_df = df.copy()\n",
        "  emotions = [col for col in new_df.columns if col not in ['frame', 'success', 'timestamp']]\n",
        "\n",
        "  for emotion in emotions:\n",
        "      new_df[f'{emotion}_Raw'] = new_df[emotion]\n",
        "      new_df[f'{emotion}_Binary'] = (new_df[f'{emotion}_Raw'] >= threshold).astype(int)\n",
        "\n",
        "  new_df = new_df.drop(columns=emotions, inplace=False)\n",
        "\n",
        "  return new_df\n",
        "\n",
        "\n",
        "def fill_empty_dfs_lists(dictionary):\n",
        "  # when we do emotion processing, some dfs will have ZERO successful frames,\n",
        "  # leading to ZERO events, and an empty df.\n",
        "  # we need to fill the empty dfs with a df with all 0s\n",
        "\n",
        "  non_empty_dfs = [[df for df in df_list if not df.empty] for df_list in dictionary.values()]\n",
        "\n",
        "  if not non_empty_dfs:\n",
        "      return dictionary  # Return the original dictionary if all DataFrames are empty\n",
        "\n",
        "  non_empty_df = non_empty_dfs[0][0]  # Choose the first non-empty DataFrame as replacement\n",
        "\n",
        "  modified_dictionary = {}\n",
        "  for key, df_list in dictionary.items():\n",
        "      modified_df_list = []\n",
        "      for df in df_list:\n",
        "        if df.empty:\n",
        "            modified_df = pd.DataFrame(0, index=non_empty_df.index, columns=non_empty_df.columns)\n",
        "            # Preserve string columns from non-empty DataFrame\n",
        "            for column in non_empty_df.columns:\n",
        "                if non_empty_df[column].dtype == object:\n",
        "                    modified_df[column] = non_empty_df[column]\n",
        "        else:\n",
        "            modified_df = df.copy()\n",
        "\n",
        "        modified_df_list.append(modified_df)\n",
        "\n",
        "      modified_dictionary[key] = modified_df_list\n",
        "\n",
        "  return modified_dictionary\n",
        "\n",
        "def analyze_emotion_events_v2(df, max_frame_gap=10, event_minimum_num_frames=1, method='HSE'):\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Emotions to analyze\n",
        "    emotions_raw = [col for col in df.columns if col not in ['frame', 'success', 'timestamp']]\n",
        "    # Removing \"_Raw\" or \"_Binary\" from each string\n",
        "    processed_strings = [s.replace(\"_Raw\", \"\").replace(\"_Binary\", \"\") for s in emotions_raw]\n",
        "    # Eliminating duplicates\n",
        "    emotions = list(set(processed_strings))\n",
        "\n",
        "    # Create DataFrame for results\n",
        "    if STATS_FEATURE_SETTING == 0:\n",
        "        results_df = pd.DataFrame(index=['avg_event_length', 'avg_event_duration', 'total_num_events', 'avg_probability', 'std', 'skewness', 'kurtosis', 'autocorrelation', 'pres_pct'])\n",
        "    elif STATS_FEATURE_SETTING == 1 or (STATS_FEATURE_SETTING == 3 and method == 'HSE'):\n",
        "        results_df = pd.DataFrame(index=['avg_event_length', 'total_num_events', 'avg_probability', 'std', 'pres_pct'])\n",
        "    elif STATS_FEATURE_SETTING == 2:\n",
        "        results_df = pd.DataFrame(index=['pres_pct'])\n",
        "    elif STATS_FEATURE_SETTING == 3 and (method == 'OGAU' or method=='OF'):\n",
        "        results_df = pd.DataFrame(index=['pres_pct', 'total_num_events'])\n",
        "\n",
        "\n",
        "    def detect_events(emotion_binary_col):\n",
        "        probThreshold = 0.5 # irrelevant because it's a binary column\n",
        "        minInterval = max_frame_gap\n",
        "        minDuration = event_minimum_num_frames\n",
        "\n",
        "        probBinary = emotion_binary_col > probThreshold\n",
        "\n",
        "        # Using np.diff to find changes in the binary array\n",
        "        changes = np.diff(probBinary.astype(int))\n",
        "\n",
        "        # Identify start (1) and stop (-1) points\n",
        "        starts = np.where(changes == 1)[0] + 1  # +1 to correct the index shift caused by diff\n",
        "        stops = np.where(changes == -1)[0] + 1\n",
        "\n",
        "        # Adjust for edge cases\n",
        "        if probBinary.iloc[0]:\n",
        "            starts = np.insert(starts, 0, 0)\n",
        "        if probBinary.iloc[-1]:\n",
        "            stops = np.append(stops, len(probBinary))\n",
        "\n",
        "        # Merge close events and filter by duration\n",
        "        events = []\n",
        "        for start, stop in zip(starts, stops):\n",
        "\n",
        "            # Construct the event considering only indices where probBinary is 1\n",
        "            event = np.arange(start, stop)[probBinary[start:stop].values]\n",
        "\n",
        "            # Check if there is a previous event to potentially merge with\n",
        "            if events and event.size > 0 and events[-1][-1] >= start - minInterval:\n",
        "                # Merge with the previous event\n",
        "                events[-1] = np.unique(np.concatenate([events[-1], event]))\n",
        "            elif event.size >= event_minimum_num_frames:\n",
        "                events.append(event)\n",
        "\n",
        "        # Filter events by minimum duration\n",
        "        valid_events = [event for event in events if len(event) >= minDuration]\n",
        "\n",
        "        return valid_events\n",
        "\n",
        "    for emotion in emotions:\n",
        "        # Identify events\n",
        "        emotion_binary_col = df[f'{emotion}_Binary']\n",
        "        emotion_presence = df[f'{emotion}_Binary'].sum()\n",
        "        pres_pct = emotion_presence / len(df) * 100  # Percentage of frames where emotion is present\n",
        "        events = detect_events(emotion_binary_col)\n",
        "\n",
        "        if not(STATS_FEATURE_SETTING == 2):\n",
        "            # Calculate features for each event\n",
        "            if events:\n",
        "                event_lengths = [len(event) for event in events]\n",
        "                event_durations = [event[-1] - event[0] + 1 for event in events]\n",
        "                probabilities = [df.loc[event, f'{emotion}_Raw'].values for event in events]\n",
        "                probabilities_flattened = np.concatenate(probabilities)\n",
        "\n",
        "                avg_event_length = np.mean(event_lengths)\n",
        "                avg_event_duration = np.mean(event_durations)\n",
        "\n",
        "                total_num_events = len(events)\n",
        "\n",
        "                # NORMALIZE TOTAL NUM EVENTS BASED ON DF SIZE\n",
        "                # total_num_events = len(events) * 1000 / df.shape[0]\n",
        "\n",
        "                avg_probability = np.mean(probabilities_flattened)\n",
        "                std_dev = np.std(probabilities_flattened)\n",
        "                skewness_val = skew(probabilities_flattened)\n",
        "                kurtosis_val = kurtosis(probabilities_flattened)\n",
        "                autocorr = acf(probabilities_flattened, fft=True, nlags=1)[1] if len(probabilities_flattened) > 1 else 0\n",
        "            else:\n",
        "                avg_event_length = 0\n",
        "                avg_event_duration = 0\n",
        "                total_num_events = 0\n",
        "                avg_probability = 0\n",
        "                std_dev = 0\n",
        "                skewness_val = 0\n",
        "                kurtosis_val = 0\n",
        "                autocorr = 0\n",
        "\n",
        "        # Add results to the DataFrame\n",
        "        if STATS_FEATURE_SETTING == 0:\n",
        "            results_df[emotion] = [avg_event_length, avg_event_duration, total_num_events, avg_probability, std_dev, skewness_val, kurtosis_val, autocorr, pres_pct]\n",
        "        elif STATS_FEATURE_SETTING == 1 or (STATS_FEATURE_SETTING == 3 and method == 'HSE'):\n",
        "            results_df[emotion] = [avg_event_length, total_num_events, avg_probability, std_dev, pres_pct]\n",
        "        elif STATS_FEATURE_SETTING == 2:\n",
        "            results_df[emotion] = [pres_pct]\n",
        "        elif STATS_FEATURE_SETTING == 3 and (method == 'OGAU' or method=='OF'):\n",
        "            results_df[emotion] = [pres_pct, total_num_events]\n",
        "\n",
        "    # Replace NaN values with 0\n",
        "    results_df.fillna(0, inplace=True)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "def average_dfs(dfs):\n",
        "    # Take the average of each X-minute window's features!\n",
        "\n",
        "    if not dfs:\n",
        "        raise ValueError(\"The list of DataFrames is empty\")\n",
        "\n",
        "    # Ensure all DataFrames have the same shape and columns\n",
        "    shape = dfs[0].shape\n",
        "    columns = dfs[0].columns\n",
        "    for df in dfs:\n",
        "        if df.shape != shape or not df.columns.equals(columns):\n",
        "            raise ValueError(\"All DataFrames must have the same shape and columns\")\n",
        "\n",
        "    # Calculate the average DataFrame\n",
        "    avg_df = sum(dfs) / len(dfs)\n",
        "\n",
        "    return avg_df\n",
        "\n",
        "\n",
        "def first_dfs(dfs):\n",
        "    # Take the first X-minute window's features!\n",
        "    return dfs[0]\n"
      ],
      "metadata": {
        "id": "C9aiyRawEoHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPENGRAPHAU AU EVENTS\n",
        "\n",
        "# dictionary to store results from each different time window we test!\n",
        "opengraphau_au_events_stats_dict_list = {}\n",
        "\n",
        "\n",
        "for time_window, opengraphau_radius_now in dfs_opengraphau_times.items():\n",
        "  print('Time Window: ', time_window)\n",
        "  THRESHOLD=0.4\n",
        "  opengraphau_radius_binarized = apply_function_to_dict_list(opengraphau_radius_now, binarize_cols, threshold=THRESHOLD)\n",
        "  opengraphau_au_events_stats = apply_function_to_dict_list(opengraphau_radius_binarized, analyze_emotion_events_v2, max_frame_gap=10, event_minimum_num_frames=12, method='OGAU')\n",
        "  opengraphau_au_events_stats_fixed = fill_empty_dfs_lists(opengraphau_au_events_stats)\n",
        "  opengraphau_au_events_stats_dict_list[time_window] = opengraphau_au_events_stats_fixed\n"
      ],
      "metadata": {
        "id": "TIEKiJEWEoKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenGraphAU - Averaging across time windows!\n",
        "\n",
        "opengraphau_au_events_stats_dict = {}\n",
        "\n",
        "\n",
        "for time_window, opengraphau_radius_now in opengraphau_au_events_stats_dict_list.items():\n",
        "  print('Time Window: ', time_window)\n",
        "\n",
        "  opengraphau_au_events_stats_fixed = apply_function_to_dict(opengraphau_radius_now, average_dfs)\n",
        "  opengraphau_au_events_stats_dict[time_window] = opengraphau_au_events_stats_fixed\n"
      ],
      "metadata": {
        "id": "zLBoCWOtSwnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenGraphAU - First time window!\n",
        "\n",
        "opengraphau_au_events_stats_dict = {}\n",
        "\n",
        "\n",
        "for time_window, opengraphau_radius_now in opengraphau_au_events_stats_dict_list.items():\n",
        "  print('Time Window: ', time_window)\n",
        "\n",
        "  opengraphau_au_events_stats_fixed = apply_function_to_dict(opengraphau_radius_now, first_dfs)\n",
        "  opengraphau_au_events_stats_dict[time_window] = opengraphau_au_events_stats_fixed\n"
      ],
      "metadata": {
        "id": "wWC5ieF2U1_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary to store results from each different time window we test!\n",
        "hsemotion_emo_stats_dict_list = {}\n",
        "\n",
        "for time_window, hsemotion_radius_now in dfs_hsemotion_times.items():\n",
        "  print('Time Window: ', time_window)\n",
        "  THRESHOLD = 0.4\n",
        "  hsemotion_radius_binarized = apply_function_to_dict_list(hsemotion_radius_now, binarize_cols, threshold=THRESHOLD)\n",
        "  hsemotion_emo_stats = apply_function_to_dict_list(hsemotion_radius_binarized, analyze_emotion_events_v2, max_frame_gap=10, event_minimum_num_frames=12, method='HSE')\n",
        "  hsemotion_emo_stats_fixed = fill_empty_dfs_lists(hsemotion_emo_stats)\n",
        "  hsemotion_emo_stats_dict_list[time_window] = hsemotion_emo_stats_fixed"
      ],
      "metadata": {
        "id": "onUWQqySEoM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HSEmotion - Averaging across time windows!\n",
        "\n",
        "hsemotion_emo_stats_dict = {}\n",
        "\n",
        "\n",
        "for time_window, hsemotion_radius_now in hsemotion_emo_stats_dict_list.items():\n",
        "  print('Time Window: ', time_window)\n",
        "\n",
        "  hsemotion_emo_stats_fixed = apply_function_to_dict(hsemotion_radius_now, average_dfs)\n",
        "  hsemotion_emo_stats_dict[time_window] = hsemotion_emo_stats_fixed\n"
      ],
      "metadata": {
        "id": "mjGblSiAT50o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HSEmotion - First time window!\n",
        "\n",
        "hsemotion_emo_stats_dict = {}\n",
        "\n",
        "\n",
        "for time_window, hsemotion_radius_now in hsemotion_emo_stats_dict_list.items():\n",
        "  print('Time Window: ', time_window)\n",
        "\n",
        "  hsemotion_emo_stats_fixed = apply_function_to_dict(hsemotion_radius_now, first_dfs)\n",
        "  hsemotion_emo_stats_dict[time_window] = hsemotion_emo_stats_fixed\n"
      ],
      "metadata": {
        "id": "mRYBhL5_U-QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkcRVBK7CZaJ"
      },
      "source": [
        "# Make Vectors for Each Timestamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFkY8uARYsGL"
      },
      "source": [
        "## Vectors for AU and emotion classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1afeC95tCzgK"
      },
      "outputs": [],
      "source": [
        "## Dictionary of list of relevant dictionaries\n",
        "openface_dict_list_dict = {}\n",
        "\n",
        "for key in openface_au_derived_dict.keys():\n",
        "  openface_dict_list_dict[key] = [openface_au_derived_dict[key], openface_emo_stats_dict[key], openface_ee_derived_dict[key], openface_oe_derived_dict[key]]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opengraphau_dict_list_dict = {}\n",
        "\n",
        "for key in opengraphau_au_events_stats_dict.keys():\n",
        "  #opengraphau_dict_list_dict[key] = [opengraphau_au_derived_dict[key], opengraphau_ee_derived_dict[key]]\n",
        "  opengraphau_dict_list_dict[key] = [opengraphau_au_events_stats_dict[key]]\n"
      ],
      "metadata": {
        "id": "xa9VoJpoIz1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hsemotion_dict_list_dict = {}\n",
        "\n",
        "for key in hsemotion_emo_stats_dict.keys():\n",
        "  hsemotion_dict_list_dict[key] = [hsemotion_emo_stats_dict[key]]\n",
        "  #hsemotion_dict_list_dict[key] = [hsemotion_emo_stats_dict[key], hsemotion_ee_derived_dict[key]]"
      ],
      "metadata": {
        "id": "eG7PC1CcGoRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def partial_combine_dictionaries(dict1, dict2):\n",
        "    # Takes element one (i.e. the AU matrix) from dict1, and all of dict2 (i.e. HSEmotion)\n",
        "    combined_dict = {}\n",
        "\n",
        "    for key in dict1:\n",
        "        combined_dict[key] = [dict1[key][0]] + dict2[key]\n",
        "\n",
        "    return combined_dict\n",
        "\n",
        "def full_combine_dictionaries(dict_list):\n",
        "    combined_dict = {}\n",
        "\n",
        "    for key in dict_list[0]:\n",
        "        combined_dict[key] = []\n",
        "        for j in dict_list:\n",
        "          combined_dict[key] = combined_dict[key] + j[key]\n",
        "\n",
        "    return combined_dict"
      ],
      "metadata": {
        "id": "w4DmMSlLa_GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ofauhsemotion_dict_list_dict = partial_combine_dictionaries(openface_dict_list_dict, hsemotion_dict_list_dict)\n",
        "\n",
        "ogauhsemotion_dict_list_dict = partial_combine_dictionaries(opengraphau_dict_list_dict, hsemotion_dict_list_dict)\n"
      ],
      "metadata": {
        "id": "1GNy0xOKbEBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_dict_list_dict = full_combine_dictionaries([openface_dict_list_dict, opengraphau_dict_list_dict, hsemotion_dict_list_dict])"
      ],
      "metadata": {
        "id": "k53vTvAQcMAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE VARIABLES - EMOTION & AFFECT\n",
        "\n",
        "#save_var(openface_dict_list_dict, forced_name=f'openface_dict_list_dict_outpatient_{PAT_NOW}')\n",
        "\n",
        "save_var(opengraphau_dict_list_dict, forced_name=f'opengraphau_dict_list_dict_outpatient_{PAT_NOW}')\n",
        "\n",
        "save_var(hsemotion_dict_list_dict, forced_name=f'hsemotion_dict_list_dict_outpatient_{PAT_NOW}')"
      ],
      "metadata": {
        "id": "ZRuy_1Y9JAqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE VARIABLES - EMOTION & AFFECT\n",
        "\n",
        "save_var(ofauhsemotion_dict_list_dict, forced_name=f'ofauhsemotion_dict_list_dict_outpatient_{PAT_NOW}')\n",
        "\n",
        "save_var(ogauhsemotion_dict_list_dict, forced_name=f'ogauhsemotion_dict_list_dict_outpatient_{PAT_NOW}')\n",
        "\n",
        "save_var(all_dict_list_dict, forced_name=f'all_dict_list_dict_outpatient_{PAT_NOW}')"
      ],
      "metadata": {
        "id": "gASR49RVbmwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmHYCC9OBxzq"
      },
      "outputs": [],
      "source": [
        "# LOAD VARIABLES - EMOTION & AFFECT\n",
        "\n",
        "#openface_dict_list_dict = load_var(f'openface_dict_list_dict_outpatient_{PAT_NOW}')\n",
        "\n",
        "opengraphau_dict_list_dict = load_var(f'opengraphau_dict_list_dict_outpatient_{PAT_NOW}')\n",
        "\n",
        "hsemotion_dict_list_dict = load_var(f'hsemotion_dict_list_dict_outpatient_{PAT_NOW}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD VARIABLES - EMOTION & AFFECT\n",
        "\n",
        "#ofauhsemotion_dict_list_dict = load_var(f'ofauhsemotion_dict_list_dict_outpatient_{PAT_NOW}')\n",
        "\n",
        "ogauhsemotion_dict_list_dict = load_var(f'ogauhsemotion_dict_list_dict_outpatient_{PAT_NOW}')\n",
        "\n",
        "#all_dict_list_dict = load_var(f'all_dict_list_dict_outpatient_{PAT_NOW}')"
      ],
      "metadata": {
        "id": "enGGAIcNbqmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcZea69MJ9Lw"
      },
      "outputs": [],
      "source": [
        "def flatten_dataframes_dict(dataframes_list):\n",
        "    # Initialize an empty dictionary to store the flattened data for each key\n",
        "    flattened_data_dict = {}\n",
        "\n",
        "    # Define the columns to ignore\n",
        "    ignore_columns = ['success', 'frame', 'timestamp', 'AU', 'emotion']\n",
        "\n",
        "    for dataframes_dict in dataframes_list:\n",
        "       for key, df in dataframes_dict.items():\n",
        "          # Filter out the columns to be ignored\n",
        "          filtered_df = df.drop(columns=[col for col in ignore_columns if col in df.columns])\n",
        "\n",
        "          # Flatten the data by converting each DataFrame into a 1D array\n",
        "          flattened_array = filtered_df.select_dtypes(include=[np.number, int, float, complex, \\\n",
        "                                                                pd.Int64Dtype(), pd.Float64Dtype(), pd.Int32Dtype(), \\\n",
        "                                                                pd.Float32Dtype()]).values.flatten()\n",
        "\n",
        "          # Convert the flattened array to NumPy array and store it in the dictionary\n",
        "          if key in flattened_data_dict:\n",
        "              flattened_data_dict[key] = np.concatenate((flattened_data_dict[key], flattened_array))\n",
        "          else:\n",
        "              flattened_data_dict[key] = np.array(flattened_array)\n",
        "\n",
        "    return flattened_data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_kZiz0dKDcm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "openface_vectors_dict = {}\n",
        "\n",
        "for key, openface_dict_list_now in openface_dict_list_dict.items():\n",
        "  openface_vectors_dict[key] = flatten_dataframes_dict(openface_dict_list_now)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opengraphau_vectors_dict = {}\n",
        "\n",
        "for key, opengraphau_dict_list_now in opengraphau_dict_list_dict.items():\n",
        "  opengraphau_vectors_dict[key] = flatten_dataframes_dict(opengraphau_dict_list_now)\n"
      ],
      "metadata": {
        "id": "L2P1ntu5JF9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hsemotion_vectors_dict = {}\n",
        "\n",
        "for key, hsemotion_dict_list_now in hsemotion_dict_list_dict.items():\n",
        "  hsemotion_vectors_dict[key] = flatten_dataframes_dict(hsemotion_dict_list_now)\n"
      ],
      "metadata": {
        "id": "IEBJtszEJGPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ofauhsemotion_vectors_dict = {}\n",
        "\n",
        "for key, ofauhsemotion_dict_list_now in ofauhsemotion_dict_list_dict.items():\n",
        "  ofauhsemotion_vectors_dict[key] = flatten_dataframes_dict(ofauhsemotion_dict_list_now)\n"
      ],
      "metadata": {
        "id": "YJQe4v4abTIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ogauhsemotion_vectors_dict = {}\n",
        "\n",
        "for key, ogauhsemotion_dict_list_now in ogauhsemotion_dict_list_dict.items():\n",
        "  ogauhsemotion_vectors_dict[key] = flatten_dataframes_dict(ogauhsemotion_dict_list_now)\n"
      ],
      "metadata": {
        "id": "3QhobZ1wbTL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_vectors_dict = {}\n",
        "\n",
        "for key, all_dict_list_now in all_dict_list_dict.items():\n",
        "  all_vectors_dict[key] = flatten_dataframes_dict(all_dict_list_now)\n"
      ],
      "metadata": {
        "id": "dn_OpS2jcZnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx6_bCNQV6uD"
      },
      "source": [
        "## Labels - datetime conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hRicOaOQVtq"
      },
      "outputs": [],
      "source": [
        "def ts_to_str(timestamp):\n",
        "    return timestamp.strftime('%-m/%-d/%Y %H:%M:%S')\n",
        "\n",
        "def str_to_ts(string_now):\n",
        "  temp_var = pd.to_datetime(pd.to_datetime(string_now).strftime('%d-%b-%Y %H:%M:%S'))\n",
        "  return pd.Timestamp(temp_var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lwF8VpTv-oC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpvfE0TfXduU"
      },
      "outputs": [],
      "source": [
        "ts_to_str(list(openface_vectors_dict['60'].keys())[18])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJvxb39jc9Pg"
      },
      "outputs": [],
      "source": [
        "str_to_ts('4/6/2023 20:30:00')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLtMi6TUvm9K"
      },
      "source": [
        "## Save to excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD95XHYFxLl9"
      },
      "outputs": [],
      "source": [
        "!pip install xlsxwriter -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntLtIcQow-03"
      },
      "outputs": [],
      "source": [
        "## Save our vectors to excel sheets!\n",
        "\n",
        "def get_dict_name(dictionary):\n",
        "    namespace = globals()\n",
        "    for name, obj in namespace.items():\n",
        "        if isinstance(obj, dict) and obj is dictionary:\n",
        "            return name\n",
        "    return None\n",
        "\n",
        "def save_dicts_to_excel(dict_list, output_path):\n",
        "  # Create an Excel writer object\n",
        "  writer = pd.ExcelWriter(output_path, engine='xlsxwriter')\n",
        "\n",
        "  # Iterate over the keys in the dictionaries\n",
        "  for key in dict_list[0].keys():\n",
        "      # Write each dataframe to a separate sheet with the corresponding key as the sheet name\n",
        "      for enum, dict_now in enumerate(dict_list):\n",
        "        name_var = f'Matrix_{enum}'\n",
        "        sheet_name_starter = f'{key}_{name_var}'\n",
        "        dict_now[key].to_excel(writer, sheet_name=sheet_name_starter[:31])\n",
        "\n",
        "  # Save the Excel file\n",
        "  writer.save()\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-hour features"
      ],
      "metadata": {
        "id": "HtF5RndQv9qj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-zpJEZSyU2S"
      },
      "outputs": [],
      "source": [
        "\n",
        "os.makedirs(FEATURE_VIS_PATH, exist_ok=True)\n",
        "\n",
        "for i in opengraphau_dict_list_dict.keys():\n",
        "  #save_dicts_to_excel(openface_dict_list_dict[i], FEATURE_VIS_PATH + f'openface_{PAT_NOW}_{int(i) / 60}_hours.xlsx')\n",
        "  save_dicts_to_excel(opengraphau_dict_list_dict[i], FEATURE_VIS_PATH + f'opengraphau_{PAT_NOW}_{int(i) / 60}_hours.xlsx')\n",
        "  save_dicts_to_excel(hsemotion_dict_list_dict[i], FEATURE_VIS_PATH + f'hsemotion_{PAT_NOW}_{int(i) / 60}_hours.xlsx')\n",
        "  save_dicts_to_excel(ogauhsemotion_dict_list_dict[i], FEATURE_VIS_PATH + f'ogauhse_{PAT_NOW}_{int(i) / 60}_hours.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "bYi6EjHkWTTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "8t1JM3aZYVaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(x=5):\n",
        "  np.random.seed(x)\n",
        "  random.seed(x)\n",
        "\n",
        "\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "r4_0pdm5WRw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from scipy.stats import pearsonr\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "\n",
        "def linRegOneMetric(vectors_dict, y, randShuffle=False, do_lasso=False, do_ridge=False, alpha=1.0):\n",
        "  # runs simple linear regression via one-left-out\n",
        "  # vectors_dict -- dictionary mapping time radius (in minutes) to features\n",
        "  # y -- a numpy array with labels (self-reported metrics)\n",
        "  # randShuffle -- do we shuffle the self-report labels?\n",
        "  # if do_lasso, does lasso regression\n",
        "  # if do_ridge, does ridge regression. Overrides do_lasso\n",
        "  # alpha - this is the weighting of either lasso or ridge\n",
        "\n",
        "  # returns a dictionary with several results:\n",
        "  # scores -- dictionary mapping each time radius to list of MSEs from each one-left-out\n",
        "  # preds -- dictionary mapping each time radius to a list of each one-left-out model's prediction\n",
        "  # y -- returns y again for convenience\n",
        "  # models -- dictionary mapping each time radius to a list of each one-left-out trained model (simple linear regression)\n",
        "\n",
        "  scores = {}\n",
        "  preds = {}\n",
        "  models = {}\n",
        "\n",
        "  if randShuffle:\n",
        "    y_using = np.random.permutation(y)\n",
        "  else:\n",
        "    y_using = y\n",
        "\n",
        "  for i in vectors_dict.keys():\n",
        "    model = LinearRegression()\n",
        "    if do_lasso:\n",
        "      model = Lasso(alpha=alpha)\n",
        "    if do_ridge:\n",
        "      model = Ridge(alpha=alpha)\n",
        "\n",
        "    # Compute MSEs via scikitlearn cross_val_score\n",
        "    scores_temp = cross_val_score(model, vectors_dict[i], y_using, cv=vectors_dict[i].shape[0], scoring='neg_mean_squared_error')\n",
        "    scores[i] = -1 * scores_temp\n",
        "\n",
        "    # Predictions via cross_val_predict\n",
        "    preds[i] = cross_val_predict(model, vectors_dict[i], y_using, cv=vectors_dict[i].shape[0])\n",
        "\n",
        "    # Now we need to iterate through and actually save the models themselves, since cross_val_score doesn't let us do that!\n",
        "    models_i_building = []\n",
        "    for test_index in range(vectors_dict[i].shape[0]):\n",
        "\n",
        "      X_train = np.delete(vectors_dict[i], test_index, axis=0)\n",
        "\n",
        "      y_train = np.delete(y_using, test_index, axis=0)\n",
        "\n",
        "      model = LinearRegression()\n",
        "      if do_lasso:\n",
        "        model = Lasso(alpha=alpha)\n",
        "      if do_ridge:\n",
        "        model = Ridge(alpha=alpha)\n",
        "      model.fit(X_train, y_train)\n",
        "      models_i_building.append(model)\n",
        "\n",
        "    models[i] = models_i_building\n",
        "\n",
        "  return scores, preds, y, models\n",
        "\n",
        "\n",
        "\n",
        "def plot_predictions(y, y_pred, randShuffleR=None, ax=None, time_rad=None, metric=None):\n",
        "    # Makes one scatterplot with Pearson's R and p value on it\n",
        "    # give it the randShuffle Pearson's R\n",
        "    # if you want to display that on the plot\n",
        "\n",
        "    # Compute Pearson's R\n",
        "    pearson_corr, p_val = pearsonr(y, y_pred)\n",
        "\n",
        "    # Create the scatter plot on the specified axes\n",
        "    if ax is None:\n",
        "        ax_original = None\n",
        "        fig, ax = plt.subplots()\n",
        "        # adjust fonts!\n",
        "        text_font = 16\n",
        "    else:\n",
        "        ax_original = ax\n",
        "        text_font = 16\n",
        "\n",
        "\n",
        "    ax.scatter(y, y_pred, label='Predicted vs. True', s=24)\n",
        "\n",
        "\n",
        "\n",
        "    # Add the correlation coefficient and p-value on the plot\n",
        "    ax.text(0.05, 0.90, f'Pearson\\'s R: {pearson_corr:.2f}', transform=ax.transAxes, fontsize=text_font)\n",
        "    ax.text(0.05, 0.80, f'P Value: {p_val:.2f}', transform=ax.transAxes, fontsize=text_font)\n",
        "    if not(randShuffleR is None):\n",
        "      ax.text(0.05, 0.70, f'Random Shuffle R: {randShuffleR:.2f}', transform=ax.transAxes, fontsize=text_font)\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('Self-Reported Scores', fontsize=17)\n",
        "    ax.set_ylabel('Predicted Scores', fontsize=17)\n",
        "\n",
        "    if metric is None:\n",
        "      title_starter = 'Predicted vs. True'\n",
        "    else:\n",
        "      title_starter = metric\n",
        "\n",
        "    if time_rad is None:\n",
        "      ax.set_title(f'{title_starter} Scores', fontsize=17)\n",
        "    else:\n",
        "      num_mins = int(time_rad)\n",
        "      if num_mins > 1:\n",
        "        ax.set_title(f'{title_starter}, Time Window = {num_mins} Minutes', fontsize=15)\n",
        "      else:\n",
        "        ax.set_title(f'{title_starter}, Time Window = {num_mins} Minute', fontsize=15)\n",
        "\n",
        "\n",
        "    # Add the line of best fit\n",
        "    sns.regplot(x=y, y=y_pred, ax=ax, line_kws={'color': 'red', 'linestyle': '--'}, label='Line of Best Fit')\n",
        "\n",
        "    # Add the shaded region for the 95% confidence interval\n",
        "    #sns.regplot(x=y, y=y_pred, ax=ax, scatter=False, ci=95, color='gray', label='95% Confidence Interval')\n",
        "\n",
        "    # Adjust the font size of the tick labels on the axes\n",
        "    ax.tick_params(axis='both', labelsize=18)\n",
        "\n",
        "    ax.set_adjustable('box')\n",
        "\n",
        "    #set aspect ratio to 1\n",
        "    ratio = 1.0\n",
        "    x_left, x_right = ax.get_xlim()\n",
        "    y_low, y_high = ax.get_ylim()\n",
        "    ax.set_aspect(abs((x_right-x_left)/(y_low-y_high))*ratio)\n",
        "\n",
        "    if ax_original is None:\n",
        "        plt.show()\n",
        "        return pearson_corr, p_val, fig\n",
        "    else:\n",
        "        return pearson_corr, p_val\n",
        "\n",
        "\n",
        "\n",
        "def plot_scatterplots(preds_dict, y, overall_title, savepath, randShuffleR=None):\n",
        "\n",
        "    plt.rcParams['lines.markersize'] = 6\n",
        "    subplot_title_font = 16\n",
        "    full_title_font = 24\n",
        "\n",
        "    num_plots = len(list(preds_dict.keys()))\n",
        "    num_cols = 4\n",
        "    num_rows = (num_plots + num_cols - 1) // num_cols\n",
        "\n",
        "    r_list = []\n",
        "    p_list = []\n",
        "\n",
        "    # Calculate the desired figure size for larger plot\n",
        "    figsize = (28, 12)\n",
        "\n",
        "    # Create subplots with auto aspect ratio\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "\n",
        "    #axes.set_adjustable('box')\n",
        "\n",
        "    if num_rows == 1:\n",
        "      axes = axes.reshape((1, num_cols))\n",
        "\n",
        "    # Flatten the axes array if necessary\n",
        "    if num_plots == 1:\n",
        "        fig, axes = plt.subplots(1, 1, figsize=figsize)\n",
        "        axes = np.array([axes]).reshape(1, 1)\n",
        "\n",
        "\n",
        "    # Loop through the dictionaries\n",
        "    for i, (key, y_preds) in enumerate(preds_dict.items()):\n",
        "        y_list = np.array(y).astype(float)\n",
        "        y_pred = np.array(y_preds).astype(float)\n",
        "        #y_pred = np.array([i[0] for i in y_pred])\n",
        "\n",
        "        # Get the subplot coordinates\n",
        "        row = i // num_cols\n",
        "        col = i % num_cols\n",
        "\n",
        "        # Plot predictions on the subplot\n",
        "        if randShuffleR is None:\n",
        "          pearson_corr, p_val = plot_predictions(y_list, y_pred, randShuffleR=randShuffleR, ax=axes[row, col])\n",
        "        else:\n",
        "          pearson_corr, p_val = plot_predictions(y_list, y_pred, randShuffleR=randShuffleR[i], ax=axes[row, col])\n",
        "        r_list.append(pearson_corr)\n",
        "        p_list.append(p_val)\n",
        "\n",
        "        num_mins = int(key)\n",
        "        if num_mins > 1:\n",
        "          axes[row, col].set_title(f'Time Window = {num_mins} Minutes', fontsize=subplot_title_font)\n",
        "        else:\n",
        "          axes[row, col].set_title(f'Time Window = {num_mins} Minute', fontsize=subplot_title_font)\n",
        "        #axes[row, col].set_aspect('equal')\n",
        "\n",
        "        # Remove x-axis and y-axis labels from subplots\n",
        "        axes[row, col].set_xlabel('')\n",
        "        axes[row, col].set_ylabel('')\n",
        "\n",
        "        #axes[row, col].set_adjustable('box')\n",
        "\n",
        "    # Add overall title\n",
        "    fig.suptitle(overall_title, fontsize=30, y=1)\n",
        "\n",
        "    # Set shared x-axis and y-axis labels\n",
        "    fig.text(0.5, 0.00, 'Self-Reported Scores', ha='center', fontsize=full_title_font)\n",
        "    fig.text(-0.01, 0.5, 'Predicted Scores', va='center', rotation='vertical', fontsize=full_title_font)\n",
        "\n",
        "    # Adjust spacing and layout\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.savefig(savepath, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return r_list, p_list, fig\n",
        "\n",
        "\n",
        "def make_mse_boxplot(scores, metric, savepath, ax=None, method_now='OpenFace'):\n",
        "    # scores -- dictionary that maps time radius (mins) to list of MSEs from one-left-out\n",
        "    # metric - e.g. Mood or Anxiety\n",
        "\n",
        "    # Combine the data into a single array\n",
        "    data = [MSE_list for MSE_list in list(scores.values())]\n",
        "\n",
        "    # Set the font sizes\n",
        "    plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    else:\n",
        "        fig = None\n",
        "\n",
        "    # Create a box plot of the data\n",
        "    labels_now = [f'{int(key) / 60}' for key in scores.keys()]\n",
        "\n",
        "    ax.boxplot(data, labels=labels_now, showmeans=True, meanprops={'marker': 'o', 'markerfacecolor': 'red', 'markersize': 10})\n",
        "\n",
        "    # Determine the highest 75th percentile value among the four entries\n",
        "    max_value = np.max([np.percentile(entry, 75) for entry in data])\n",
        "\n",
        "    # Set the y-axis range conditionally\n",
        "    if max_value > 100:\n",
        "        ax.set_ylim(0, 100)\n",
        "    else:\n",
        "        ax.set_ylim(0, max_value)\n",
        "\n",
        "    # Set the labels and title\n",
        "    ax.set_xlabel('Time Window (Hours)')\n",
        "    ax.set_ylabel('Mean Squared Error')\n",
        "    ax.set_title(f'{metric} Prediction via {method_now}', y=1.1)\n",
        "\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.savefig(savepath, bbox_inches='tight')\n",
        "\n",
        "    # Show the plot if fig is None\n",
        "    if fig is None:\n",
        "        plt.show()\n",
        "    else:\n",
        "        return fig\n",
        "\n",
        "def make_r_barplot(r_list, time_radius_list, metric, savepath, ax=None, method_now='OpenFace'):\n",
        "    plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "    x_labels = [f'{int(i) / 60}' for i in time_radius_list]\n",
        "\n",
        "    if ax is None:\n",
        "        original_ax = None\n",
        "        fig, ax = plt.subplots()\n",
        "    else:\n",
        "        original_ax = ax\n",
        "\n",
        "    ax.bar(x_labels, r_list)\n",
        "\n",
        "    # Set the y-axis range\n",
        "    ax.set_ylim(-0.5, 1)\n",
        "\n",
        "    # Set the labels and title\n",
        "    ax.set_xlabel('Time Window (Hours)')\n",
        "    ax.set_ylabel(\"Pearson's R\")\n",
        "    ax.set_title(f'{metric} Prediction via {method_now}', y=1.1)\n",
        "\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.savefig(savepath, bbox_inches='tight')\n",
        "\n",
        "    # Show the plot if ax is None\n",
        "    if original_ax is None:\n",
        "        plt.show()\n",
        "        return fig\n",
        "\n",
        "# def get_label_from_index(index, spreadsheet_path=FEATURE_LABEL_PATH+'opengraphau_0.5_hours.xlsx'):\n",
        "#     if 'hsemotion' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\"]\n",
        "#       row_label_cols = [\"emotion\"]\n",
        "#     elif 'opengraphau' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
        "#       row_label_cols = [\"AU\", \"emotion\"]\n",
        "#     elif 'openface' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
        "#       row_label_cols = [\"AU\", \"emotion\"]\n",
        "#     elif 'ofauhse' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
        "#       row_label_cols = [\"AU\", \"emotion\"]\n",
        "#     elif 'ogauhse' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
        "#       row_label_cols = [\"AU\", \"emotion\"]\n",
        "#     elif 'all' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\", \"Matrix_3\", \"Matrix_4\"]\n",
        "#       row_label_cols = [\"AU\", \"emotion\", \"AU\", \"emotion\", \"emotion\"]\n",
        "#     else:\n",
        "#       print('BUG IN THE CODE! CHECK get_label_from_index')\n",
        "#       print('spreadsheet path is ', spreadsheet_path)\n",
        "\n",
        "\n",
        "#     xls = pd.ExcelFile(spreadsheet_path)\n",
        "\n",
        "#     for i, matrix in enumerate(matrices):\n",
        "#         # Find the sheet ending with the current matrix name\n",
        "#         sheet_name = next((s for s in xls.sheet_names if s.endswith(matrix)), None)\n",
        "#         if sheet_name is not None:\n",
        "#             # Load the sheet into a DataFrame, with the first row as column names\n",
        "#             if row_label_cols[i] == 'AU':\n",
        "#               df = pd.read_excel(spreadsheet_path, sheet_name=sheet_name, header=0, usecols=lambda x: x != 'Unnamed: 0')\n",
        "#             else:\n",
        "#               df = pd.read_excel(spreadsheet_path, sheet_name=sheet_name, header=0)\n",
        "\n",
        "#             # Get the column labels from the DataFrame\n",
        "#             col_labels = df.columns.tolist()\n",
        "#             col_labels = col_labels[1:]\n",
        "\n",
        "#             # Get the row labels and column labels based on the matrix type\n",
        "#             row_label_col = row_label_cols[i]\n",
        "#             row_labels = None\n",
        "\n",
        "#             if row_label_col in df.columns:\n",
        "#                 row_labels = df[row_label_col].tolist()\n",
        "#             else:\n",
        "#                 row_labels = df.iloc[:, 0]\n",
        "\n",
        "#             # Get the numerical entries in the sheet excluding columns \"AU\" and \"emotion\"\n",
        "#             numerical_entries = df.loc[:, ~df.columns.isin([\"AU\", \"emotion\"])].values.flatten()\n",
        "#             numerical_entries = numerical_entries[~pd.isnull(numerical_entries)]\n",
        "\n",
        "#             # Check if the index is within the range of numerical entries\n",
        "#             if index < len(numerical_entries):\n",
        "#                 # Find the label corresponding to the index\n",
        "#                 row_index, col_index = divmod(index, len(col_labels))\n",
        "\n",
        "#                 if row_labels is None:\n",
        "#                     return col_labels[col_index]\n",
        "#                 else:\n",
        "#                     if \"AU\" == row_label_cols[i]:\n",
        "#                       return f\"AU{row_labels[row_index]} {col_labels[col_index]}\"\n",
        "#                     else:\n",
        "#                       return f\"{col_labels[col_index]} {row_labels[row_index]}\"\n",
        "#             else:\n",
        "#                 index -= len(numerical_entries)\n",
        "\n",
        "#     # Return None if the index is out of range or no suitable sheets found\n",
        "#     print('BIG PROBLEM in get_label_from_index: no suitable sheets or index out of range')\n",
        "#     return None\n",
        "\n",
        "\n",
        "\n",
        "# def get_label_from_index(index, spreadsheet_path=FEATURE_LABEL_PATH+'openface_0.5_hours.xlsx'):\n",
        "#     if 'hsemotion' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
        "#       row_label_cols = [\"emotion\", \"emotion\"]\n",
        "#     elif 'opengraphau' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
        "#       row_label_cols = [\"AU\", \"emotion\"]\n",
        "#     elif 'openface' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\", \"Matrix_3\"]\n",
        "#       row_label_cols = [\"AU\", \"emotion\", \"emotion\", None]\n",
        "#     elif 'ofauhse' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\"]\n",
        "#       row_label_cols = [\"AU\", \"emotion\", \"emotion\"]\n",
        "#     elif 'ogauhse' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\"]\n",
        "#       row_label_cols = [\"AU\", \"emotion\", \"emotion\"]\n",
        "#     elif 'all' in spreadsheet_path:\n",
        "#       matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\", \"Matrix_3\", \"Matrix_4\", \"Matrix_5\", \"Matrix_6\", \"Matrix_7\"]\n",
        "#       row_label_cols = [\"AU\", \"emotion\", \"emotion\", None, \"AU\", \"emotion\", \"emotion\", \"emotion\"]\n",
        "#     else:\n",
        "#       print('BUG IN THE CODE! CHECK get_label_from_index')\n",
        "#       print('spreadsheet path is ', spreadsheet_path)\n",
        "\n",
        "\n",
        "#     xls = pd.ExcelFile(spreadsheet_path)\n",
        "\n",
        "#     for i, matrix in enumerate(matrices):\n",
        "#         # Find the sheet ending with the current matrix name\n",
        "#         sheet_name = next((s for s in xls.sheet_names if s.endswith(matrix)), None)\n",
        "#         if sheet_name is not None:\n",
        "#             # Load the sheet into a DataFrame, with the first row as column names\n",
        "#             df = pd.read_excel(spreadsheet_path, sheet_name=sheet_name, header=0, usecols=lambda x: x != 'Unnamed: 0')\n",
        "\n",
        "#             # Get the column labels from the DataFrame\n",
        "#             col_labels = df.columns.tolist()\n",
        "#             if not(row_label_cols[i] is None):\n",
        "#               col_labels = col_labels[1:]\n",
        "\n",
        "#             # Check if it's Matrix_3 (single row)\n",
        "#             if i == 3 and (row_label_cols[i] is None):\n",
        "#                 if index < len(col_labels):\n",
        "#                     return col_labels[index]\n",
        "\n",
        "#             # Get the row labels and column labels based on the matrix type\n",
        "#             row_label_col = row_label_cols[i]\n",
        "#             row_labels = None\n",
        "\n",
        "#             if row_label_col is not None and row_label_col in df.columns:\n",
        "#                 row_labels = df[row_label_col].tolist()\n",
        "\n",
        "#             # Get the numerical entries in the sheet excluding columns \"AU\" and \"emotion\"\n",
        "#             numerical_entries = df.loc[:, ~df.columns.isin([\"AU\", \"emotion\"])].values.flatten()\n",
        "#             numerical_entries = numerical_entries[~pd.isnull(numerical_entries)]\n",
        "\n",
        "#             # Check if the index is within the range of numerical entries\n",
        "#             if index < len(numerical_entries):\n",
        "#                 # Find the label corresponding to the index\n",
        "#                 row_index, col_index = divmod(index, len(col_labels))\n",
        "\n",
        "#                 if row_labels is None:\n",
        "#                     return col_labels[col_index]\n",
        "#                 else:\n",
        "#                     if \"AU\" == row_label_cols[i]:\n",
        "#                       return f\"AU{row_labels[row_index]} {col_labels[col_index]}\"\n",
        "#                     else:\n",
        "#                       return f\"{row_labels[row_index]} {col_labels[col_index]}\"\n",
        "#             else:\n",
        "#                 index -= len(numerical_entries)\n",
        "\n",
        "#     # Return None if the index is out of range or no suitable sheets found\n",
        "#     print('BIG PROBLEM')\n",
        "#     return None\n",
        "\n",
        "\n",
        "def get_label_from_index(index, spreadsheet_path=FEATURE_LABEL_PATH+'openface_0.5_hours.xlsx'):\n",
        "    if 'hsemotion' in spreadsheet_path:\n",
        "      matrices = [\"Matrix_0\"]\n",
        "      row_label_cols = [\"emotion\"]\n",
        "    elif 'opengraphau' in spreadsheet_path:\n",
        "      matrices = [\"Matrix_0\"]\n",
        "      row_label_cols = [\"AU\"]\n",
        "    elif 'openface' in spreadsheet_path:\n",
        "      matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\", \"Matrix_3\"]\n",
        "      row_label_cols = [\"AU\", \"emotion\", \"emotion\", None]\n",
        "    elif 'ofauhse' in spreadsheet_path:\n",
        "      matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
        "      row_label_cols = [\"AU\", \"emotion\"]\n",
        "    elif 'ogauhse' in spreadsheet_path:\n",
        "      matrices = [\"Matrix_0\", \"Matrix_1\"]\n",
        "      row_label_cols = [\"AU\", \"emotion\"]\n",
        "    elif 'all' in spreadsheet_path:\n",
        "      matrices = [\"Matrix_0\", \"Matrix_1\", \"Matrix_2\", \"Matrix_3\", \"Matrix_4\", \"Matrix_5\"]\n",
        "      row_label_cols = [\"AU\", \"emotion\", \"emotion\", None, \"AU\", \"emotion\"]\n",
        "    else:\n",
        "      print('BUG IN THE CODE! CHECK get_label_from_index')\n",
        "      print('spreadsheet path is ', spreadsheet_path)\n",
        "\n",
        "\n",
        "    xls = pd.ExcelFile(spreadsheet_path)\n",
        "\n",
        "    for i, matrix in enumerate(matrices):\n",
        "        # Find the sheet ending with the current matrix name\n",
        "        sheet_name = next((s for s in xls.sheet_names if s.endswith(matrix)), None)\n",
        "        if sheet_name is not None:\n",
        "            # Load the sheet into a DataFrame, with the first row as column names\n",
        "            df = pd.read_excel(spreadsheet_path, sheet_name=sheet_name, header=0)\n",
        "\n",
        "            # Get the column labels from the DataFrame\n",
        "            col_labels = df.columns.tolist()[1:]\n",
        "\n",
        "            row_labels = df['Unnamed: 0'].tolist()\n",
        "\n",
        "            # Get the numerical entries in the sheet excluding columns \"AU\" and \"emotion\" and \"Unnamed: 0\"\n",
        "            numerical_entries = df.loc[:, ~df.columns.isin([\"AU\", \"emotion\", \"Unnamed: 0\"])].values.flatten()\n",
        "            numerical_entries = numerical_entries[~pd.isnull(numerical_entries)]\n",
        "\n",
        "            # Check if the index is within the range of numerical entries\n",
        "            if index < len(numerical_entries):\n",
        "                # Find the label corresponding to the index\n",
        "                row_index, col_index = divmod(index, len(col_labels))\n",
        "\n",
        "                return f\"{col_labels[col_index]} {row_labels[row_index]}\"\n",
        "\n",
        "            else:\n",
        "                index -= len(numerical_entries)\n",
        "\n",
        "    # Return None if the index is out of range or no suitable sheets found\n",
        "    print('BUG IN THE CODE! INDEX TOO LARGE! CHECK get_label_from_index')\n",
        "    print('spreadsheet path is ', spreadsheet_path)\n",
        "    return None\n",
        "\n",
        "def getTopFeaturesfromWeights(model_list, spreadsheet_path=FEATURE_LABEL_PATH+'openface_2.0_hours.xlsx'):\n",
        "  # given a list of linear regression models,\n",
        "  # returns their top 5 features (on average) from just weights!\n",
        "\n",
        "  coef_array = [model_now.coef_ for model_now in model_list]\n",
        "  coef_avg = np.mean(coef_array, axis=0)\n",
        "\n",
        "  top_5_features = np.argsort(np.abs(coef_avg))[::-1][:5]\n",
        "\n",
        "  top_5_english = [get_label_from_index(feat_ind, spreadsheet_path=spreadsheet_path) for feat_ind in top_5_features]\n",
        "\n",
        "  return top_5_english\n",
        "\n",
        "\n",
        "def featureAblate(vectors_array, y, do_lasso=False, do_ridge=False):\n",
        "  # runs one-left-out linear regression,\n",
        "  # deleting one feature at a time to determine most important features\n",
        "\n",
        "  # vectors_array -- numpy array of feature vectors\n",
        "  # y -- self-reported labels (e.g. for Mood, Anxiety, or something else)\n",
        "  # if do_lasso, does lasso regression\n",
        "  # if do_ridge, does ridge regression. Overrides do_lasso\n",
        "\n",
        "  # returns scores, prs\n",
        "  # scores -- (n_features, n_timestamps) numpy array of MSEs\n",
        "  # prs -- (n_features,) numpy vector of pearson's R\n",
        "\n",
        "  num_features = vectors_array.shape[1]\n",
        "  num_timestamps = vectors_array.shape[0]\n",
        "\n",
        "  scores = np.zeros((num_features, num_timestamps))\n",
        "  prs = np.zeros((num_features,))\n",
        "\n",
        "  # loop through each feature (for openface, 0 through 144) and delete just that\n",
        "  for deleteNow in range(num_features):\n",
        "    data = np.delete(vectors_array, deleteNow, axis=1)\n",
        "\n",
        "    # make into dictionary to feed into our lin reg function\n",
        "    data = {'placeholder': data}\n",
        "\n",
        "    scores_temp, preds, y, _ = linRegOneMetric(data, y, do_lasso=do_lasso, do_ridge=do_ridge)\n",
        "    scores_temp = scores_temp['placeholder']\n",
        "    preds = preds['placeholder']\n",
        "\n",
        "    # save MSEs\n",
        "    scores[deleteNow, :] =  scores_temp\n",
        "\n",
        "    # compute and save Pearson's R\n",
        "    pearson_corr, _ = pearsonr(y, preds)\n",
        "    prs[deleteNow] = pearson_corr\n",
        "\n",
        "  return scores, prs\n",
        "\n",
        "def featureAblate2D(vectors_array, y):\n",
        "  # runs one-left-out linear regression,\n",
        "  # deleting TWO features at a time to determine most important features\n",
        "\n",
        "  # vectors_array -- numpy array of feature vectors\n",
        "  # y -- self-reported labels (e.g. for Mood, Anxiety, or something else)\n",
        "\n",
        "  # returns prs\n",
        "  # prs -- (n_features, n_features) numpy vector of pearson's R\n",
        "  # Note: ALWAYS index into prs with first index LOWER than second!\n",
        "\n",
        "  num_features = vectors_array.shape[1]\n",
        "\n",
        "  prs = np.zeros((num_features,num_features))\n",
        "\n",
        "  # loop through each feature (for openface, 0 through 144) and delete just that\n",
        "  for deleteNow in range(num_features):\n",
        "    # delete a second one!\n",
        "    for secondDelete in range(deleteNow+1, num_features):\n",
        "      data = np.delete(vectors_array, [deleteNow, secondDelete], axis=1)\n",
        "\n",
        "      # make into dictionary to feed into our lin reg function\n",
        "      data = {'placeholder': data}\n",
        "\n",
        "      _, preds, _, _ = linRegOneMetric(data, y)\n",
        "      preds = preds['placeholder']\n",
        "\n",
        "      # compute and save Pearson's R\n",
        "      pearson_corr, _ = pearsonr(y, preds)\n",
        "      prs[deleteNow, secondDelete] = pearson_corr\n",
        "\n",
        "  return prs\n",
        "\n",
        "def featureAblate3D(vectors_array, y):\n",
        "  # runs one-left-out linear regression,\n",
        "  # deleting THREE features at a time to determine most important features\n",
        "\n",
        "  # vectors_array -- numpy array of feature vectors\n",
        "  # y -- self-reported labels (e.g. for Mood, Anxiety, or something else)\n",
        "\n",
        "  # returns prs\n",
        "  # prs -- (n_features, n_features, n_features) numpy vector of pearson's R\n",
        "  # Note: ALWAYS index into prs with earlier indices LOWER than subsequent ones.\n",
        "\n",
        "  num_features = vectors_array.shape[1]\n",
        "\n",
        "  prs = np.zeros((num_features, num_features, num_features))\n",
        "\n",
        "  # loop through each feature (for openface, 0 through 144) and delete just that\n",
        "  for deleteNow in range(num_features):\n",
        "    # delete a second one!\n",
        "    for secondDelete in range(deleteNow+1, num_features):\n",
        "      # delete a third one!\n",
        "      for thirdDelete in range(secondDelete+1, num_features):\n",
        "        data = np.delete(vectors_array, [deleteNow, secondDelete, thirdDelete], axis=1)\n",
        "\n",
        "        # make into dictionary to feed into our lin reg function\n",
        "        data = {'placeholder': data}\n",
        "\n",
        "        _, preds, _, _ = linRegOneMetric(data, y)\n",
        "        preds = preds['placeholder']\n",
        "\n",
        "        # compute and save Pearson's R\n",
        "        pearson_corr, _ = pearsonr(y, preds)\n",
        "        prs[deleteNow, secondDelete, thirdDelete] = pearson_corr\n",
        "\n",
        "  return prs\n",
        "\n",
        "def plotFeatAbMSEs(feat_ab_scores, original_mse_list, metric, time_radius, savepath, top_n=5, ax=None, spreadsheet_path=FEATURE_LABEL_PATH+'openface_2.0_hours.xlsx'):\n",
        "  # takes feat_ab_scores, a numpy array (n_features, n_timestamps) of MSEs\n",
        "  # outputs box and whisker plot of top_n features for the model\n",
        "\n",
        "  # procedure: get the top_n features with lowest mse averaged across timestamps\n",
        "  # make a box and whisker plot with each feature on x axis and MSEs on y axis\n",
        "  # for x axis labels, convert the index of each feature to english label\n",
        "  # by calling get_label_from_index(feat_ind)\n",
        "\n",
        "\n",
        "  # Get the average MSE across timestamps for each feature\n",
        "  avg_mses = np.mean(feat_ab_scores, axis=1)\n",
        "\n",
        "  # avg MSEs minus original_avg_MSE (make it difference!)\n",
        "  avg_mses = avg_mses - np.mean(original_mse_list)\n",
        "\n",
        "  # Get the indices of the top_n features with the highest difference in MSEs from original\n",
        "  top_indices = np.argsort(avg_mses)[-top_n:]\n",
        "  top_indices = top_indices[::-1]\n",
        "\n",
        "  # Get the English labels for the top_n features\n",
        "  top_labels = [get_label_from_index(ind, spreadsheet_path=spreadsheet_path) for ind in top_indices]\n",
        "\n",
        "  # Get the MSE values for the top_n features\n",
        "  top_mses = feat_ab_scores[top_indices]\n",
        "\n",
        "  # Adjust so it's top mses minus original\n",
        "  original_list_repeated = np.repeat(np.array(original_mse_list).reshape(1, -1), top_n, axis=0)\n",
        "  top_mses = top_mses - original_list_repeated\n",
        "\n",
        "  # Create a box and whisker plot\n",
        "  if ax is None:\n",
        "      original_ax = None\n",
        "      fig, ax = plt.subplots()\n",
        "  else:\n",
        "      original_ax = ax\n",
        "  ax.boxplot(top_mses.T, labels=top_labels, showmeans=True, meanprops={'marker': 'o', 'markerfacecolor': 'red', 'markersize': 10})\n",
        "\n",
        "  # Rotate x-axis labels by 45 degrees\n",
        "  ax.set_xticklabels(top_labels, rotation=45)\n",
        "\n",
        "  # Set the axis labels\n",
        "  ax.set_xlabel('Features')\n",
        "  ax.set_ylabel('Ablated - Original MSEs')\n",
        "\n",
        "  # Set the title\n",
        "  num_hrs = int(time_radius) / 60\n",
        "  if num_hrs > 1:\n",
        "      ax.set_title(f'Top {top_n} Features: {metric}, Time Window = {num_hrs} Hours')\n",
        "  else:\n",
        "      ax.set_title(f'Top {top_n} Features: {metric}, Time Window = {num_hrs} Hour')\n",
        "\n",
        "  plt.savefig(savepath, bbox_inches='tight')\n",
        "\n",
        "  # Show the plot if ax is None\n",
        "  if original_ax is None:\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "  return top_indices, fig\n",
        "\n",
        "def plotFeatAbPRs(feat_ab_prs, original_r_val, metric, time_radius, savepath, top_n=5, ax=None, spreadsheet_path=FEATURE_LABEL_PATH+'openface_2.0_hours.xlsx'):\n",
        "  # takes feat_ab_prs, a numpy array (n_features, ) of Pearson's R vals post-ablation\n",
        "  # outputs bar plot of top_n features TO REMOVE for the model\n",
        "\n",
        "  # procedure: get the top_n features with highest pearson's R\n",
        "  # make a bar plot with each feature on x axis and pearson's R from feat_ab_prs on y axis\n",
        "  # for x axis labels, convert the index of each feature to english label\n",
        "  # by calling get_label_from_index(feat_ind)\n",
        "\n",
        "  # if ax is given, plot on ax. If ax=None, make new fig, ax\n",
        "\n",
        "\n",
        "  # Get the top_n features with highest Pearson's R values\n",
        "  top_features_indices = np.argsort(feat_ab_prs)[-top_n:]\n",
        "  top_features_indices = top_features_indices[::-1]\n",
        "\n",
        "  # Get the labels for the top_n features\n",
        "  top_features_labels = [get_label_from_index(index, spreadsheet_path=spreadsheet_path) for index in top_features_indices]\n",
        "\n",
        "  # Get the corresponding Pearson's R values for the top_n features\n",
        "  top_features_prs = feat_ab_prs[top_features_indices]\n",
        "\n",
        "  # Plot the bar plot\n",
        "  if ax is None:\n",
        "      fig, ax = plt.subplots()\n",
        "  ax.bar(top_features_labels, top_features_prs)\n",
        "\n",
        "  # Rotate x-axis labels by 45 degrees\n",
        "  ax.set_xticklabels(top_features_labels, rotation=45)\n",
        "\n",
        "  # Set plot title and axis labels\n",
        "  # Set the title\n",
        "  num_hrs = int(time_radius) / 60\n",
        "  if num_hrs > 1:\n",
        "      ax.set_title(f'Top {top_n} Features to Remove: {metric}, Time Window = {num_hrs} Hours')\n",
        "  else:\n",
        "      ax.set_title(f'Top {top_n} Features to Remove: {metric}, Time Window = {num_hrs} Hour')\n",
        "\n",
        "  ax.set_xlabel(\"Features\")\n",
        "  ax.set_ylabel(f\"Pearson's R (Original={round(original_r_val, 2)})\")\n",
        "\n",
        "  # Save the plot\n",
        "  plt.savefig(savepath, bbox_inches='tight')\n",
        "\n",
        "  # Show the plot if ax=None\n",
        "  if ax is None:\n",
        "      plt.show()\n",
        "\n",
        "def find_max_indices(array, top_n):\n",
        "    # Flatten the 2D array into a 1D array\n",
        "    flattened_array = array.flatten()\n",
        "\n",
        "    # Find the indices of the top n maximum values in the flattened array\n",
        "    max_indices = np.argsort(flattened_array)[-top_n:][::-1]\n",
        "\n",
        "    # Convert the flattened indices to the corresponding row and column indices in the original array\n",
        "    row_indices, col_indices = np.unravel_index(max_indices, array.shape)\n",
        "\n",
        "    # Combine the row and column indices into pairs\n",
        "    index_combinations = list(zip(row_indices, col_indices))\n",
        "\n",
        "    return index_combinations\n",
        "\n",
        "def plot_feat_scatterplots(vectors_array, y, feat_ind_list, metric, savepath, spreadsheet_path=FEATURE_LABEL_PATH+'openface_2.0_hours.xlsx'):\n",
        "    # for each feature, plot feature on x axis and self-report score on y axis\n",
        "    # vectors_array is the array of feature vectors for ONE time radius\n",
        "    # y - self-reports\n",
        "    # feat_ind_list - list of the indices of the top features\n",
        "    # metric -- e.g. Mood or Anxiety\n",
        "    # savepath - where to save the figure\n",
        "\n",
        "    plt.rcParams['lines.markersize'] = 15\n",
        "\n",
        "    num_plots = len(feat_ind_list)\n",
        "    num_cols = min([len(feat_ind_list), 4])\n",
        "    num_rows = (num_plots + num_cols - 1) // num_cols\n",
        "\n",
        "    r_list = []\n",
        "    p_list = []\n",
        "\n",
        "    # Calculate the desired figure size for larger plot\n",
        "    figsize = (28, 12)\n",
        "\n",
        "    # Create subplots with auto aspect ratio\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "\n",
        "    #axes.set_adjustable('box')\n",
        "\n",
        "    if num_rows == 1:\n",
        "      axes = axes.reshape((1, num_cols))\n",
        "\n",
        "    # Flatten the axes array if necessary\n",
        "    if num_plots == 1:\n",
        "        fig, axes = plt.subplots(1, 1, figsize=figsize)\n",
        "        axes = np.array([axes]).reshape(1, 1)\n",
        "\n",
        "\n",
        "    # Loop through the dictionaries\n",
        "    for enum, i in enumerate(feat_ind_list):\n",
        "        x_list = vectors_array[:, i].astype(float)\n",
        "        y_list = np.array(y).astype(float)\n",
        "\n",
        "        #y_pred = np.array([i[0] for i in y_pred])\n",
        "\n",
        "        # Get the subplot coordinates\n",
        "        row = enum // num_cols\n",
        "        col = enum % num_cols\n",
        "\n",
        "        # Plot predictions on the subplot\n",
        "        pearson_corr, p_val = plot_predictions(x_list, y_list, randShuffleR=None, ax=axes[row, col])\n",
        "\n",
        "\n",
        "        axes[row, col].set_title(f'{metric} vs. {get_label_from_index(i, spreadsheet_path=spreadsheet_path)}', fontsize=24)\n",
        "\n",
        "\n",
        "        # Redo x-axis and y-axis labels for subplot\n",
        "        axes[row, col].set_xlabel(get_label_from_index(i, spreadsheet_path=spreadsheet_path), fontsize=24)\n",
        "        axes[row, col].set_ylabel('')\n",
        "\n",
        "        #axes[row, col].set_adjustable('box')\n",
        "\n",
        "    # Add overall title\n",
        "    fig.suptitle(f'Top {num_plots} Features for {metric}', fontsize=30, y=1.05)\n",
        "\n",
        "    # Set shared y-axis label\n",
        "    #fig.text(0.5, 0, f'Self-Reported {metric} Scores', ha='center', fontsize=24)\n",
        "    fig.text(-0.01, 0.5, f'Self-Reported {metric} Scores', va='center', rotation='vertical', fontsize=24)\n",
        "\n",
        "    # Adjust spacing and layout\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.savefig(savepath, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return r_list, p_list, fig\n",
        "\n",
        "\n",
        "# def extractOneMetric(metric, vectors_now, df_moodTracking=df_moodTracking, remove_outliers=False):\n",
        "#   # extracts the vectors needed for linear regression\n",
        "#   # e.g. Mood only, for all time windows\n",
        "#   # metric -- a string that is a self-report metric (ex. 'Mood' or 'Pain')\n",
        "#   # vectors_now -- our feature vectors (all)\n",
        "#   # df_moodTracking -- load in and pre-process self-report google sheet\n",
        "\n",
        "#   # returns vectors_return and y\n",
        "#   # vectors_return -- a dictionary mapping time radius (in minutes) to features\n",
        "#   # y -- a numpy array with labels (self-reported metrics)\n",
        "\n",
        "\n",
        "#   y = df_moodTracking[metric].values.astype(float)\n",
        "\n",
        "#   # # just valid indices (remove nan self-reports!)\n",
        "#   # valid_indices = ~pd.isna(y)\n",
        "#   # y = y[valid_indices]\n",
        "\n",
        "#   # Initially, set valid_indices to include all indices\n",
        "#   valid_indices = np.arange(len(y))\n",
        "\n",
        "#   # Step 1: Remove NaN values\n",
        "#   nan_mask = ~pd.isna(y)\n",
        "#   y = y[nan_mask]\n",
        "#   valid_indices = valid_indices[nan_mask]\n",
        "\n",
        "\n",
        "#   if remove_outliers:\n",
        "#     # Step 2: Remove outliers\n",
        "#     mean_y = np.mean(y)\n",
        "#     std_y = np.std(y)\n",
        "#     outlier_mask = (y >= mean_y - 2 * std_y) & (y <= mean_y + 2 * std_y)\n",
        "#     y = y[outlier_mask]\n",
        "#     valid_indices = valid_indices[outlier_mask]\n",
        "\n",
        "\n",
        "\n",
        "#   vectors_return = {}\n",
        "\n",
        "#   # We will delete indices of self-reports where at least one timestamp doesn't have ANY data at all!\n",
        "#   indices_to_delete = []\n",
        "\n",
        "#   # loop through the timestamps\n",
        "#   # Determine which timestamps to delete (indices_to_delete)\n",
        "#   for i in vectors_now.keys():\n",
        "\n",
        "#     vectors_one_timestamp = np.array([vectors_now[i][str_to_ts(dt)] for dt in df_moodTracking['Datetime']])\n",
        "\n",
        "#     # we want just the valid features (where self-report is not nan)\n",
        "#     vectors_one_timestamp = vectors_one_timestamp[valid_indices]\n",
        "\n",
        "#     # Figure out the correct size of the vector and delete all others\n",
        "#     correct_vector_dim = 0\n",
        "#     for enum_num, vector in enumerate(vectors_one_timestamp):\n",
        "#         if vector.size == 0:\n",
        "#             indices_to_delete.append(enum_num)\n",
        "#         elif vector.shape[0] > correct_vector_dim:\n",
        "#           correct_vector_dim = vector.shape[0]\n",
        "#     for enum_num, vector in enumerate(vectors_one_timestamp):\n",
        "#         if vector.size > 0 and vector.shape[0] < correct_vector_dim:\n",
        "#             indices_to_delete.append(enum_num)\n",
        "\n",
        "\n",
        "#   # Delete those indices from all timestamps\n",
        "#   for i in vectors_now.keys():\n",
        "\n",
        "#     vectors_one_timestamp = np.array([vectors_now[i][str_to_ts(dt)] for dt in df_moodTracking['Datetime']])\n",
        "\n",
        "#     # we want just the valid features (where self-report is not nan)\n",
        "#     vectors_one_timestamp = vectors_one_timestamp[valid_indices]\n",
        "\n",
        "#     # Delete indices from the full previous loop\n",
        "#     vectors_one_timestamp = np.delete(vectors_one_timestamp, indices_to_delete, axis=0)\n",
        "\n",
        "#     if vectors_one_timestamp.ndim == 1:\n",
        "#       print(f'WARNING: NEEDED TO RESHAPE FOR TIME WINDOW {i}')\n",
        "#       # Stack the arrays along a new axis to get a 2D array\n",
        "#       vectors_one_timestamp = np.stack(vectors_one_timestamp, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "#     vectors_return[i] = vectors_one_timestamp\n",
        "\n",
        "#   y = np.delete(y, indices_to_delete, axis=0)\n",
        "\n",
        "#   # Make sure we get the right 2D shape for each time window\n",
        "#   for i in vectors_return.keys():\n",
        "\n",
        "#     vectors_one_timestamp = vectors_return[i]\n",
        "\n",
        "#     # If it's still 1d, then let's force the right 2D shape\n",
        "#     if vectors_one_timestamp.ndim == 1:\n",
        "#       print(f'WARNING: NEEDED TO DOUBLE RESHAPE FOR TIME WINDOW {i}')\n",
        "#       vectors_return[i] = vectors_one_timestamp.reshape(y.shape[0], -1)\n",
        "\n",
        "\n",
        "#   return vectors_return, y\n",
        "\n",
        "\n",
        "def extractOneMetric(metric, vectors_now, df_moodTracking=df_moodTracking, remove_outliers=False):\n",
        "  # extracts the vectors needed for linear regression\n",
        "  # e.g. Mood only, for all time windows\n",
        "  # metric -- a string that is a self-report metric (ex. 'MADRS')\n",
        "  # vectors_now -- our feature vectors (all)\n",
        "  # df_moodTracking -- load in and pre-process self-report google sheet\n",
        "\n",
        "  # returns vectors_return and y\n",
        "  # vectors_return -- a dictionary mapping time radius (in minutes) to features\n",
        "  # y -- a numpy array with labels (self-reported metrics)\n",
        "\n",
        "\n",
        "  y = df_moodTracking[metric].values.astype(float)\n",
        "  y = np.array([float(y_now) for y_now in y])\n",
        "\n",
        "  # # just valid indices (remove nan self-reports!)\n",
        "  # valid_indices = ~pd.isna(y)\n",
        "  # y = y[valid_indices]\n",
        "\n",
        "  # Initially, set valid_indices to include all indices\n",
        "  valid_indices = np.arange(len(y))\n",
        "\n",
        "  # Step 1: Remove NaN values\n",
        "  nan_mask = ~pd.isna(y)\n",
        "  y = y[nan_mask]\n",
        "  valid_indices = valid_indices[nan_mask]\n",
        "\n",
        "\n",
        "  if remove_outliers:\n",
        "    # Step 2: Remove outliers\n",
        "    mean_y = np.mean(y)\n",
        "    std_y = np.std(y)\n",
        "    outlier_mask = (y >= mean_y - 2 * std_y) & (y <= mean_y + 2 * std_y)\n",
        "    y = y[outlier_mask]\n",
        "    valid_indices = valid_indices[outlier_mask]\n",
        "\n",
        "\n",
        "\n",
        "  vectors_return = {}\n",
        "\n",
        "  # Temp debug: outpatient dataset spreadsheet seems to be inserting spaces\n",
        "  #replace_underscore_space = lambda s: s.replace(\"_ \", \"_\")\n",
        "  def modify_keys(dictionary):\n",
        "    # Using dictionary comprehension to create a new dictionary\n",
        "    # with keys that have spaces removed\n",
        "    return {key.replace(' ', ''): value for key, value in dictionary.items()}\n",
        "\n",
        "  # loop through the outpatient videos at each time truncation we're considering (e.g. 10 mins)\n",
        "  for i in vectors_now.keys():\n",
        "    vectors_now_dict = vectors_now[i]\n",
        "    vectors_now_dict_fixed = modify_keys(vectors_now_dict)\n",
        "\n",
        "    #import pdb; pdb.set_trace()\n",
        "    vectors_one_timestamp = np.array([vectors_now_dict_fixed[fn.replace(' ', '')] for fn in df_moodTracking['Filename']])\n",
        "\n",
        "    # we want just the valid features (where self-report is not nan)\n",
        "    vectors_one_timestamp = vectors_one_timestamp[valid_indices]\n",
        "\n",
        "    vectors_return[i] = vectors_one_timestamp\n",
        "\n",
        "  return vectors_return, y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_pearsons_r_vs_alpha(pearson_r_list, ALPHAS_FOR_SEARCH, method, save_path):\n",
        "    \"\"\"\n",
        "    Plots Pearson's R values against Alphas and saves the plot to the specified path.\n",
        "\n",
        "    :param pearson_r_list: List of Pearson's R values.\n",
        "    :param ALPHAS_FOR_SEARCH: List of Alpha values.\n",
        "    :param method: The method used (string).\n",
        "    :param save_path: File path to save the plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(ALPHAS_FOR_SEARCH, pearson_r_list, marker='o')\n",
        "    plt.title(f'LASSO: Alpha Search {method}')\n",
        "    plt.xlabel('Alpha')\n",
        "    plt.ylabel(\"Pearson's R\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "bUEpxgLPWRw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alpha Search"
      ],
      "metadata": {
        "id": "ECLHmN7qYYiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ALPHA PARAMETER SEARCH FOR LASSO - RUN THIS FIRST!\n",
        "\n",
        "all_metrics = [col for col in df_moodTracking.columns if col != 'Datetime']\n",
        "\n",
        "FILE_ENDING = '.png'\n",
        "\n",
        "# We are just searching using lasso regression\n",
        "#RESULTS_PREFIX_LIST = ['OF_L_', 'OGAU_L_', 'OFAUHSE_L_', 'OGAUHSE_L_', 'HSE_L_', 'ALL_L_']\n",
        "RESULTS_PREFIX_LIST = ['OGAU_L_', 'OGAUHSE_L_', 'HSE_L_']\n",
        "#RESULTS_PREFIX_LIST = ['OGAUHSE_L_']\n",
        "\n",
        "\n",
        "EMOTIONS_FOR_SEARCH = ['MADRS'] # We are just searching on Mood\n",
        "TIME_WINDOW_FOR_SEARCH = 10 # We are just searching 10 minutes\n",
        "\n",
        "# List of alpha values to search through\n",
        "ALPHAS_FOR_SEARCH = np.arange(0, 1, 0.1)\n",
        "#ALPHAS_FOR_SEARCH = np.arange(0, 1.6, 0.1)\n",
        "#ALPHAS_FOR_SEARCH = np.arange(0, 3, 0.2)\n",
        "#ALPHAS_FOR_SEARCH = np.arange(0, 5, 0.2)\n",
        "\n",
        "\n",
        "# This will populate with the best alphas for each prefix in RESULTS_PREFIX_LIST\n",
        "best_alphas_lasso = {}\n",
        "\n",
        "for RESULTS_PREFIX in RESULTS_PREFIX_LIST:\n",
        "  do_lasso = False\n",
        "  do_ridge = False\n",
        "\n",
        "  if '_L_' in RESULTS_PREFIX:\n",
        "    do_lasso = True\n",
        "\n",
        "  if '_R_' in RESULTS_PREFIX:\n",
        "    do_ridge = True\n",
        "\n",
        "\n",
        "  if 'OF_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+f'openface_0.5_hours.xlsx'\n",
        "    vectors_now = openface_vectors_dict\n",
        "    method_now = 'OpenFace'\n",
        "\n",
        "  elif 'OGAU_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+'opengraphau_0.5_hours.xlsx'\n",
        "    vectors_now = opengraphau_vectors_dict\n",
        "    method_now = 'OpenGraphAU'\n",
        "\n",
        "  elif 'OFAUHSE_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+'ofauhse_0.5_hours.xlsx'\n",
        "    vectors_now = ofauhsemotion_vectors_dict\n",
        "    method_now = 'OFAU+HSE'\n",
        "\n",
        "  elif 'OGAUHSE_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+'ogauhse_0.5_hours.xlsx'\n",
        "    vectors_now = ogauhsemotion_vectors_dict\n",
        "    method_now = 'OGAU+HSE'\n",
        "\n",
        "  elif 'HSE_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+'hsemotion_0.5_hours.xlsx'\n",
        "    vectors_now = hsemotion_vectors_dict\n",
        "    method_now = 'HSEmotion'\n",
        "\n",
        "  elif 'ALL_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+'all_0.5_hours.xlsx'\n",
        "    vectors_now = all_vectors_dict\n",
        "    method_now = 'ALL(OF+OG+HSE)'\n",
        "\n",
        "\n",
        "  # Let's put each setting in its own folder!\n",
        "  os.makedirs(RESULTS_PATH_BASE + 'SEARCH_Alpha_Lasso/' + RESULTS_PREFIX, exist_ok=True)\n",
        "  results_prefix_unmodified = RESULTS_PREFIX\n",
        "  RESULTS_PREFIX = 'SEARCH_Alpha_Lasso/' + RESULTS_PREFIX + '/' + RESULTS_PREFIX\n",
        "\n",
        "  # This will store the best R, averaged across all metrics we're testing, for each alpha\n",
        "  pearson_r_list = []\n",
        "\n",
        "  for alpha_now in ALPHAS_FOR_SEARCH:\n",
        "\n",
        "    avg_best_R = 0\n",
        "\n",
        "    # Loop through EMOTIONS_FOR_SEARCH\n",
        "    for metric in EMOTIONS_FOR_SEARCH:\n",
        "      print('METRIC NOW: ', metric)\n",
        "      vectors_return, y = extractOneMetric(metric, vectors_now=vectors_now)\n",
        "\n",
        "      # Limit to just one time window for alpha search\n",
        "      tmp_vectors = vectors_return\n",
        "      vectors_return = {}\n",
        "      vectors_return[TIME_WINDOW_FOR_SEARCH] = tmp_vectors[TIME_WINDOW_FOR_SEARCH]\n",
        "      del tmp_vectors\n",
        "\n",
        "      scores, preds, y, models = linRegOneMetric(vectors_return, y, do_lasso=do_lasso, do_ridge=do_ridge, alpha=alpha_now)\n",
        "      scores_r, preds_r, _, models_r = linRegOneMetric(vectors_return, y, randShuffle=True, alpha=alpha_now)\n",
        "\n",
        "      # make scatterplots\n",
        "      randShuffleR, _, _ = plot_scatterplots(preds_r, y, f'{metric} Random Shuffle', RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_scatterRand_{alpha_now}{FILE_ENDING}')\n",
        "      r_list, p_list, scatterFig = plot_scatterplots(preds, y, metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_scatterplots_{alpha_now}{FILE_ENDING}', randShuffleR=randShuffleR)\n",
        "\n",
        "      # Determine our best time radius for this metric based on Pearson's R\n",
        "      best_time_radius = list(scores.keys())[np.argmax(r_list)]\n",
        "      best_mse_list = scores[best_time_radius]\n",
        "      best_avg_mse = np.mean(scores[best_time_radius])\n",
        "      best_pearson_r = r_list[np.argmax(r_list)]\n",
        "\n",
        "      # Add to our avg best R\n",
        "      avg_best_R = avg_best_R + best_pearson_r\n",
        "\n",
        "      # bar plot for pearson r\n",
        "      rPlotFig = make_r_barplot(r_list, list(scores.keys()), metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_R_{alpha_now}{FILE_ENDING}', method_now=method_now)\n",
        "\n",
        "      # make MSE plot\n",
        "      MSEPlotFig = make_mse_boxplot(scores, metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_MSE_{alpha_now}{FILE_ENDING}', method_now=method_now)\n",
        "\n",
        "    # Add one R value for this alpha value to pearson_r_list\n",
        "    avg_best_R = avg_best_R / len(EMOTIONS_FOR_SEARCH)\n",
        "    pearson_r_list.append(avg_best_R)\n",
        "\n",
        "  # Plot R vs. alpha for this setting\n",
        "  plot_pearsons_r_vs_alpha(pearson_r_list=pearson_r_list, ALPHAS_FOR_SEARCH=ALPHAS_FOR_SEARCH, method=method_now, save_path=RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_Alpha_Search{FILE_ENDING}')\n",
        "\n",
        "  # Find best alpha for this setting\n",
        "  best_index_of_alpha = np.argmax(pearson_r_list)\n",
        "  best_alpha_value = ALPHAS_FOR_SEARCH[best_index_of_alpha]\n",
        "  best_alphas_lasso[results_prefix_unmodified] = best_alpha_value\n"
      ],
      "metadata": {
        "id": "cz5HFPcNWRw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE VARIABLES\n",
        "\n",
        "save_var(best_alphas_lasso, forced_name=f'best_alphas_lasso_{PAT_NOW}')\n",
        "\n"
      ],
      "metadata": {
        "id": "eVpa4rFGWRw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD VARIABLES\n",
        "\n",
        "best_alphas_lasso = load_var(f'best_alphas_lasso_{PAT_NOW}')\n",
        "\n"
      ],
      "metadata": {
        "id": "_BRg7NCvWRw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Plots"
      ],
      "metadata": {
        "id": "LbrmWuiEYghJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATE ALL PLOTS! ONE CODE BLOCK\n",
        "\n",
        "if 'best_alphas_lasso' not in globals():\n",
        "    raise NameError(\"GO RUN THE LASSO ALPHA PARAMETER SEARCH BLOCK FIRST!\")\n",
        "\n",
        "#all_metrics = [col for col in df_moodTracking.columns if col != 'Datetime']\n",
        "#all_metrics = ['Mood', 'Anxiety', 'Hunger']\n",
        "all_metrics = ['MADRS']\n",
        "\n",
        "\n",
        "FILE_ENDING = '.png'\n",
        "# RESULTS_PREFIX_LIST = ['OF_', 'OGAU_', 'OFAUHSE_', 'OGAUHSE_', 'HSE_', 'ALL_',\n",
        "#                        'OF_L_', 'OGAU_L_', 'OFAUHSE_L_', 'OGAUHSE_L_', 'HSE_L_', 'ALL_L_',\n",
        "#                        'OF_R_', 'OGAU_R_', 'OFAUHSE_R_', 'OGAUHSE_R_', 'HSE_R_', 'ALL_R_']\n",
        "\n",
        "# RESULTS_PREFIX_LIST = ['OF_L_', 'OGAUHSE_L_', 'OGAU_L_', 'OFAUHSE_L_', 'HSE_L_', 'ALL_L_']\n",
        "\n",
        "RESULTS_PREFIX_LIST = ['OGAU_L_', 'OGAUHSE_L_', 'HSE_L_']\n",
        "\n",
        "#RESULTS_PREFIX_LIST = ['OGAUHSE_L_']\n",
        "\n",
        "# Do we remove ground truth labels that are over 2 standard deviations from the mean?\n",
        "REMOVE_OUTLIERS = False\n",
        "\n",
        "\n",
        "for RESULTS_PREFIX in RESULTS_PREFIX_LIST:\n",
        "  do_lasso = False\n",
        "  do_ridge = False\n",
        "\n",
        "  if '_L_' in RESULTS_PREFIX:\n",
        "    do_lasso = True\n",
        "\n",
        "  if '_R_' in RESULTS_PREFIX:\n",
        "    do_ridge = True\n",
        "\n",
        "\n",
        "  if 'OF_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+f'openface_0.5_hours.xlsx'\n",
        "    vectors_now = openface_vectors_dict\n",
        "    method_now = 'OpenFace'\n",
        "\n",
        "  elif 'OGAU_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+'opengraphau_0.5_hours.xlsx'\n",
        "    vectors_now = opengraphau_vectors_dict\n",
        "    method_now = 'OpenGraphAU'\n",
        "\n",
        "  elif 'OFAUHSE_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+'ofauhse_0.5_hours.xlsx'\n",
        "    vectors_now = ofauhsemotion_vectors_dict\n",
        "    method_now = 'OFAU+HSE'\n",
        "\n",
        "  elif 'OGAUHSE_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+'ogauhse_0.5_hours.xlsx'\n",
        "    vectors_now = ogauhsemotion_vectors_dict\n",
        "    method_now = 'OGAU+HSE'\n",
        "\n",
        "  elif 'HSE_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+'hsemotion_0.5_hours.xlsx'\n",
        "    vectors_now = hsemotion_vectors_dict\n",
        "    method_now = 'HSEmotion'\n",
        "\n",
        "  elif 'ALL_' in RESULTS_PREFIX:\n",
        "    spreadsheet_path = FEATURE_LABEL_PATH+'all_0.5_hours.xlsx'\n",
        "    vectors_now = all_vectors_dict\n",
        "    method_now = 'ALL(OF+OG+HSE)'\n",
        "\n",
        "\n",
        "  # Let's put each setting in its own folder!\n",
        "  os.makedirs(RESULTS_PATH_BASE + RESULTS_PREFIX, exist_ok=True)\n",
        "  results_prefix_unmodified = RESULTS_PREFIX\n",
        "  RESULTS_PREFIX = RESULTS_PREFIX + '/' + RESULTS_PREFIX\n",
        "\n",
        "\n",
        "  # Loop through metrics (Anxiety, Depression, Mood, etc.)\n",
        "  for metric in all_metrics:\n",
        "    print('METRIC NOW: ', metric)\n",
        "    if do_lasso:\n",
        "      alpha_now = best_alphas_lasso[results_prefix_unmodified]\n",
        "    elif do_ridge:\n",
        "      alpha_now = best_alphas_ridge[results_prefix_unmodified]\n",
        "    else:\n",
        "      # Neither lasso nor ridge, so alpha is irrelevant\n",
        "      alpha_now = 1.0\n",
        "\n",
        "    vectors_return, y = extractOneMetric(metric, vectors_now=vectors_now, remove_outliers=REMOVE_OUTLIERS)\n",
        "    scores, preds, y, models = linRegOneMetric(vectors_return, y, do_lasso=do_lasso, do_ridge=do_ridge, alpha=alpha_now)\n",
        "    scores_r, preds_r, _, models_r = linRegOneMetric(vectors_return, y, randShuffle=True, alpha=alpha_now)\n",
        "\n",
        "    # make scatterplots\n",
        "    randShuffleR, _, _ = plot_scatterplots(preds_r, y, f'{metric} Random Shuffle', RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_scatterRand{FILE_ENDING}')\n",
        "    r_list, p_list, scatterFig = plot_scatterplots(preds, y, metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_scatterplots{FILE_ENDING}', randShuffleR=randShuffleR)\n",
        "\n",
        "    # Determine our best time radius for this metric based on Pearson's R\n",
        "    best_time_radius = list(scores.keys())[np.argmax(r_list)]\n",
        "    best_mse_list = scores[best_time_radius]\n",
        "    best_avg_mse = np.mean(scores[best_time_radius])\n",
        "    best_pearson_r = r_list[np.argmax(r_list)]\n",
        "\n",
        "    # bar plot for pearson r\n",
        "    rPlotFig = make_r_barplot(r_list, list(scores.keys()), metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_R{FILE_ENDING}', method_now=method_now)\n",
        "\n",
        "    # make MSE plot\n",
        "    MSEPlotFig = make_mse_boxplot(scores, metric, RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_MSE{FILE_ENDING}', method_now=method_now)\n",
        "\n",
        "    # Feature ablation\n",
        "    feat_ab_scores, feat_ab_prs = featureAblate(vectors_return[best_time_radius], y, do_lasso=do_lasso, do_ridge=do_ridge)\n",
        "\n",
        "    top_indices, featAbMSEFig = plotFeatAbMSEs(feat_ab_scores, best_mse_list, metric, best_time_radius, savepath=RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_featAblate_MSEs{FILE_ENDING}', spreadsheet_path=spreadsheet_path)\n",
        "    plotFeatAbPRs(feat_ab_prs, best_pearson_r, metric, best_time_radius, savepath=RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_featAblate_R{FILE_ENDING}', spreadsheet_path=spreadsheet_path)\n",
        "\n",
        "    # extract just ONE scatterplot (the best pearson's R) and save it individually\n",
        "    plt.rcParams['lines.markersize'] = 9\n",
        "    _, _, bestScatterFig = plot_predictions(y, preds[best_time_radius], randShuffleR=randShuffleR[np.argmax(r_list)], ax=None, time_rad=best_time_radius, metric=metric)\n",
        "    bestScatterFig.savefig(RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_linReg_bestScatter{FILE_ENDING}', bbox_inches='tight')\n",
        "\n",
        "    # Plot top n features vs. self-reported scores\n",
        "    PLOT_NOW = 3\n",
        "    plot_feat_scatterplots(vectors_array=vectors_return[best_time_radius], y=y, feat_ind_list=top_indices[:PLOT_NOW], metric=metric, savepath=RESULTS_PATH_BASE + f'{RESULTS_PREFIX}{metric}_topFeats{FILE_ENDING}', spreadsheet_path=spreadsheet_path)\n"
      ],
      "metadata": {
        "id": "43grLIvwWRw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "flOR18IHgixA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}