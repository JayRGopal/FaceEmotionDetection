# -*- coding: utf-8 -*-
"""Video to Frames.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ufoo7O1s62MKK_t_GUWoQHcP1BzedYgO

# Full Pipeline - OpenGraphAU

### Need to define all the functions before coming back to this block
"""

# Set the parameters
VIDEO_PATH = '/content/drive/MyDrive/Stanford/Demo_Video.mp4'
SAVE_PATH = '/content/sample_output.mp4'
START_FRAME = 1500
NUM_TO_EXTRACT = 300
MODEL_TYPE = 'OpenGraphAU'
MODEL_BACKBONE = 'resnet50'
POST_PROCESSING_METHOD = 'RNN'

# Extract video frames
(ims, im_test) = extract_images(path=VIDEO_PATH, start_frame=START_FRAME, num_to_extract=NUM_TO_EXTRACT)

# Detect a face in each frame
faces = detect_extract_faces(ims)

# Load the relevant network and get its predictions
net = load_network(model_type=MODEL_TYPE, backbone=MODEL_BACKBONE)
predictions = get_model_preds(faces, net, model_type=MODEL_TYPE)

# Post-processing
preds_post = postprocess_outs(predictions, method=POST_PROCESSING_METHOD)

# Create and download an output video
labels = extract_labels(ims, preds_post, model_type=MODEL_TYPE)
save_video_from_images(labels, video_name=SAVE_PATH, fps=30)
files.download(SAVE_PATH)

"""# Full Pipeline - OpenFace

### Run the analysis on Docker and export the CSV file with outputs
"""

# Set the parameters
VIDEO_PATH = '/content/drive/MyDrive/Stanford/Demo_Video.mp4'
CSV_PATH = '/content/drive/MyDrive/Stanford/OpenFace/demo_video.csv'
SAVE_PATH = '/content/sample_output.mp4'
START_FRAME = 100
NUM_TO_EXTRACT = 700
MODEL_TYPE = 'OpenFace'

# Extract video frames
(ims, im_test) = extract_images(path=VIDEO_PATH, start_frame=START_FRAME, num_to_extract=NUM_TO_EXTRACT)

# Load the predictions (exported from Docker)
predictions = get_openface_preds(csv_path=CSV_PATH, start_frame=START_FRAME, num_to_extract=NUM_TO_EXTRACT)

# Create and download an output video
labels = extract_labels(ims, preds_post, model_type=MODEL_TYPE)
save_video_from_images(labels, video_name=SAVE_PATH, fps=30)
files.download(SAVE_PATH)

"""# Video to Image Frames"""

import numpy as np
import tensorflow as tf
import torch
from matplotlib import pyplot as plt
import torchvision
import os
from PIL import Image
import torch.nn as nn
import torchvision.models
import math
import cv2

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import cv2
from google.colab.patches import cv2_imshow

def extract_images(path, start_frame, num_to_extract): 
  capture = cv2.VideoCapture(path)

  ims = []
  frameNr = 0

  while frameNr < (start_frame + num_to_extract):
      success, frame = capture.read()
      if success:
          if frameNr >= start_frame:
            ims.append(frame)
      else:
          break
      frameNr = frameNr+1
  capture.release()

  ims = np.array(ims)

  im_test = torch.tensor(np.resize(ims[0:ims.shape[0]], (ims.shape[0], 3, 244, 244))).type(torch.float32)
  return (ims, im_test)

PATH = '/content/drive/MyDrive/Stanford/Demo_Video.mp4'
START_FRAME = 100
NUM_TO_EXTRACT = 700
(ims, im_test) = extract_images(path=PATH, start_frame=START_FRAME, num_to_extract=NUM_TO_EXTRACT)

ims.shape

"""# Face Detection (RetinaFace)"""

!pip install retina-face -q

from retinaface import RetinaFace

def detect_extract_faces(ims, face_shape=(224, 224, 3)):
  NUM_TO_EXTRACT = ims.shape[0]
  faces = torch.empty((NUM_TO_EXTRACT, face_shape[0], face_shape[1], face_shape[2]), dtype=torch.float32)
  for i in range(NUM_TO_EXTRACT):
    one_face = RetinaFace.extract_faces(ims[i])
    if len(one_face) > 0:
      one_face = np.array(one_face[0])
      dims_fixed = tf.image.resize_with_crop_or_pad(one_face, face_shape[0], face_shape[1])
      torch_one = torch.tensor(dims_fixed.numpy())
      faces[i] = torch_one.float()
    else:
      faces[i] = torch.zeros(face_shape[0], face_shape[1], face_shape[2]).float()
  faces = torch.swapaxes(faces, 1, 3) / 255
  return faces

faces = detect_extract_faces(ims)

faces.shape

"""# ME-GraphAU & OpenGraphAU

"""

# Here's a table with English labels for AU1, AU2, etc.
# https://en.wikipedia.org/wiki/Facial_Action_Coding_System

MODEL_TYPE = 'OpenGraphAU'
# MODEL_TYPE = 'BP4D'

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content'
!git clone https://github.com/cvi-szu/me-graphau
# %cd '/content/me-graphau/'

!cp '/content/drive/MyDrive/Stanford/ME-GraphAU/resnet50-19c8e357.pth' '/content/me-graphau/checkpoints/'

!pip install timm -q

from model.MEFL import MEFARG
from OpenGraphAU.model.ANFL import MEFARG as MEFARG_OpenGraphAU
from collections import OrderedDict


def load_network(model_type, backbone):
  if model_type == 'BP4D':
    net = MEFARG(num_classes=12, backbone=backbone)
    path = '/content/drive/MyDrive/Stanford/ME-GraphAU/MEFARG_resnet50_BP4D_fold3.pth'
    net.load_state_dict(torch.load(path).get('state_dict'))
  elif model_type == 'OpenGraphAU':
    net = MEFARG_OpenGraphAU(num_main_classes=27, num_sub_classes=14, backbone=backbone, neighbor_num=4)

    path = '/content/drive/MyDrive/Stanford/ME-GraphAU/OpenGprahAU-ResNet50_first_stage.pth'
    oau_state_dict = torch.load(path).get('state_dict')
    oau_keys = oau_state_dict.keys()

    # filter out module.
    oau_state_dict_mod = OrderedDict([(k[7:], oau_state_dict[k]) if "module." in k else (k, oau_state_dict[k]) for k in oau_keys])
    
    net.load_state_dict(oau_state_dict_mod)

  net.eval()
  return net

net = load_network(model_type=MODEL_TYPE, backbone='resnet50')

def get_model_preds(faces, net, model_type):
  if torch.cuda.is_available():
      faces = faces.cuda()
      net = net.cuda()

  with torch.no_grad():
    if model_type == 'BP4D':
      pred_ff = net(faces / 255)
      real_preds_ff = pred_ff[0].cpu().numpy()
    elif model_type == 'OpenGraphAU':
      pred_ff = net(faces)
      real_preds_ff = pred_ff.cpu().numpy()
  return real_preds_ff

real_preds_ff = get_model_preds(faces, net, model_type=MODEL_TYPE)

plt.imshow(faces[20].cpu().transpose(0, 2))

"""# OpenFace"""

# Commented out IPython magic to ensure Python compatibility.
# %cd '/'

import pandas as pd

def get_openface_preds(csv_path, start_frame, num_to_extract):
  df = pd.read_csv(csv_path, index_col=0)
  openface_outs = df[[' AU01_c', ' AU02_c', ' AU04_c', ' AU05_c', ' AU06_c', ' AU07_c', ' AU09_c', ' AU10_c', ' AU12_c', ' AU14_c', ' AU15_c', ' AU17_c', ' AU20_c', ' AU23_c', ' AU25_c', ' AU26_c', ' AU28_c', ' AU45_c']].to_numpy()
  openface_outs = openface_outs[start_frame:(start_frame + num_to_extract)]
  return openface_outs

csv_path = '/content/drive/MyDrive/Stanford/OpenFace/demo_video.csv'
openface_outs = get_openface_preds(csv_path, START_FRAME, NUM_TO_EXTRACT)

openface_outs.shape

"""# Post-Processing

### Goal: increase stability of AU predictions, given the outputs (num frames  X  num AUs) of the network

"""

!pip install pykalman -q

from pykalman import KalmanFilter

def simple_moving_average(X, window_size):
    """Calculate the simple moving average over a window of size window_size."""
    X_avg = np.zeros_like(X)
    for i in range(X.shape[0]):
        if i < window_size:
            X_avg[i] = np.mean(X[:i+1], axis=0)
        else:
            X_avg[i] = np.mean(X[i-window_size+1:i+1], axis=0)
    return X_avg


def exponential_moving_average(X, alpha):
    """Calculate the exponential moving average with decay parameter alpha."""
    X_ema = np.zeros_like(X)
    X_ema[0] = X[0]
    for i in range(1, X.shape[0]):
        X_ema[i] = alpha * X[i] + (1 - alpha) * X_ema[i-1]
    return X_ema



def kalman_filter(X):
    """Smooth the predictions using a Kalman filter."""
    kf = KalmanFilter(n_dim_obs=X.shape[1], n_dim_state=X.shape[1])
    X_kf, _ = kf.smooth(X)
    return X_kf

preds_post_sm = simple_moving_average(real_preds_ff, 10)
preds_post_em = exponential_moving_average(real_preds_ff, 0.9)
preds_post_kf = kalman_filter(real_preds_ff)

!pip install tslearn -q

from scipy import signal

def temporal_difference(X):
    """Smooth the predictions using temporal difference."""
    # Compute the temporal differences
    diff = np.diff(X, axis=0)
    
    # Define the filter coefficients
    b = np.ones(5) / 5
    a = 1
    
    # Apply the filter to the differences
    diff_smoothed = signal.lfilter(b, a, diff, axis=0)
    
    # Integrate the smoothed differences
    X_smoothed = np.cumsum(np.vstack((X[0], diff_smoothed)), axis=0)
    return X_smoothed

import tensorflow as tf

def rnn_smoothing(X):
    """Smooth the predictions using a recurrent neural network."""
    # Define the RNN model
    model = tf.keras.models.Sequential([
        tf.keras.layers.InputLayer(input_shape=(X.shape[1],)),
        tf.keras.layers.Reshape((1, X.shape[1])),
        tf.keras.layers.LSTM(64, return_sequences=True),
        tf.keras.layers.LSTM(64, return_sequences=True),
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dense(X.shape[1])
    ])
    
    # Compile the model
    model.compile(loss='mse', optimizer='adam')
    
    # Train the model
    model.fit(X, X, epochs=10, verbose=0)
    
    # Use the model to predict the smoothed output
    X_smoothed = model.predict(X)
    return X_smoothed

from tslearn.metrics import dtw
from tslearn.piecewise import PiecewiseAggregateApproximation

def dtw_smoothing(X):
    """Smooth the predictions using dynamic time warping."""
    # Compute the piecewise aggregate approximation of the input data
    n_segments = int(np.sqrt(X.shape[0]))
    paa = PiecewiseAggregateApproximation(n_segments=n_segments)
    X_paa = paa.fit_transform(X)
    
    # Compute the distance matrix between all pairs of frames
    distance_matrix = np.zeros((X.shape[0], X.shape[0]))
    for i in range(X.shape[0]):
        for j in range(X.shape[0]):
            distance_matrix[i, j] = dtw(X_paa[i], X_paa[j])
    
    # Compute the DTW-based similarity matrix
    similarity_matrix = np.exp(-distance_matrix ** 2 / (2 * np.median(distance_matrix) ** 2))
    
    # Smooth the output using the similarity matrix
    X_smoothed = np.zeros_like(X)
    for i in range(X.shape[0]):
        X_smoothed[i] = np.dot(similarity_matrix[i], X) / np.sum(similarity_matrix[i])
    return X_smoothed

preds_post_td = temporal_difference(real_preds_ff)
preds_post_rnn = rnn_smoothing(real_preds_ff)
preds_post_dtw = dtw_smoothing(real_preds_ff)

def postprocess_outs(preds, method='RNN'):
  method_dictionary = {
      'RNN': rnn_smoothing,
      'DTW': dtw_smoothing,
      'TD': temporal_difference,
      'KF': kalman_filter,
      'EMA': exponential_moving_average,
      'SMA': simple_moving_average
  }

  postproc_func = method_dictionary.get(method)
  return postproc_func(preds)

"""# Presentation"""

def draw_text_BP4D(img, probs):
    import cv2
    # BP4D labels
    AU_names = ['Inner brow raiser',
          'Outer brow raiser',
          'Brow lowerer',
          'Cheek raiser',
          'Lid tightener',
          'Upper lip raiser',
          'Lip corner puller',
          'Dimpler',
          'Lip corner depressor',
          'Chin raiser',
          'Lip tightener',
          'Lip pressor']
    AU_ids = ['1', '2', '4', '6', '7', '10', '12', '14', '15', '17', '23', '24']
    
    # from PIL import Image, ImageDraw, ImageFont
    #img = cv2.imread(path)
    pos_y = img.shape[0] // 40 + 20
    pos_x  = img.shape[1] + img.shape[1] // 100
    pos_x_ =  img.shape[1]  * 3 // 2 - img.shape[1] // 100

    img = cv2.copyMakeBorder(img, 0,0,0,img.shape[1], cv2.BORDER_CONSTANT, value=(255,255,255))
    # num_aus = len(words)
    # for i, item in enumerate(words):
    #     y = pos_y + (i * img.shape[0] // 17 )
    #     img = cv2.putText(img, str(item), (pos_x, y), cv2.FONT_HERSHEY_SIMPLEX, round(img.shape[1] / 2048, 3), (0,0,255), 2)
    # pos_y = pos_y + (num_aus * img.shape[0] // 17 )
    #for i, item in enumerate(range(12)):
    for i, item in enumerate(range(len(AU_ids))):
        # y = pos_y  + (i * img.shape[0] // 13)
        y = pos_y  + (i * img.shape[0] // len(AU_ids) + 1)
        color = (0,0,0)
        if float(probs[item]) > 0.5:
            color = (0,0,255)
        img = cv2.putText(img,  AU_names[i] + ' -- AU' +AU_ids[i] +': {:.2f}'.format(probs[i]), (pos_x, y), cv2.FONT_HERSHEY_SIMPLEX, round(img.shape[1] / 1600, 3), color, 2)
        
    return img

def draw_text(img, probs):
    # OpenGraphAU
    AU_names = ['Inner brow raiser',
        'Outer brow raiser',
        'Brow lowerer',
        'Upper lid raiser',
        'Cheek raiser',
        'Lid tightener',
        'Nose wrinkler',
        'Upper lip raiser',
        'Nasolabial deepener',
        'Lip corner puller',
        'Sharp lip puller',
        'Dimpler',
        'Lip corner depressor',
        'Lower lip depressor',
        'Chin raiser',
        'Lip pucker',
        'Tongue show',
        'Lip stretcher',
        'Lip funneler',
        'Lip tightener',
        'Lip pressor',
        'Lips part',
        'Jaw drop',
        'Mouth stretch',
        'Lip bite',
        'Nostril dilator',
        'Nostril compressor',
        'Left Inner brow raiser',
        'Right Inner brow raiser',
        'Left Outer brow raiser',
        'Right Outer brow raiser',
        'Left Brow lowerer',
        'Right Brow lowerer',
        'Left Cheek raiser',
        'Right Cheek raiser',
        'Left Upper lip raiser',
        'Right Upper lip raiser',
        'Left Nasolabial deepener',
        'Right Nasolabial deepener',
        'Left Dimpler',
        'Right Dimpler']
    AU_ids = ['1', '2', '4', '5', '6', '7', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '22',
           '23', '24', '25', '26', '27', '32', '38', '39', 'L1', 'R1', 'L2', 'R2', 'L4', 'R4', 'L6', 'R6', 'L10', 'R10', 'L12', 'R12', 'L14', 'R14']
    # from PIL import Image, ImageDraw, ImageFont
    pos_y = img.shape[0] // 40 + 15
    pos_x  = img.shape[1] + img.shape[1] // 100
    pos_x_ =  img.shape[1]  * 3 // 2 - img.shape[1] // 100

    img = cv2.copyMakeBorder(img, 0,0,0,img.shape[1], cv2.BORDER_CONSTANT, value=(255,255,255))
    # num_aus = len(words)
    # for i, item in enumerate(words):
    #     y = pos_y + (i * img.shape[0] // 17 )
    #     img = cv2.putText(img, str(item), (pos_x, y), cv2.FONT_HERSHEY_SIMPLEX, round(img.shape[1] / 2048, 3), (0,0,255), 2)
    # pos_y = pos_y + (num_aus * img.shape[0] // 17 )
    for i, item in enumerate(range(21)):
        y = pos_y  + (i * img.shape[0] // 22)
        color = (0,0,0)
        if float(probs[item]) > 0.5:
            color = (0,0,255)
        img = cv2.putText(img,  AU_names[i] + ' -- AU' +AU_ids[i] +': {:.2f}'.format(probs[i]), (pos_x, y), cv2.FONT_HERSHEY_SIMPLEX, round(img.shape[1] / 2800, 3), color, 2)

    for i, item in enumerate(range(21,41)):
        y = pos_y  + (i * img.shape[0] // 22)
        color = (0,0,0)
        if float(probs[item]) > 0.5:
            color = (0,0,255)
        img = cv2.putText(img,  AU_names[item] + ' -- AU' +AU_ids[item] +': {:.2f}'.format(probs[item]), (pos_x_, y), cv2.FONT_HERSHEY_SIMPLEX, round(img.shape[1] / 2800, 3), color, 2)
    return img

def draw_text_openface(img, probs):
    # OpenFace
    AU_names = ['Inner brow raiser',
        'Outer brow raiser',
        'Brow lowerer',
        'Upper lid raiser',
        'Cheek raiser',
        'Lid tightener',
        'Nose wrinkler', #9
        'Upper lip raiser',
        'Lip corner puller',
        'Dimpler',
        'Lip corner depressor', #15
        'Chin raiser',
        'Lip stretcher', #20
        'Lip tightener', #23
        'Lips part',
        'Jaw drop', #26
        'Lip suck',
        'Blink']
    AU_ids = ['1', '2', '4', '5', '6', '7', '9', '10', '12', '14', '15', '17', '20', '23', '25', '26', '28', '45']
    
    # from PIL import Image, ImageDraw, ImageFont
    pos_y = img.shape[0] // 40 + 15
    pos_x  = img.shape[1] + img.shape[1] // 100
    pos_x_ =  img.shape[1]  * 3 // 2 - img.shape[1] // 100

    img = cv2.copyMakeBorder(img, 0,0,0,img.shape[1], cv2.BORDER_CONSTANT, value=(255,255,255))
    # num_aus = len(words)
    # for i, item in enumerate(words):
    #     y = pos_y + (i * img.shape[0] // 17 )
    #     img = cv2.putText(img, str(item), (pos_x, y), cv2.FONT_HERSHEY_SIMPLEX, round(img.shape[1] / 2048, 3), (0,0,255), 2)
    # pos_y = pos_y + (num_aus * img.shape[0] // 17 )
    for i, item in enumerate(range(len(AU_ids))):
        # y = pos_y  + (i * img.shape[0] // 13)
        y = pos_y  + (i * img.shape[0] // len(AU_ids) + 1)
        color = (0,0,0)
        if float(probs[item]) > 0.5:
            color = (0,0,255)
        img = cv2.putText(img,  AU_names[i] + ' -- AU' +AU_ids[i] +': {:.2f}'.format(probs[i]), (pos_x, y), cv2.FONT_HERSHEY_SIMPLEX, round(img.shape[1] / 1600, 3), color, 2)
        
    return img

i = 80
if MODEL_TYPE == 'BP4D':
  x = draw_text_BP4D(ims[i], real_preds_ff[i])
elif MODEL_TYPE == 'OpenGraphAU':
  x = draw_text(ims[i], real_preds_ff[i])
fig, ax = plt.subplots(figsize=(16, 9))
ax.imshow(x, interpolation='nearest')
plt.tight_layout()

def extract_labels(images, preds, model_type):
  labeled_frames_ff = []
  for i in range(preds.shape[0]):
    if model_type == 'BP4D':
      labeled_frames_ff.append(draw_text_BP4D(images[i], preds[i]))
    elif model_type == 'OpenGraphAU':
      labeled_frames_ff.append(draw_text(images[i], preds[i]))
    elif model_type == 'OpenFace':
      labeled_frames_ff.append(draw_text_openface(images[i], preds[i]))
  labeled_frames_ff = np.array(labeled_frames_ff)
  return labeled_frames_ff

labels_no_post = extract_labels(ims, real_preds_ff, 'OpenGraphAU')
labels_sm = extract_labels(ims, preds_post_sm, 'OpenGraphAU')
labels_em = extract_labels(ims, preds_post_em, 'OpenGraphAU')
labels_kf = extract_labels(ims, preds_post_kf, 'OpenGraphAU')

labels_td = extract_labels(ims, preds_post_td, 'OpenGraphAU')
labels_rnn = extract_labels(ims, preds_post_rnn, 'OpenGraphAU')
labels_dtw = extract_labels(ims, preds_post_dtw, 'OpenGraphAU')

labels_openface = extract_labels(ims, openface_outs, 'OpenFace')

def save_video_from_images(images, video_name, fps=30):
    """
    Saves a video file from multiple images.

    Args:
    - images: a NumPy array of images
    - video_name: the name of the output video file
    - fps: the frame rate of the output video file (default is 30 fps)

    Returns:
    None
    """

    # Get the shape of the first image in the array
    height, width, channels = images[0].shape

    # Create a VideoWriter object
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = cv2.VideoWriter(video_name, fourcc, fps, (width, height))

    # Iterate through the images and write them to the video file
    for image in images:
        video_writer.write(image)

    # Release the VideoWriter object
    video_writer.release()

save_video_from_images(labels_no_post, '/content/sample_no_post.mp4', fps=30)
save_video_from_images(labels_sm, '/content/sample_simple_moving_avg.mp4', fps=30)
save_video_from_images(labels_em, '/content/sample_exp_moving_avg_post.mp4', fps=30)
save_video_from_images(labels_kf, '/content/sample_kalman_filter.mp4', fps=30)

save_video_from_images(labels_td, '/content/sample_temporal_difference.mp4', fps=30)
save_video_from_images(labels_rnn, '/content/sample_rnn.mp4', fps=30)
save_video_from_images(labels_dtw, '/content/sample_dynamic_time_warping.mp4', fps=30)

save_video_from_images(labels_openface, '/content/sample_openface.mp4', fps=30)

from google.colab import files
files.download('/content/sample_openface.mp4')

"""# Old Code"""

labeled_frames_ff = []
for i in range(NUM_TO_EXTRACT):
  if MODEL_TYPE == 'BP4D':
    labeled_frames_ff.append(draw_text_BP4D(ims[i], real_preds_ff[i]))
  elif MODEL_TYPE == 'OpenGraphAU':
    labeled_frames_ff.append(draw_text(ims[i], real_preds_ff[i]))
labeled_frames_ff = np.array(labeled_frames_ff)